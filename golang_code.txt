# /Users/gopher/Desktop/workspace/adk-go/cmd/helloworld_runner/main.go
package main

import (
	"context"
	"log"
	"os"
	"os/signal"
	"syscall"

	"github.com/KennethanCeyer/adk-go/adk"
	"github.com/KennethanCeyer/adk-go/examples/helloworld"
)

func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	go func() {
		<-sigChan
		log.Println("Shutdown signal received, cancelling context...")
		cancel()
	}()

	if helloworld.Agent == nil {
		log.Fatal("helloworld.Agent is not initialized. Check init() in examples/helloworld/agent.go and ensure GEMINI_API_KEY is set.")
	}

	runner, err := adk.NewSimpleCLIRunner(helloworld.Agent)
	if err != nil {
		log.Fatalf("Failed to create agent runner: %v", err)
	}

	log.Printf("HelloWorld Agent Runner starting... (Agent: %s, Model: %s)", helloworld.Agent.Name(), helloworld.Agent.ModelName())
	runner.Start(ctx)
	log.Println("HelloWorld Agent Runner finished.")
}

# /Users/gopher/Desktop/workspace/adk-go/tools/rolldie.go
package tools

import (
	"context"
	"crypto/rand"
	"fmt"
	"log"
	"math/big"
	// "github.com/KennethanCeyer/adk-go/adk" // adk.Tool is implemented
)

type RollDieTool struct{}

func NewRollDieTool() *RollDieTool {
	return &RollDieTool{}
}

func (t *RollDieTool) Name() string { return "roll_die" }

func (t *RollDieTool) Description() string {
	return "Rolls a die with a specified number of sides and returns the result."
}

func (t *RollDieTool) Parameters() any {
	return map[string]any{
		"type": "object",
		"properties": map[string]any{
			"sides": map[string]any{
				"type":        "integer",
				"description": "The number of sides on the die (e.g., 6, 20). Must be positive.",
			},
		},
		"required": []string{"sides"},
	}
}

func (t *RollDieTool) Execute(ctx context.Context, args any) (any, error) {
	argsMap, ok := args.(map[string]any)
	if !ok { return nil, fmt.Errorf("rolldie: invalid args format, expected map[string]any, got %T", args) }
	sidesVal, ok := argsMap["sides"]
	if !ok { return nil, fmt.Errorf("rolldie: missing 'sides' argument") }

	var sides int64
	switch v := sidesVal.(type) {
	case float64:
		if v <= 0 || v != float64(int64(v)) { return nil, fmt.Errorf("rolldie: 'sides' (float64) must be positive integer, got %f", v) }
		sides = int64(v)
	case int:
		if v <= 0 { return nil, fmt.Errorf("rolldie: 'sides' (int) must be positive, got %d", v) }
		sides = int64(v)
	// Consider json.Number if LLM strictly adheres to JSON spec for numbers from schema
	default:
		return nil, fmt.Errorf("rolldie: 'sides' argument must be numeric, got type %T", sidesVal)
	}
	if sides <= 0 { return nil, fmt.Errorf("rolldie: 'sides' must be positive, got %d", sides) }

	nBig, err := rand.Int(rand.Reader, big.NewInt(sides))
	if err != nil { return nil, fmt.Errorf("rolldie: rand.Int failed: %w", err) }
	result := nBig.Int64() + 1
	log.Printf("RollDieTool: Executed. Rolled %d (d%d)", result, sides)
	return map[string]any{"result": result, "sides_rolled": sides}, nil
}

# /Users/gopher/Desktop/workspace/adk-go/llmproviders/gemini.go
package llmproviders

import (
	"context"
	"fmt"
	"log"
	"os"

	"github.com/KennethanCeyer/adk-go/adk"
	adktypes "github.com/KennethanCeyer/adk-go/adk/types"
	"github.com/google/generative-ai-go/genai"
	"google.golang.org/api/iterator"
	"google.golang.org/api/option"
)

type GeminiLLMProvider struct {
	apiKey string
}

func NewGeminiLLMProvider() (*GeminiLLMProvider, error) {
	apiKey := os.Getenv("GEMINI_API_KEY")
	if apiKey == "" {
		return nil, fmt.Errorf("GEMINI_API_KEY environment variable not set")
	}
	return &GeminiLLMProvider{apiKey: apiKey}, nil
}

func convertADKToolsToGenaiTools(adkTools []adk.Tool) []*genai.Tool {
	if len(adkTools) == 0 { return nil }
	genaiTools := make([]*genai.Tool, len(adkTools))
	for i, t := range adkTools {
		var paramSchema *genai.Schema
		paramSchemaData := t.Parameters()
		if paramSchemaData != nil {
			if schemaMap, ok := paramSchemaData.(map[string]any); ok {
				paramSchema = &genai.Schema{}
				if schemaTypeStr, typeOk := schemaMap["type"].(string); typeOk {
					switch schemaTypeStr {
					case "object": paramSchema.Type = genai.TypeObject
					case "string": paramSchema.Type = genai.TypeString
					case "integer": paramSchema.Type = genai.TypeInteger
					case "number": paramSchema.Type = genai.TypeNumber
					case "boolean": paramSchema.Type = genai.TypeBoolean
					case "array": paramSchema.Type = genai.TypeArray
						if itemsMap, itemsOk := schemaMap["items"].(map[string]any); itemsOk {
							itemSchema := &genai.Schema{}
							if itemTypeStr, itemTypeOk := itemsMap["type"].(string); itemTypeOk {
								switch itemTypeStr {
								case "string": itemSchema.Type = genai.TypeString; case "integer": itemSchema.Type = genai.TypeInteger; case "number": itemSchema.Type = genai.TypeNumber; default: itemSchema.Type = genai.TypeString
								}
							}
							paramSchema.Items = itemSchema
						}
					default: paramSchema.Type = genai.TypeObject
					}
				} else { paramSchema.Type = genai.TypeObject }

				if props, pOk := schemaMap["properties"].(map[string]any); pOk {
					paramSchema.Properties = make(map[string]*genai.Schema)
					for k, v := range props {
						propSchemaMap, psOk := v.(map[string]any);
						if !psOk { continue }
						propDesc, _ := propSchemaMap["description"].(string)
						var currentPropType genai.Type
						propTypeStr, typeOk := propSchemaMap["type"].(string)
						if !typeOk { currentPropType = genai.TypeString
						} else {
							switch propTypeStr {
							case "string": currentPropType = genai.TypeString; case "integer": currentPropType = genai.TypeInteger; case "number": currentPropType = genai.TypeNumber; case "boolean": currentPropType = genai.TypeBoolean; case "object": currentPropType = genai.TypeObject; case "array": currentPropType = genai.TypeArray; default: currentPropType = genai.TypeString
							}
						}
						paramSchema.Properties[k] = &genai.Schema{Type: currentPropType, Description: propDesc}
					}
				}
				if reqAny, rOk := schemaMap["required"].([]any); rOk {
					for _, req := range reqAny { if rStr, rsOk := req.(string); rsOk { paramSchema.Required = append(paramSchema.Required, rStr) } }
				} else if reqStr, rSOk := schemaMap["required"].([]string); rSOk { paramSchema.Required = reqStr }

			} else if schemaGenai, ok := paramSchemaData.(*genai.Schema); ok { paramSchema = schemaGenai
			} else if paramSchemaData != nil { log.Printf("Warning: Tool %s param schema is unhandled type %T", t.Name(), paramSchemaData); paramSchema = nil }
		}
		genaiTools[i] = &genai.Tool{FunctionDeclarations: []*genai.FunctionDeclaration{{Name: t.Name(), Description: t.Description(), Parameters: paramSchema}}}
	}
	return genaiTools
}

func convertADKMessagesToGenaiContent(messages []adktypes.Message) []*genai.Content {
	var contents []*genai.Content
	for _, msg := range messages {
		content := &genai.Content{Role: msg.Role}
		for _, p := range msg.Parts {
			var genaiPart genai.Part
			if p.Text != nil { genaiPart = genai.Text(*p.Text)
			} else if p.FunctionCall != nil { if msg.Role == "model" { genaiPart = genai.FunctionCall{Name: p.FunctionCall.Name, Args: p.FunctionCall.Args} } else { continue }
			} else if p.FunctionResponse != nil { genaiPart = genai.FunctionResponse{Name: p.FunctionResponse.Name, Response: p.FunctionResponse.Response}
			} else { continue }
			content.Parts = append(content.Parts, genaiPart)
		}
		if len(content.Parts) > 0 { contents = append(contents, content) }
	}
	return contents
}

func convertGenaiCandidateToADKMessage(candidate *genai.Candidate) adktypes.Message {
	msg := adktypes.Message{Role: "model"}
	if candidate == nil { text := "Error: LLM candidate nil."; msg.Parts = []adktypes.Part{{Text: &text}}; return msg }
	if candidate.Content == nil {
		text := "Error: LLM no content."
		if fr := candidate.FinishReason; fr != genai.FinishReasonStop && fr != genai.FinishReasonUnspecified { text = fmt.Sprintf("LLM stop: %s.", fr.String()) }
		msg.Parts = []adktypes.Part{{Text: &text}}; return msg
	}
	for _, p := range candidate.Content.Parts {
		var adkPart adktypes.Part
		switch v := p.(type) {
		case genai.Text: text := string(v); adkPart.Text = &text
		case genai.FunctionCall: adkPart.FunctionCall = &adktypes.FunctionCall{Name: v.Name, Args: v.Args}
		default: unsupportedText := fmt.Sprintf("[LLM Part Type %T]", v); adkPart.Text = &unsupportedText
		}
		msg.Parts = append(msg.Parts, adkPart)
	}
	return msg
}

func (g *GeminiLLMProvider) GenerateContent(
	ctx context.Context, modelName string, systemInstruction string, tools []adk.Tool,
	history []adktypes.Message, latestMessage adktypes.Message,
) (adktypes.Message, error) {
	client, err := genai.NewClient(ctx, option.WithAPIKey(g.apiKey))
	if err != nil { return adktypes.Message{}, fmt.Errorf("genai client: %w", err) }
	defer client.Close()

	model := client.GenerativeModel(modelName)
	if systemInstruction != "" { model.SystemInstruction = &genai.Content{Parts: []genai.Part{genai.Text(systemInstruction)}} }
	if len(tools) > 0 { model.Tools = convertADKToolsToGenaiTools(tools) }

	chatSession := model.StartChat()
	if len(history) > 0 { chatSession.History = convertADKMessagesToGenaiContent(history) }
	
	latestGenaiContents := convertADKMessagesToGenaiContent([]adktypes.Message{latestMessage})
	var latestPartsToSend []genai.Part
    if len(latestGenaiContents) > 0 && len(latestGenaiContents[0].Parts) > 0 {
        latestPartsToSend = latestGenaiContents[0].Parts
    } else {
        log.Println("Warning: latestMessage converted to empty parts for LLM. Sending an empty text part.");
        latestPartsToSend = []genai.Part{genai.Text("")} // Send a minimal valid part
    }

	iter := chatSession.SendMessageStream(ctx, latestPartsToSend...)
	var aggResp genai.GenerateContentResponse; var aggParts []genai.Part
	for {
		resp, err := iter.Next()
		if err == iterator.Done { break }
		if err != nil { return adktypes.Message{}, fmt.Errorf("stream LLM: %w", err) }
		if aggResp.Candidates == nil && aggResp.PromptFeedback == nil { aggResp.PromptFeedback = resp.PromptFeedback }
		if len(resp.Candidates) > 0 && resp.Candidates[0].Content != nil {
			for _, sp := range resp.Candidates[0].Content.Parts {
				if fc, ok := sp.(genai.FunctionCall); ok { aggParts = []genai.Part{fc}; break
				} else if t, ok := sp.(genai.Text); ok {
					if len(aggParts) > 0 { if lt, lOk := aggParts[len(aggParts)-1].(genai.Text); lOk { aggParts[len(aggParts)-1] = genai.Text(string(lt) + string(t)); continue } }
					aggParts = append(aggParts, t)
				} else { aggParts = append(aggParts, sp) }
			}
			aggResp.Candidates = []*genai.Candidate{{FinishReason: resp.Candidates[0].FinishReason}}
		}
	}
	if len(aggResp.Candidates) > 0 {
		if aggResp.Candidates[0].Content == nil { aggResp.Candidates[0].Content = &genai.Content{} }
		aggResp.Candidates[0].Content.Parts = aggParts; aggResp.Candidates[0].Content.Role = "model"
	} else if len(aggParts) > 0 { aggResp.Candidates = []*genai.Candidate{{Content: &genai.Content{Role: "model", Parts: aggParts}}} }

	if len(aggResp.Candidates) == 0 {
		bt := "LLM empty/no candidates."
	
		if pf := aggResp.PromptFeedback; pf != nil && pf.BlockReason != genai.BlockReasonUnspecified { bt = fmt.Sprintf("LLM blocked. R: %s. M: %s", pf.BlockReason.String(), pf.BlockReason.String()) }
		log.Println("Warning:", bt); return adktypes.Message{Role: "model", Parts: []adktypes.Part{{Text: &bt}}}, nil
	}
	return convertGenaiCandidateToADKMessage(aggResp.Candidates[0]), nil
}

# /Users/gopher/Desktop/workspace/adk-go/README.md
# ADK-Go: Agent Development Kit in Go (Migration In Progress)

<html>
    <h2 align="center">
      <img src="https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png" width="256"/>
    </h2>
    <h3 align="center">
      An open-source, code-first Go toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.
    </h3>
    <h3 align="center">
      Important Links:
      <a href="https://google.github.io/adk-docs/">Docs</a>
    </h3>
</html>

ðŸ‘‹ Welcome to ADK-Go!

This repository is currently undergoing a migration from its original Python-based Agent Development Kit (ADK) to Go. The primary goal is to create a robust, performant, and idiomatic Go version of the ADK.

**Please Note**: This is an active development project. Many features from the Python ADK are not yet implemented or are in the process of being ported. The initial focus is on establishing a core, runnable agent interaction loop.

This README provides instructions to set up and run a **foundational "Hello World" agent example**. This example demonstrates the basic structure, core ADK concepts in Go, and interaction with Google's Gemini LLM.

## Target Project Structure (For Runnable HelloWorld)

While the repository contains more files from the ongoing migration, the "Hello World" example relies on the following core structure:

```plaintext
adk-go/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ helloworld_runner/
â”‚       â””â”€â”€ main.go         # Executable for the HelloWorld agent
â”œâ”€â”€ adk/
â”‚   â”œâ”€â”€ agent.go            # Core Agent, Tool, LLMProvider interfaces; BaseAgent impl.
â”‚   â”œâ”€â”€ runner.go           # SimpleCLIRunner for command-line interaction
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ types.go        # Defines Message, Part, FunctionCall, FunctionResponse
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ helloworld/
â”‚       â””â”€â”€ agent.go        # HelloWorld agent definition and initialization
â”œâ”€â”€ llmproviders/
â”‚   â””â”€â”€ gemini.go           # Gemini LLMProvider implementation
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ rolldie.go          # RollDieTool implementation
â”œâ”€â”€ go.mod                  # Manages project dependencies
â”œâ”€â”€ go.sum                  # Checksums for dependencies
â””â”€â”€ README.md
```

As the migration progresses, other directories and files from your provided list (like `flows`, `auth`, `events`, etc.) will be populated and integrated.

Prerequisites

- **Go**: Version 1.20 or later. ([Installation Guide](https://go.dev/doc/install))
- **Git**: For cloning the repository.
- **Google Cloud Project**: A Google Cloud Project with the Vertex AI API (or Generative Language API via Google AI Studio) enabled.
- **Gemini API Key**: An API key for Google Gemini.
  - Important: Secure your API key. Do not commit it directly into code. Use environment variables.
- **Environment Variable**: Set the `GEMINI_API_KEY` environment variable.

## Setup & Running the HelloWorld Agent

1. **Clone the Repository**

   Open your terminal and clone the project repository:

   ```bash
   git clone https://github.com/KennethanCeyer/adk-go.git
   cd adk-go
   ```

2. **Tidy Dependencies**

   Ensure all necessary Go modules are downloaded and the `go.mod` and `go.sum` files are up-to-date.

   ```bash
   go mod tidy
   ```

   This command will automatically download `github.com/google/generative-ai-go/genai` and its dependencies based on the imports in the Go files.

3. **Set `GEMINI_API_KEY` Environment Variable (if not already done, as described in "Prerequisites")**

4. **Run the HelloWorld Agent**

   From the `adk-go` root directory

   ```bash
   go run ./cmd/helloworld_runner/main.go
   ```

5. **Interact with the Agent**

   Once the runner starts, it will prompt you for input. Try the following.

   ```bash
   2025/05/22 17:00:10 HelloWorldAgent initialized in examples/helloworld/agent.go.
   2025/05/22 17:00:10 HelloWorld Agent Runner starting... (Agent: HelloWorldAgent, Model: gemini-1.5-flash-latest)
   --- Starting Agent: HelloWorldAgent ---
   Type 'exit' or 'quit' to stop.
   [user]: Hello! Can you roll a 6-sided die for me?
   2025/05/22 17:00:22 RollDieTool: Executed. Rolled 2 (d6)
   [HelloWorldAgent]: You rolled a 2 on a 6-sided die.
   ```

   ![Helloworld example](./docs/helloworld_example.png)

## Next Steps & Contribution

This "Hello World" example serves as the initial building block. The next steps in the migration will involve:

- Implementing more core ADK features (e.g., advanced agent types, session management, event handling, complex flows).
- Porting additional tools and planners.
- Adding comprehensive tests.

# /Users/gopher/Desktop/workspace/adk-go/adk/types/types.go
package types

type Message struct {
	Role  string `json:"role"`
	Parts []Part `json:"parts"`
}

type Part struct {
	Text             *string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
}

type FunctionCall struct {
	Name string
	Args map[string]any
}

type FunctionResponse struct {
	Name     string
	Response map[string]any
}

# /Users/gopher/Desktop/workspace/adk-go/adk/runner.go
package adk

import (
	"bufio"
	"context"
	"fmt"
	"log"
	"os"
	"strings"

	adktypes "github.com/KennethanCeyer/adk-go/adk/types"
)

type SimpleCLIRunner struct {
	AgentToRun Agent
}

func NewSimpleCLIRunner(agent Agent) (*SimpleCLIRunner, error) {
	if agent == nil {
		return nil, fmt.Errorf("agent cannot be nil")
	}
	return &SimpleCLIRunner{AgentToRun: agent}, nil
}

func (r *SimpleCLIRunner) Start(ctx context.Context) {
	fmt.Printf("--- Starting Agent: %s ---\n", r.AgentToRun.Name())
	fmt.Println("Type 'exit' or 'quit' to stop.")

	var history []adktypes.Message
	scanner := bufio.NewScanner(os.Stdin)

	for {
		select {
		case <-ctx.Done():
			log.Println("Runner context cancelled, shutting down.")
			return
		default:
		}

		fmt.Print("[user]: ")
		if !scanner.Scan() {
			if err := scanner.Err(); err != nil {
				log.Printf("Error reading input: %v", err)
			}
			return
		}
		userInputText := strings.TrimSpace(scanner.Text())

		if strings.ToLower(userInputText) == "exit" || strings.ToLower(userInputText) == "quit" {
			fmt.Println("Exiting agent.")
			return
		}
		if userInputText == "" {
			continue
		}

		userMessage := adktypes.Message{Role: "user", Parts: []adktypes.Part{{Text: &userInputText}}}

		currentHistoryForCall := make([]adktypes.Message, len(history))
		copy(currentHistoryForCall, history)

		agentResponse, err := r.AgentToRun.Process(ctx, currentHistoryForCall, userMessage)
		if err != nil {
			log.Printf("Error from agent %s: %v", r.AgentToRun.Name(), err)
			fmt.Printf("[%s-error]: Sorry, I encountered an error: %v\n", r.AgentToRun.Name(), err)
			continue
		}

		history = append(history, userMessage)
		history = append(history, agentResponse)

		var responseTexts []string
		for _, part := range agentResponse.Parts {
			if part.Text != nil {
				responseTexts = append(responseTexts, *part.Text)
			}
			if part.FunctionCall != nil {
				responseTexts = append(responseTexts, fmt.Sprintf("[INFO: Agent should have handled FunctionCall: %s. This indicates an issue in BaseAgent.Process if seen by user.]", part.FunctionCall.Name))
			}
		}
		fmt.Printf("[%s]: %s\n", r.AgentToRun.Name(), strings.Join(responseTexts, "\n"))

		const maxHistoryItems = 20
		if len(history) > maxHistoryItems {
			history = history[len(history)-maxHistoryItems:]
		}
	}
}

# /Users/gopher/Desktop/workspace/adk-go/adk/agent.go
package adk

import (
	"context"
	"fmt"

	adktypes "github.com/KennethanCeyer/adk-go/adk/types"
)

type Tool interface {
	Name() string
	Description() string
	Parameters() any
	Execute(ctx context.Context, args any) (any, error)
}

type LLMProvider interface {
	GenerateContent(
		ctx context.Context,
		modelName string,
		systemInstruction string,
		tools []Tool,
		history []adktypes.Message,
		latestMessage adktypes.Message,
	) (adktypes.Message, error)
}

type Agent interface {
	Name() string
	SystemInstruction() string
	Tools() []Tool
	ModelName() string
	LLMProvider() LLMProvider
	Process(ctx context.Context, history []adktypes.Message, latestMessage adktypes.Message) (adktypes.Message, error)
}

type BaseAgent struct {
	AgentNameField              string
	AgentSystemInstructionField string
	AgentToolsField             []Tool
	AgentModelNameField         string
	AgentLLMProviderField       LLMProvider
}

func NewBaseAgent(name, systemInstruction, modelName string, llmProvider LLMProvider, tools []Tool) *BaseAgent {
	if tools == nil {
		tools = []Tool{}
	}
	return &BaseAgent{
		AgentNameField:              name,
		AgentSystemInstructionField: systemInstruction,
		AgentModelNameField:         modelName,
		AgentLLMProviderField:       llmProvider,
		AgentToolsField:             tools,
	}
}

func (a *BaseAgent) Name() string              { return a.AgentNameField }
func (a *BaseAgent) SystemInstruction() string { return a.AgentSystemInstructionField }
func (a *BaseAgent) Tools() []Tool             { return a.AgentToolsField }
func (a *BaseAgent) ModelName() string         { return a.AgentModelNameField }
func (a *BaseAgent) LLMProvider() LLMProvider  { return a.AgentLLMProviderField }

func (a *BaseAgent) Process(ctx context.Context, history []adktypes.Message, latestMessage adktypes.Message) (adktypes.Message, error) {
	if a.AgentLLMProviderField == nil {
		return adktypes.Message{}, fmt.Errorf("agent %s has no LLMProvider configured", a.AgentNameField)
	}

	llmResponseMsg, err := a.AgentLLMProviderField.GenerateContent(
		ctx,
		a.AgentModelNameField,
		a.AgentSystemInstructionField,
		a.AgentToolsField,
		history,
		latestMessage,
	)
	if err != nil {
		return adktypes.Message{}, fmt.Errorf("LLMProvider.GenerateContent failed for agent %s: %w", a.AgentNameField, err)
	}

	var pendingFunctionCall *adktypes.FunctionCall
	for _, part := range llmResponseMsg.Parts {
		if part.FunctionCall != nil {
			pendingFunctionCall = part.FunctionCall
			break
		}
	}

	if pendingFunctionCall != nil {
		var selectedTool Tool
		for _, t := range a.AgentToolsField {
			if t.Name() == pendingFunctionCall.Name {
				selectedTool = t
				break
			}
		}

		var toolResponseMessage adktypes.Message
		if selectedTool == nil {
			errText := fmt.Sprintf("LLM requested unknown tool '%s'", pendingFunctionCall.Name)
			toolResponseMessage = adktypes.Message{
				Role: "user",
				Parts: []adktypes.Part{{
					FunctionResponse: &adktypes.FunctionResponse{
						Name:     pendingFunctionCall.Name,
						Response: map[string]any{"error": errText},
					},
				}},
			}
		} else {
			toolResultData, toolErr := selectedTool.Execute(ctx, pendingFunctionCall.Args)
			toolExecutionResponse := adktypes.FunctionResponse{Name: selectedTool.Name()}
			if toolErr != nil {
				toolExecutionResponse.Response = map[string]any{"error": toolErr.Error()}
			} else {
				toolExecutionResponse.Response = toolResultData.(map[string]any)
			}
			toolResponseMessage = adktypes.Message{
				Role:  "user",
				Parts: []adktypes.Part{{FunctionResponse: &toolExecutionResponse}},
			}
		}

		updatedHistory := append(history, latestMessage, llmResponseMsg)

		finalLlmResponseMsg, err := a.AgentLLMProviderField.GenerateContent(
			ctx,
			a.AgentModelNameField,
			a.AgentSystemInstructionField,
			a.AgentToolsField,
			updatedHistory,
			toolResponseMessage,
		)
		if err != nil {
			return adktypes.Message{}, fmt.Errorf("LLMProvider.GenerateContent after tool call failed for agent %s: %w", a.AgentNameField, err)
		}
		return finalLlmResponseMsg, nil
	}
	return llmResponseMsg, nil
}

# /Users/gopher/Desktop/workspace/adk-go/examples/helloworld/agent.go
package helloworld

import (
	"fmt"
	"log"

	"github.com/KennethanCeyer/adk-go/adk"
	"github.com/KennethanCeyer/adk-go/llmproviders"
	"github.com/KennethanCeyer/adk-go/tools"
)

var Agent adk.Agent

func init() {
	geminiProvider, err := llmproviders.NewGeminiLLMProvider()
	if err != nil {
		panic(fmt.Sprintf("helloworld.init: Failed to create GeminiLLMProvider: %v. Check GEMINI_API_KEY.", err))
	}

	rollDieTool := tools.NewRollDieTool()
	agentTools := []adk.Tool{rollDieTool}

	Agent = adk.NewBaseAgent(
		"HelloWorldAgent",
		"You are a friendly assistant. You can roll dice. When you use the roll_die tool, tell the user what was rolled and the die type (e.g., 'You rolled a 5 on a 6-sided die.').",
		"gemini-1.5-flash-latest",
		geminiProvider,
		agentTools,
	)
	log.Println("HelloWorldAgent initialized in examples/helloworld/agent.go.")
}

# /Users/gopher/Desktop/workspace/adk-go/golang_code.txt
# /Users/gopher/Desktop/workspace/adk-go/cmd/echoagent_runner/main.go
package main

import (
	"bufio"
	"context"
	"fmt"
	"log"
	"os"
	"os/signal"
	"strings"
	"syscall"

	"github.com/KennethanCeyer/adk-go/examples/echoagent"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
)

func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	go func() {
		<-sigChan
		log.Println("Shutdown signal received, cancelling context...")
		cancel()
		// Allow time for cleanup or force exit if needed after a delay
		// time.Sleep(1 * time.Second)
		// os.Exit(1)
	}()

	if echoagent.ConcreteLlmAgent == nil {
		log.Fatal("EchoAgent runner: ConcreteLlmAgent is not initialized. Check init() in examples/echoagent/agent.go and ensure GEMINI_API_KEY is set.")
	}
	agentToRun := echoagent.ConcreteLlmAgent

	fmt.Printf("--- Starting Agent: %s ---\n", agentToRun.GetName())
	if agentToRun.GetDescription() != "" {
		fmt.Printf("Description: %s\n", agentToRun.GetDescription())
	}
	fmt.Printf("Model: %s\n", agentToRun.GetModelIdentifier())
	if len(agentToRun.GetTools()) > 0 {
		fmt.Println("Available Tools:")
		for _, tool := range agentToRun.GetTools() {
			fmt.Printf("  - %s: %s\n", tool.Name(), tool.Description())
		}
	}
	fmt.Println("Type 'exit' or 'quit' to stop.")
	fmt.Println("------------------------------------")

	var history []modelstypes.Content
	scanner := bufio.NewScanner(os.Stdin)

	for {
		if ctx.Err() != nil { // Check if context was cancelled (e.g., by SIGINT)
			log.Println("Context cancelled, exiting runner loop.")
			break
		}
		fmt.Print("[user]: ")
		if !scanner.Scan() {
			if err := scanner.Err(); err != nil {
				log.Printf("Input error: %v", err)
			} else {
				log.Println("EOF received, exiting.")
			}
			break
		}
		userInputText := strings.TrimSpace(scanner.Text())

		if strings.ToLower(userInputText) == "exit" || strings.ToLower(userInputText) == "quit" {
			fmt.Println("Exiting agent.")
			break
		}
		if userInputText == "" {
			continue
		}

		currentUserContent := modelstypes.Content{
			Role:  "user",
			Parts: []modelstypes.Part{{Text: &userInputText}},
		}

		agentResponseContent, err := agentToRun.Process(ctx, history, currentUserContent)
		if err != nil {
			log.Printf("Error from agent.Process: %v", err)
			fmt.Printf("[%s-error]: I encountered an issue: %v\n", agentToRun.GetName(), err)
			history = append(history, currentUserContent) // Add user message for context
			continue
		}

		history = append(history, currentUserContent)
		if agentResponseContent != nil {
			history = append(history, *agentResponseContent)
		}

		if agentResponseContent != nil && len(agentResponseContent.Parts) > 0 {
			var responseTexts []string
			for _, part := range agentResponseContent.Parts {
				if part.Text != nil {
					responseTexts = append(responseTexts, *part.Text)
				}
				if part.FunctionCall != nil {
					log.Printf("Runner: Agent response contained FunctionCall: Name=%s. This should have been handled by agent's Process loop.", part.FunctionCall.Name)
					responseTexts = append(responseTexts, fmt.Sprintf("[Debug: Agent yielded unhandled FunctionCall to tool '%s']", part.FunctionCall.Name))
				}
			}
			fmt.Printf("[%s]: %s\n", agentToRun.GetName(), strings.Join(responseTexts, "\n"))
		} else {
			fmt.Printf("[%s]: (Agent returned no displayable content)\n", agentToRun.GetName())
		}

		const maxHistoryTurns = 10 // Keep last 10 pairs of user/model messages
		if len(history) > maxHistoryTurns*2 {
			history = history[len(history)-(maxHistoryTurns*2):]
		}
	}
	log.Println("EchoAgent Runner finished.")
}

# /Users/gopher/Desktop/workspace/adk-go/cmd/helloworld_runner/main.go
package main

import (
	"context"
	"log"
	"os"
	"os/signal"
	"syscall"

	"github.com/KennethanCeyer/adk-go/adk"
	"github.com/KennethanCeyer/adk-go/examples/helloworld"
)

func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	go func() {
		<-sigChan
		log.Println("Shutdown signal received, cancelling context...")
		cancel()
	}()

	if helloworld.Agent == nil {
		log.Fatal("HelloWorld agent is not initialized. Ensure GEMINI_API_KEY is set.")
	}

	runner, err := adk.NewSimpleCLIRunner(helloworld.Agent)
	if err != nil {
		log.Fatalf("Failed to create agent runner: %v", err)
	}

	log.Println("HelloWorld Agent Runner starting... (Model: " + helloworld.Agent.ModelName() + ")")
	runner.Start(ctx)
	log.Println("HelloWorld Agent Runner finished.")
}

# /Users/gopher/Desktop/workspace/adk-go/tools/types/types.go
package types

import (
	"context"
)

type Tool interface {
	Name() string
	Description() string
	Parameters() any // e.g., map[string]any for JSON schema
	Execute(ctx context.Context, args any) (result any, err error)
}

# /Users/gopher/Desktop/workspace/adk-go/tools/example/echo_tool.go
package example

import (
	"context"
	"fmt"

	"github.com/google/generative-ai-go/genai"
)

// EchoTool is a simple tool that echoes input.
type EchoTool struct{}

// NewEchoTool creates an EchoTool.
func NewEchoTool() *EchoTool {
	return &EchoTool{}
}

// Name returns the tool's name.
func (t *EchoTool) Name() string {
	return "echo_tool"
}

// Description returns the tool's purpose.
func (t *EchoTool) Description() string {
	return "Echoes back the input message provided to it. Useful for testing function calling."
}

// Parameters defines the schema for arguments.
func (t *EchoTool) Parameters() any {
	return &genai.Schema{
		Type: genai.TypeObject,
		Properties: map[string]*genai.Schema{
			"message_to_echo": {
				Type:        genai.TypeString,
				Description: "The exact message that the tool should echo back.",
			},
		},
		Required: []string{"message_to_echo"},
	}
}

// Execute runs the tool.
func (t *EchoTool) Execute(ctx context.Context, args any) (any, error) {
	argsMap, ok := args.(map[string]any)
	if !ok {
		return nil, fmt.Errorf("echo_tool: invalid arguments format, expected map[string]any, got %T", args)
	}

	messageVal, ok := argsMap["message_to_echo"]
	if !ok {
		return nil, fmt.Errorf("echo_tool: missing 'message_to_echo' argument")
	}

	messageStr, ok := messageVal.(string)
	if !ok {
		return nil, fmt.Errorf("echo_tool: 'message_to_echo' argument must be a string, got %T", messageVal)
	}

	return map[string]any{"echoed_message": "Echo: " + messageStr}, nil
}

# /Users/gopher/Desktop/workspace/adk-go/tools/interface.go
package tools

import (
	"context"
)

// Tool defines the interface for agent capabilities.
type Tool interface {
	Name() string
	Description() string
	Parameters() any // JSON schema as map[string]any or *genai.Schema.
	Execute(ctx context.Context, args any) (result any, err error)
}

# /Users/gopher/Desktop/workspace/adk-go/tools/rolldie.go
package tools

import (
	"context"
	"crypto/rand"
	"fmt"
	"log"
	"math/big"
	// No *genai.Schema directly returned, so no genai import needed here
	// "github.com/google/generative-ai-go/genai"
)

// RollDieTool is a simple tool that simulates rolling a die.
type RollDieTool struct{}

// NewRollDieTool creates a new RollDieTool.
func NewRollDieTool() *RollDieTool {
	return &RollDieTool{}
}

// Name returns the tool's name.
func (t *RollDieTool) Name() string {
	return "roll_die"
}

// Description returns the tool's description.
func (t *RollDieTool) Description() string {
	return "Rolls a die with a specified number of sides and returns the result."
}

// Parameters defines the JSON schema for the arguments this tool accepts.
// It returns a map[string]any representing the JSON schema.
func (t *RollDieTool) Parameters() any {
	return map[string]any{
		"type": "object",
		"properties": map[string]any{
			"sides": map[string]any{
				"type":        "integer", // JSON Schema type
				"description": "The number of sides on the die (e.g., 6 for a standard die, 20 for a d20). Must be a positive integer.",
			},
		},
		"required": []string{"sides"},
	}
}

// Execute performs the die roll.
// args is expected to be map[string]any as per JSON object schema.
func (t *RollDieTool) Execute(ctx context.Context, args any) (any, error) {
	argsMap, ok := args.(map[string]any)
	if !ok {
		return nil, fmt.Errorf("invalid arguments format for RollDieTool: expected map[string]any, got %T", args)
	}

	sidesVal, ok := argsMap["sides"]
	if !ok {
		return nil, fmt.Errorf("missing 'sides' argument in RollDieTool")
	}

	var sides int64
	// JSON numbers might come as float64 from the LLM, even if schema says integer.
	switch v := sidesVal.(type) {
	case float64:
		if v <= 0 || v != float64(int64(v)) { // Check if positive and effectively an integer
			return nil, fmt.Errorf("'sides' must be a positive integer, got %f", v)
		}
		sides = int64(v)
	case int:
		if v <= 0 { return nil, fmt.Errorf("'sides' must be a positive integer, got %d", v) }
		sides = int64(v)
	case int32:
		if v <= 0 { return nil, fmt.Errorf("'sides' must be a positive integer, got %d", v) }
		sides = int64(v)
	case int64:
		if v <= 0 { return nil, fmt.Errorf("'sides' must be a positive integer, got %d", v) }
		sides = v
	default:
		return nil, fmt.Errorf("'sides' argument must be a numeric type that can convert to an integer, got type %T", sidesVal)
	}

	if sides <= 0 { // Final safety check
		return nil, fmt.Errorf("number of sides must be a positive integer after conversion, got %d", sides)
	}

	nBig, err := rand.Int(rand.Reader, big.NewInt(sides))
	if err != nil {
		return nil, fmt.Errorf("failed to generate random number for die roll: %w", err)
	}
	result := nBig.Int64() + 1

	log.Printf("RollDieTool: Executed. Rolled a %d (d%d)", result, sides)
	// The result should be structured as a map for the LLM to understand as a JSON object.
	return map[string]any{"result": result, "sides_rolled": sides}, nil
}

# /Users/gopher/Desktop/workspace/adk-go/llmproviders/gemini.go
package llmproviders

import (
	"context"
	"fmt"
	"log"
	"os"

	"github.com/KennethanCeyer/adk-go/adk"
	"github.com/google/generative-ai-go/genai"
	"google.golang.org/api/iterator"
	"google.golang.org/api/option"
)

// GeminiLLMProvider implements the adk.LLMProvider interface for Google Gemini models.
type GeminiLLMProvider struct {
	apiKey string
}

// NewGeminiLLMProvider creates a new provider for Gemini.
func NewGeminiLLMProvider() (*GeminiLLMProvider, error) {
	apiKey := os.Getenv("GEMINI_API_KEY")
	if apiKey == "" {
		return nil, fmt.Errorf("GEMINI_API_KEY environment variable not set")
	}
	return &GeminiLLMProvider{apiKey: apiKey}, nil
}

func convertADKToolsToGenaiTools(adkTools []adk.Tool) []*genai.Tool {
	if len(adkTools) == 0 {
		return nil
	}
	genaiTools := make([]*genai.Tool, len(adkTools))
	for i, t := range adkTools {
		var paramSchema *genai.Schema
		paramSchemaData := t.Parameters() // This should return a map[string]any or *genai.Schema

		if paramSchemaData != nil {
			if schemaMap, ok := paramSchemaData.(map[string]any); ok {
				// Attempt to convert map[string]any to *genai.Schema
				// This is a simplified conversion
				paramSchema = &genai.Schema{Type: genai.TypeObject} // Default to object
				if schemaType, ok := schemaMap["type"].(string); ok {
					switch schemaType {
					case "object":
						paramSchema.Type = genai.TypeObject
					case "string":
						paramSchema.Type = genai.TypeString
					case "integer":
						paramSchema.Type = genai.TypeInteger
					case "number":
						paramSchema.Type = genai.TypeNumber // Use TypeNumber for float/double
					case "boolean":
						paramSchema.Type = genai.TypeBoolean
					case "array":
						paramSchema.Type = genai.TypeArray
						// Note: Items for array would also need to be defined here
					}
				}

				if props, pOk := schemaMap["properties"].(map[string]any); pOk {
					paramSchema.Properties = make(map[string]*genai.Schema)
					for k, v := range props {
						propSchemaMap, psOk := v.(map[string]any)
						if !psOk { continue }
						propTypeStr, _ := propSchemaMap["type"].(string)
						propDesc, _ := propSchemaMap["description"].(string)
						var propType genai.SchemaType
						switch propTypeStr {
						case "string": propType = genai.TypeString
						case "integer": propType = genai.TypeInteger
						case "number": propType = genai.TypeNumber
						case "boolean": propType = genai.TypeBoolean
						default: propType = genai.TypeString // Default or handle error
						}
						paramSchema.Properties[k] = &genai.Schema{Type: propType, Description: propDesc}
					}
				}
				if requiredAny, rOk := schemaMap["required"].([]any); rOk {
					for _, req := range requiredAny {
						if rStr, rsOk := req.(string); rsOk {
							paramSchema.Required = append(paramSchema.Required, rStr)
						}
					}
				} else if requiredStr, rOkStr := schemaMap["required"].([]string); rOkStr {
                    paramSchema.Required = requiredStr
                }


			} else if schemaGenai, ok := paramSchemaData.(*genai.Schema); ok {
				paramSchema = schemaGenai // Already a *genai.Schema
			} else {
				log.Printf("Warning: Tool %s parameter schema is of unexpected type %T. Attempting to proceed.", t.Name(), paramSchemaData)
			}
		}

		genaiTools[i] = &genai.Tool{
			FunctionDeclarations: []*genai.FunctionDeclaration{{
				Name:        t.Name(),
				Description: t.Description(),
				Parameters:  paramSchema,
			}},
		}
	}
	return genaiTools
}

func convertADKMessagesToGenaiContent(messages []adk.Message) []*genai.Content {
	var contents []*genai.Content
	for _, msg := range messages {
		content := &genai.Content{Role: msg.Role} // Role "user" or "model"
		for _, p := range msg.Parts {
			var genaiPart genai.Part
			if p.Text != nil {
				genaiPart = genai.Text(*p.Text)
			} else if p.FunctionCall != nil {
				// This part is tricky: genai.Content for requests usually doesn't take FunctionCall.
				// FunctionCall is typically *in a response from the model*.
				// If this is history, and the model previously made a FunctionCall, its role should be "model".
				if msg.Role == "model" {
					genaiPart = genai.FunctionCall{Name: p.FunctionCall.Name, Args: p.FunctionCall.Args}
				} else {
					log.Printf("Warning: Skipping FunctionCall part in a non-model message for genai history. Role: %s", msg.Role)
					continue
				}
			} else if p.FunctionResponse != nil {
				// For sending a tool's response *to* the model, it's a Part in a "user" role message.
				// (The genai SDK documentation sometimes refers to a "function" role for this,
				// but "user" role with a FunctionResponse part is also a common pattern for Gemini).
				// We are using "user" role for FR in BaseAgent.Process().
				genaiPart = genai.FunctionResponse{Name: p.FunctionResponse.Name, Response: p.FunctionResponse.Response}
			} else {
				log.Printf("Warning: Empty or unhandled ADK Part type: %+v", p)
				continue
			}
			content.Parts = append(content.Parts, genaiPart)
		}
		if len(content.Parts) > 0 {
			contents = append(contents, content)
		}
	}
	return contents
}

func convertGenaiCandidateToADKMessage(candidate *genai.Candidate) adk.Message {
	msg := adk.Message{Role: "model"} // LLM responses are always from the "model"
	if candidate == nil {
		text := "Error: LLM candidate was nil."
		msg.Parts = []adk.Part{{Text: &text}}
		return msg
	}
	if candidate.Content == nil {
        // This can happen if content is filtered due to safety settings
		text := "Error: LLM returned no content (it might have been filtered due to safety settings or other reasons)."
        if candidate.FinishReason != genai.FinishReasonStop && candidate.FinishReason != genai.FinishReasonUnspecified {
            text = fmt.Sprintf("LLM generation stopped. Reason: %s.", candidate.FinishReason.String())
        }
		msg.Parts = []adk.Part{{Text: &text}}
		return msg
	}


	for _, p := range candidate.Content.Parts {
		var adkPart adk.Part
		switch v := p.(type) {
		case genai.Text:
			text := string(v)
			adkPart.Text = &text
		case genai.FunctionCall:
			adkPart.FunctionCall = &adk.FunctionCall{Name: v.Name, Args: v.Args}
		// FunctionResponse is not expected in a Candidate from LLM, it's an input *to* LLM.
		default:
			unsupportedText := fmt.Sprintf("[Unsupported LLM Part Type: %T]", v)
			adkPart.Text = &unsupportedText
			log.Printf(unsupportedText)
		}
		msg.Parts = append(msg.Parts, adkPart)
	}
	return msg
}

// GenerateContent uses the Google AI Go SDK to interact with Gemini.
func (g *GeminiLLMProvider) GenerateContent(
	ctx context.Context,
	modelName string,
	systemInstruction string,
	tools []adk.Tool,
	history []adk.Message,
	latestMessage adk.Message,
) (adk.Message, error) {
	client, err := genai.NewClient(ctx, option.WithAPIKey(g.apiKey))
	if err != nil {
		return adk.Message{}, fmt.Errorf("failed to create genai client: %w", err)
	}
	defer client.Close()

	model := client.GenerativeModel(modelName)

	if systemInstruction != "" {
		model.SystemInstruction = genai.Text(systemInstruction)
	}

	if len(tools) > 0 {
		model.Tools = convertADKToolsToGenaiTools(tools)
	}

	chatSession := model.StartChat()
	if len(history) > 0 {
		chatSession.History = convertADKMessagesToGenaiContent(history)
	}
	
	// The latestMessage is what we are sending now.
	latestContentForGenai := convertADKMessagesToGenaiContent([]adk.Message{latestMessage})
	if len(latestContentForGenai) == 0 || len(latestContentForGenai[0].Parts) == 0 {
		return adk.Message{}, fmt.Errorf("latest message resulted in no valid parts for LLM")
	}

	// SendMessageStream expects parts directly
	iter := chatSession.SendMessageStream(ctx, latestContentForGenai[0].Parts...)

	var aggregatedResponse genai.GenerateContentResponse
	var aggregatedParts []genai.Part // To store parts if they come in chunks

	for {
		resp, err := iter.Next()
		if err == iterator.Done {
			break
		}
		if err != nil {
			return adk.Message{}, fmt.Errorf("error streaming LLM response: %w", err)
		}
		// The first non-error response contains PromptFeedback and SafetyRatings
		if aggregatedResponse.Candidates == nil && aggregatedResponse.PromptFeedback == nil {
			aggregatedResponse.PromptFeedback = resp.PromptFeedback
			// Note: SafetyRatings are per-candidate, but PromptFeedback is top-level
		}

		if len(resp.Candidates) > 0 {
			// Aggregate content parts
			if resp.Candidates[0].Content != nil {
				aggregatedParts = append(aggregatedParts, resp.Candidates[0].Content.Parts...)
			}
			// Preserve the finish reason from the last candidate in the stream
			aggregatedResponse.Candidates = []*genai.Candidate{{FinishReason: resp.Candidates[0].FinishReason}}
		}
	}
    
    // After loop, populate the content of the single candidate in aggregatedResponse
    if len(aggregatedResponse.Candidates) > 0 {
        if aggregatedResponse.Candidates[0].Content == nil {
            aggregatedResponse.Candidates[0].Content = &genai.Content{}
        }
        aggregatedResponse.Candidates[0].Content.Parts = aggregatedParts
        aggregatedResponse.Candidates[0].Content.Role = "model" // From LLM
    } else if len(aggregatedParts) > 0 { // Handle case where Candidates was initially nil
         aggregatedResponse.Candidates = []*genai.Candidate{
            {Content: &genai.Content{Role: "model", Parts: aggregatedParts}},
         }
    }


	if len(aggregatedResponse.Candidates) == 0 {
		// This case implies no valid candidates were streamed or constructed.
		// Check PromptFeedback for block reasons.
		blockedText := "LLM response was empty or no valid candidates."
		if aggregatedResponse.PromptFeedback != nil && aggregatedResponse.PromptFeedback.BlockReason != genai.BlockReasonUnspecified {
			blockedText = fmt.Sprintf("LLM response blocked. Reason: %s. Message: %s",
				aggregatedResponse.PromptFeedback.BlockReason.String(),
				aggregatedResponse.PromptFeedback.BlockReasonMessage)
		}
		log.Println("Warning:", blockedText)
		return adk.Message{Role: "model", Parts: []adk.Part{{Text: &blockedText}}}, nil
	}
	
	// Convert the (aggregated) first candidate's content to ADK message format
	return convertGenaiCandidateToADKMessage(aggregatedResponse.Candidates[0]), nil
}

# /Users/gopher/Desktop/workspace/adk-go/llmproviders/interfaces/interfaces.go
package llmproviders

import (
	"context"

	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools"
)

type LLMProvider interface {
	GenerateContent(
		ctx context.Context,
		modelName string,
		systemInstruction *modelstypes.Content,
		tools []toolstypes.Tool,
		history []modelstypes.Content,
		latestContent modelstypes.Content,
	) (*modelstypes.Content, error)
}

# /Users/gopher/Desktop/workspace/adk-go/auth/types/types.go
package types

type AuthConfigName string

type AuthConfig struct {
	Name         AuthConfigName
	AuthScheme   string
	Scope        string
	ClientID     string
	ClientSecret string
	// Other scheme-specific fields (e.g., auth_url, token_url)
}

type AuthCredential struct {
	Name         AuthConfigName
	AccessToken  string
	RefreshToken string
	ExpiresIn    int64 // Seconds
	ObtainedAt   int64 // Unix timestamp
	Scope        string
	IDToken      string
}

func (ac *AuthCredential) IsExpired() bool {
	if ac.AccessToken == "" || ac.ExpiresIn <= 0 || ac.ObtainedAt <= 0 {
		return true
	}
	// Needs actual time checking logic, e.g. using time.Now().Unix()
	// return time.Now().Unix() >= (ac.ObtainedAt + ac.ExpiresIn - 60) // 60s buffer
	return false // Placeholder
}

// AuthHandler interface (can also be in auth/interfaces/interfaces.go)
// To avoid too many "types" packages, interfaces can live with their functional code or in a dedicated interfaces sub-package.
// For now, keeping it here for simplicity of this correction step.
// If AuthHandler needs to be used by other packages that auth might import,
// it would need to be in a more common or dedicated interface package.
type AuthHandler interface {
	GetAllAuthConfigs() []AuthConfig
	GetAuthConfig(name AuthConfigName) (*AuthConfig, error)
	// Session type will be from github.com/KennethanCeyer/adk-go/sessions/types
	GetCredential(name AuthConfigName, session any) (AuthCredential, error)
	// IsToolAuthorized(toolName string, authConfigsUsedByTool []AuthConfigName, session any) (bool, error)
	// GetAuthTool() any // Would return a toolstypes.Tool
}

# /Users/gopher/Desktop/workspace/adk-go/auth/auth_preprocessor.go
package auth

import (
	"fmt"
	"log"
	"strings"
	"time"

	agentiface "github.com/KennethanCeyer/adk-go/agents"
	"github.com/KennethanCeyer/adk-go/agents/invocation"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"

	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions"
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

const defaultAuthInstructionTemplate = `
You have access to a set of tools that may require user authorization.
If you need to use a tool that requires authorization, you must first request authorization by calling the "%s" function.
The function call will return one of the following:
- A success message: "Successfully obtained credential."
- An error message: "Failed to obtain credential. <reason>"
- A message indicating that user interaction is required: "User interaction is required to obtain credential. <auth_url>"
Only if the function call returns a success message, you can then proceed to use the tool that requires authorization.
Otherwise, you must inform the user about the error or the required user interaction.

Current user:
<User>
%s
</User>
`

type AuthLlmRequestProcessor struct{}

// BaseLlmRequestProcessor interface (should be defined elsewhere, e.g., flows/llmflows/processors/interfaces.go)
type BaseLlmRequestProcessor interface {
	RunAsync(invocationCtx *invocation.InvocationContext, llmReq *modelstypes.LlmRequest) (<-chan *eventstypes.Event, error)
}

var _ BaseLlmRequestProcessor = (*AuthLlmRequestProcessor)(nil)

func (p *AuthLlmRequestProcessor) RunAsync(
	invocationCtx *invocation.InvocationContext,
	llmReq *modelstypes.LlmRequest,
) (<-chan *eventstypes.Event, error) {
	outCh := make(chan *eventstypes.Event)

	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || invocationCtx.Session == nil {
			log.Println("AuthLlmRequestProcessor: InvocationContext, Agent, or Session is nil.")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(agentiface.LlmAgent)
		if !ok {
			// invocationCtx.Agent is agentiface.Agent. We need to check if it's also an LlmAgent.
			// This might require a type assertion from the specific AgentInterface used in InvocationContext.
			// For now, assuming InvocationContext.Agent can be directly asserted if it's set with an LlmAgent.
			log.Printf("AuthLlmRequestProcessor: Agent in InvocationContext is not an LlmAgent.")
			return
		}

		authHandler := llmAgent.GetAuthHandler()
		if authHandler == nil {
			return
		}

		authConfigs := authHandler.GetAllAuthConfigs()
		if len(authConfigs) == 0 {
			return
		}

		// Cast session from any to *sessionstypes.Session for GetCredential
		// This relies on AuthHandler's GetCredential taking `any` and handling the cast,
		// or defining GetCredential more specifically. The authtypes.AuthHandler definition
		// uses `any` for session, so it's consistent for now.
		userInfo := p.buildUserInfo(invocationCtx.Session, authConfigs, authHandler)
		authInstruction := p.buildAuthInstruction(userInfo)

		if llmReq.Config == nil {
			llmReq.Config = &modelstypes.GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &modelstypes.Content{Parts: []modelstypes.Part{}}
		}

		var currentSysInstructionText string
		if len(llmReq.Config.SystemInstruction.Parts) > 0 && llmReq.Config.SystemInstruction.Parts[0].Text != nil {
			currentSysInstructionText = *llmReq.Config.SystemInstruction.Parts[0].Text
		}

		if currentSysInstructionText != "" {
			currentSysInstructionText += "\n\n"
		}
		newInstructionText := currentSysInstructionText + authInstruction
		
		if len(llmReq.Config.SystemInstruction.Parts) == 0 {
		    llmReq.Config.SystemInstruction.Parts = append(llmReq.Config.SystemInstruction.Parts, modelstypes.Part{Text: &newInstructionText})
		} else {
		    llmReq.Config.SystemInstruction.Parts[0].Text = &newInstructionText
		}

		if p.isAuthFlowCompletedInPriorTurn(invocationCtx.Session.Events) {
			log.Println("AuthLlmRequestProcessor: Auth flow was recently completed or attempted in a prior turn.")
		}
	}()

	return outCh, nil
}

func (p *AuthLlmRequestProcessor) buildUserInfo(
	session *sessionstypes.Session,
	authConfigs []authtypes.AuthConfig,
	authHandler authtypes.AuthHandler,
) string {
	if session == nil {
		return "User information not available (session is nil)."
	}
	var userInfoParts []string
	userInfoParts = append(userInfoParts, fmt.Sprintf("  User ID: %s", session.UserID))
	userInfoParts = append(userInfoParts, fmt.Sprintf("  App ID: %s", session.AppName))

	for _, authConfig := range authConfigs {
		credential, err := authHandler.GetCredential(authConfig.Name, session) // Pass session directly
		status := "Not authorized"

		if err != nil {
			log.Printf("AuthLlmRequestProcessor: Error getting credential for %s: %v", authConfig.Name, err)
			status = fmt.Sprintf("Error checking authorization for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
		} else {
			if credential.AccessToken != "" {
				if !credential.IsExpired() { // IsExpired defined on authtypes.AuthCredential
					status = fmt.Sprintf("Authorized for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
				} else {
					status = fmt.Sprintf("Authorization expired for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
				}
			} else {
				status = fmt.Sprintf("Not authorized for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
			}
		}
		userInfoParts = append(userInfoParts, "  "+status)
	}
	return strings.Join(userInfoParts, "\n")
}


func (p *AuthLlmRequestProcessor) buildAuthInstruction(userInfo string) string {
	return fmt.Sprintf(defaultAuthInstructionTemplate, functions.REQUEST_EUC_FUNCTION_CALL_NAME, userInfo)
}

func (p *AuthLlmRequestProcessor) isAuthFlowCompletedInPriorTurn(historyEvents []*eventstypes.Event) bool {
	if len(historyEvents) == 0 {
		return false
	}
	const turnsToLookBack = 2
	maxEventsToCheck := turnsToLookBack * 2
	if maxEventsToCheck > len(historyEvents) {
		maxEventsToCheck = len(historyEvents)
	}

	for i := len(historyEvents) - 1; i >= len(historyEvents)-maxEventsToCheck; i-- {
		event := historyEvents[i]
		if event == nil || event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil && part.FunctionResponse.Name == functions.REQUEST_EUC_FUNCTION_CALL_NAME {
				return true
			}
		}
	}
	return false
}

func newEventID() commontypes.EventID {
	return commontypes.EventID(fmt.Sprintf("evt-%d", time.Now().UnixNano()))
}

func getTimestamp() commontypes.Timestamp {
	return commontypes.Timestamp(float64(time.Now().UnixNano()) / 1e9)
}

# /Users/gopher/Desktop/workspace/adk-go/agents/invocation/types/types.go
package types

import (
	"context"

	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

// AgentInterfacePlaceholder is used to avoid import cycle with agents package.
// Replace with actual `agents.Agent` type if structure allows.
type AgentInterfacePlaceholder interface {
	GetName() string
	GetDescription() string
}

// RunConfig holds configuration for a specific agent execution.
type RunConfig struct {
	// Add fields as needed, e.g., MaxToolCalls, Streaming.
}

// InvocationContext provides context for a single agent invocation.
type InvocationContext struct {
	Ctx          context.Context
	InvocationID commontypes.InvocationID
	Agent        AgentInterfacePlaceholder // Use placeholder
	Session      *sessionstypes.Session
	UserContent  *modelstypes.Content
	RunConfig    *RunConfig
}

// ReadonlyContext offers a read-only view of InvocationContext.
type ReadonlyContext struct {
	invocationCtx *InvocationContext
}

// NewReadonlyContext creates a ReadonlyContext.
func NewReadonlyContext(invCtx *InvocationContext) *ReadonlyContext {
	return &ReadonlyContext{invocationCtx: invCtx}
}

// GetSession returns the session from the context.
func (roc *ReadonlyContext) GetSession() *sessionstypes.Session {
	if roc.invocationCtx == nil {
		return nil
	}
	return roc.invocationCtx.Session
}

# /Users/gopher/Desktop/workspace/adk-go/agents/invocation/inovocation.go
package invocation

import (
	"context"

	agentiface "github.com/KennethanCeyer/adk-go/agents"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

type RunConfig struct {
	// Fields for run-specific configurations if needed.
}

type InvocationContext struct {
	Ctx          context.Context
	InvocationID commontypes.InvocationID
	Agent        agentiface.Agent // The agent being invoked
	Session      *sessionstypes.Session
	UserContent  *modelstypes.Content // Initial user input for this specific invocation
	RunConfig    *RunConfig
}

type ReadonlyContext struct {
	invocationCtx *InvocationContext
}

func NewReadonlyContext(invCtx *InvocationContext) *ReadonlyContext {
	return &ReadonlyContext{invocationCtx: invCtx}
}

// Add getter methods for ReadonlyContext if specific fields need to be exposed.

# /Users/gopher/Desktop/workspace/adk-go/agents/base_agent.go
package agents

import (
	"fmt"
	"regexp"
	// Hypothetical Go ADK packages
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/genai/types" // For genai.Content
	// "github.com/KennethanCeyer/adk-go/telemetry"
)

// Forward declarations for types that would be defined in other packages.
// In a real Go ADK, these would be imported.
type Content struct { // Placeholder for genai.types.Content
	Role  string `json:"role,omitempty"`
	Parts []Part `json:"parts,omitempty"`
}

type Part struct { // Placeholder for genai.types.Part
	Text string `json:"text,omitempty"`
	// Other fields like FunctionCall, FunctionResponse, InlineData would be here
}

type Event struct { // Placeholder for events.Event
	InvocationID string       `json:"invocationId,omitempty"`
	Author       string       `json:"author,omitempty"`
	Branch       string       `json:"branch,omitempty"`
	Content      *Content     `json:"content,omitempty"`
	Actions      EventActions `json:"actions,omitempty"` // Placeholder
	ID           string       `json:"id,omitempty"`
	Timestamp    float64      `json:"timestamp,omitempty"`
	// ... other Event fields
}

type EventActions struct { // Placeholder for events.EventActions
	StateDelta map[string]interface{} `json:"stateDelta,omitempty"`
	// ... other EventActions fields
}

type InvocationContext struct { // Placeholder
	Agent            Agent
	Branch           string
	InvocationID     string
	Session          *Session // Placeholder
	EndInvocation    bool
	UserContent      *Content
	ArtifactService  interface{} // Placeholder
	SessionService   interface{} // Placeholder
	MemoryService    interface{} // Placeholder
	LiveRequestQueue interface{} // Placeholder for LiveRequestQueue
	RunConfig        *RunConfig  // Placeholder
}

type Session struct { // Placeholder for sessions.Session
	State map[string]interface{}
	// ... other Session fields
}

type RunConfig struct { // Placeholder for RunConfig
	// ... RunConfig fields
}

type CallbackContext struct { // Placeholder
	InvocationContext *InvocationContext
	EventActions      EventActions
	State             MutableState // Placeholder for a state map that tracks deltas
}

type MutableState map[string]interface{} // Simplified placeholder

func (ms MutableState) HasDelta() bool {
	// In a real implementation, this would check if _delta has items
	return len(ms) > 0 // Simplified
}

// Agent is the base interface for all agents in the Agent Development Kit.
type Agent interface {
	GetName() string
	GetDescription() string
	ParentAgent() Agent
	SetParentAgent(parent Agent) error
	SubAgents() []Agent
	AddSubAgent(subAgent Agent)
	RunAsync(parentCtx *InvocationContext) (<-chan *Event, error)
	RunLive(parentCtx *InvocationContext) (<-chan *Event, error)
	RootAgent() Agent
	FindAgent(name string) Agent
	FindSubAgent(name string) Agent
	GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback
	GetCanonicalAfterAgentCallbacks() []SingleAgentCallback
}

// SingleAgentCallback defines the signature for agent lifecycle callbacks.
type SingleAgentCallback func(callbackCtx *CallbackContext) (*Content, error)

// BaseAgent provides a common structure for agents.
type BaseAgent struct {
	Name                   string                `json:"name"`
	Description            string                `json:"description,omitempty"`
	Parent                 Agent                 `json:"-"` // Exclude from JSON, handle manually
	Subs                   []Agent               `json:"subAgents,omitempty"`
	BeforeAgentCallback    []SingleAgentCallback `json:"-"` // Callbacks are not typically part of serialized state
	AfterAgentCallback     []SingleAgentCallback `json:"-"`
	agentInvocationContext *InvocationContext    // Internal context for the current run
}

// NewBaseAgent creates a new BaseAgent.
// Name must be a valid Go identifier and not "user".
func NewBaseAgent(name string, description string) (*BaseAgent, error) {
	if !isValidIdentifier(name) {
		return nil, fmt.Errorf("invalid agent name: `%s`. Agent name must be a valid identifier", name)
	}
	if name == "user" {
		return nil, fmt.Errorf("agent name cannot be `user`. `user` is reserved for end-user's input")
	}
	return &BaseAgent{
		Name:        name,
		Description: description,
		Subs:        make([]Agent, 0),
	}, nil
}

func (ba *BaseAgent) GetName() string {
	return ba.Name
}

func (ba *BaseAgent) GetDescription() string {
	return ba.Description
}

func (ba *BaseAgent) ParentAgent() Agent {
	return ba.Parent
}

func (ba *BaseAgent) SetParentAgent(parent Agent) error {
	if ba.Parent != nil && ba.Parent != parent {
		return fmt.Errorf("agent `%s` already has a parent agent: `%s`, cannot add to: `%s`", ba.Name, ba.Parent.GetName(), parent.GetName())
	}
	ba.Parent = parent
	return nil
}

func (ba *BaseAgent) SubAgents() []Agent {
	return ba.Subs
}

func (ba *BaseAgent) AddSubAgent(subAgent Agent) {
	if err := subAgent.SetParentAgent(ba); err != nil {
		// Or handle error as appropriate, e.g. log and skip
		fmt.Printf("Error setting parent for sub-agent %s: %v\n", subAgent.GetName(), err)
		return
	}
	ba.Subs = append(ba.Subs, subAgent)
}

// RunAsync is the entry method to run an agent via text-based conversation.
func (ba *BaseAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	// In Go, we'd typically use channels for async generators.
	// The actual implementation of _runAsyncImpl would push events to this channel.
	// The telemetry.tracer logic would wrap the core processing.
	// For this snippet, we'll simulate the channel.

	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run [%s]", ba.Name)):
		
		event, err := ba.handleBeforeAgentCallback(ctx)
		if err != nil {
			// ì–´ë–»ê²Œ ì—ëŸ¬ë¥¼ outChìœ¼ë¡œ ë³´ë‚¼ ì§€ ê³ ë¯¼ í•„ìš”. Eventì— Error í•„ë“œ ì¶”ê°€?
			// For now, log and potentially send an error event.
			fmt.Printf("Error in beforeAgentCallback for %s: %v\n", ba.Name, err)
			// outCh <- &events.Event{Error: err.Error()} // Conceptual
			return
		}
		if event != nil {
			outCh <- event
		}
		if ctx.EndInvocation {
			return
		}

		err = ba.runAsyncImpl(ctx, outCh) // Pass channel to implementation
		if err != nil {
			fmt.Printf("Error in runAsyncImpl for %s: %v\n", ba.Name, err)
			return
		}

		if ctx.EndInvocation {
			return
		}

		event, err = ba.handleAfterAgentCallback(ctx)
		if err != nil {
			fmt.Printf("Error in afterAgentCallback for %s: %v\n", ba.Name, err)
			return
		}
		if event != nil {
			outCh <- event
		}
	}()

	return outCh, nil
}

// runAsyncImpl is the core logic to run this agent via text-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runAsyncImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	// This is where the Python _run_async_impl's `yield Event(...)` would happen.
	// In Go, we send to the channel.
	// Example: outCh <- &events.Event{Author: ba.Name, Content: ...}
	return fmt.Errorf("_runAsyncImpl for %T is not implemented", ba)
}

// RunLive is the entry method to run an agent via video/audio-based conversation.
func (ba *BaseAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run_live [%s]", ba.Name)):
		err := ba.runLiveImpl(ctx, outCh) // Pass channel
		if err != nil {
			fmt.Printf("Error in runLiveImpl for %s: %v\n", ba.Name, err)
			// Potentially send an error event
		}
	}()
	return outCh, nil
}

// runLiveImpl is the core logic to run this agent via video/audio-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runLiveImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	return fmt.Errorf("_runLiveImpl for %T is not implemented", ba)
}

func (ba *BaseAgent) RootAgent() Agent {
	current := ba
	for current.ParentAgent() != nil {
		current = current.ParentAgent().(*BaseAgent) // Assuming BaseAgent for simplicity
	}
	return current
}

func (ba *BaseAgent) FindAgent(name string) Agent {
	if ba.Name == name {
		return ba
	}
	return ba.FindSubAgent(name)
}

func (ba *BaseAgent) FindSubAgent(name string) Agent {
	for _, subAgent := range ba.Subs {
		if found := subAgent.FindAgent(name); found != nil {
			return found
		}
	}
	return nil
}

func (ba *BaseAgent) createInvocationContext(parentCtx *InvocationContext) *InvocationContext {
	// Create a copy and update agent and branch
	// This is a simplified deep copy. Real implementation needs care.
	ctxCopy := *parentCtx
	ctxCopy.Agent = ba
	if parentCtx.Branch != "" {
		ctxCopy.Branch = fmt.Sprintf("%s.%s", parentCtx.Branch, ba.Name)
	} else {
		ctxCopy.Branch = ba.Name
	}
	return &ctxCopy
}

func (ba *BaseAgent) GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback {
	return ba.BeforeAgentCallback
}

func (ba *BaseAgent) GetCanonicalAfterAgentCallbacks() []SingleAgentCallback {
	return ba.AfterAgentCallback
}

func (ba *BaseAgent) handleBeforeAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalBeforeAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalBeforeAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("beforeAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions, // Assuming EventActions is part of CallbackContext
			}
			ctx.EndInvocation = true
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			Actions:      callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

func (ba *BaseAgent) handleAfterAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalAfterAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalAfterAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("afterAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions,
			}
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			// Content might be nil if callback only changed state
			Actions: callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

// isValidIdentifier checks if a string is a valid Go identifier (simplified).
// Go identifiers are more restrictive than Python's.
func isValidIdentifier(name string) bool {
	if name == "" {
		return false
	}
	// Regex for a simplified Go identifier: starts with a letter, followed by letters or digits.
	// Go allows Unicode letters/digits. This regex is simplified.
	// Actual Go spec: letter { letter | unicode_digit } .
	// For ADK agent names, we might enforce stricter ASCII for interoperability or simplicity.
	matched, _ := regexp.MatchString(`^[a-zA-Z_][a-zA-Z0-9_]*$`, name)
	return matched
}

# /Users/gopher/Desktop/workspace/adk-go/agents/llm_agent.go
package agents

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/tools" // Placeholder for tools package
	// "github.com/KennethanCeyer/adk-go/models" // Placeholder for models package
	// "github.com/KennethanCeyer/adk-go/examples" // Placeholder for examples package
	// "github.com/KennethanCeyer/adk-go/planners" // Placeholder for planners package
	// "github.com/KennethanCeyer/adk-go/codeexecutors" // Placeholder for code executors
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, genai.Content etc.
)

// --- Forward declarations / Placeholders ---
// These would be actual types in a full Go ADK.

type BaseLlm struct { // Placeholder for models.BaseLlm
	ModelName string
}

type GenerateContentConfig struct { // Placeholder for genai.GenerateContentConfig
	SystemInstruction   string        `json:"systemInstruction,omitempty"`
	Tools               []Tool        `json:"tools,omitempty"`         // Placeholder for genai.Tool
	ResponseSchema      interface{}   `json:"responseSchema,omitempty"` // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string        `json:"responseMimeType,omitempty"`
	ThinkingConfig      interface{}   `json:"thinkingConfig,omitempty"`   // Placeholder for genai.ThinkingConfig
	Temperature         *float32      `json:"temperature,omitempty"`
	SafetySettings      []interface{} `json:"safetySettings,omitempty"` // Placeholder for genai.SafetySetting
}

type LlmRequest struct { // Placeholder for models.LlmRequest
	Model               string
	Config              *GenerateContentConfig
	Contents            []Content
	ToolsDict           map[string]Tool // Placeholder
	LiveConnectConfig   interface{}     // Placeholder for genai.LiveConnectConfig
	SystemInstruction   string          // Added for simplicity, original ADK appends to config
	OutputSchema        interface{}     // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string
}

func (lr *LlmRequest) AppendInstructions(instructions []string) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	if lr.Config.SystemInstruction != "" {
		lr.Config.SystemInstruction += "\n\n"
	}
	lr.Config.SystemInstruction += strings.Join(instructions, "\n\n")
}

func (lr *LlmRequest) SetOutputSchema(schema interface{}) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	lr.Config.ResponseSchema = schema
	lr.Config.ResponseMIMEType = "application/json"
}

type LlmResponse struct { // Placeholder for models.LlmResponse
	Content         *Content    `json:"content,omitempty"`
	ErrorCode       string      `json:"errorCode,omitempty"`
	ErrorMessage    string      `json:"errorMessage,omitempty"`
	Partial         bool        `json:"partial,omitempty"`
	CustomMetadata  map[string]interface{} `json:"customMetadata,omitempty"`
}


type Tool interface { // Placeholder for tools.BaseTool
	GetName() string
	GetDescription() string
	GetDeclaration() (*ToolDeclaration, error) // Placeholder for genai.FunctionDeclaration
	ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error
}

type ToolDeclaration struct { // Placeholder for genai.FunctionDeclaration
	Name        string      `json:"name"`
	Description string      `json:"description"`
	Parameters  interface{} `json:"parameters"`
}

type FunctionTool struct { // Placeholder for tools.FunctionTool
	BaseToolImpl
	fn interface{}
}

func NewFunctionTool(fn interface{}) *FunctionTool {
	// Simplified name/description extraction
	name := "unknown_function"
	// In Go, reflection for function name/doc is more involved
	// For now, users might need to provide this explicitly if FunctionTool wraps a raw func
	return &FunctionTool{BaseToolImpl: BaseToolImpl{ToolName: name}, fn: fn}
}

func (ft *FunctionTool) GetName() string { return ft.ToolName }
func (ft *FunctionTool) GetDescription() string { return ft.ToolDescription }
func (ft *FunctionTool) GetDeclaration() (*ToolDeclaration, error) { /* TODO */ return nil, nil }
func (ft *FunctionTool) ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error { /* TODO */ return nil }


type BaseToolset interface { // Placeholder for tools.BaseToolset
	GetTools(readonlyCtx *ReadonlyContext) ([]Tool, error)
}

type Example struct { // Placeholder for examples.Example
	Input  Content   `json:"input"`
	Output []Content `json:"output"`
}

type BaseExampleProvider interface { // Placeholder for examples.BaseExampleProvider
	GetExamples(query string) ([]Example, error)
}

type Planner interface { // Placeholder for planners.BasePlanner
	// Methods would be defined here
}

type CodeExecutor interface { // Placeholder for codeexecutors.BaseCodeExecutor
	// Methods would be defined here
}

type ToolContext struct { // Placeholder for tools.ToolContext
	InvocationContext *InvocationContext
	State             MutableState
	// ... other fields like FunctionCallID, EventActions
}

// Represents ReadonlyContext
type ReadonlyContext struct {
	InvocationContext *InvocationContext
}

func (rc *ReadonlyContext) GetState() map[string]interface{} {
	// Return a read-only copy or wrapper
	// For simplicity, returning the direct map for now, assuming no modification.
	if rc.InvocationContext != nil && rc.InvocationContext.Session != nil {
		return rc.InvocationContext.Session.State
	}
	return make(map[string]interface{})
}


// LLMProvider interface (simplified from previous example)
type LLMProvider interface {
    GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error)
    GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error)
    // Potentially a Connect method for bidirectional streaming if needed for live mode
}

// MockLLMProvider for LlmAgent
type MockLLM struct {
	BaseLlm
}

func (m *MockLLM) GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error) {
	// Simulate LLM call
	log.Printf("MockLLM GenerateContentStream called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	ch := make(chan *LlmResponse, 1)
	go func() {
		defer close(ch)
		// Simplified: just echo back part of the first user message if available
		responseText := "Default mock response"
		if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
			responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
		}
		ch <- &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}
	}()
	return ch, nil
}

func (m *MockLLM) GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error) {
	log.Printf("MockLLM GenerateContent called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	responseText := "Default mock response"
	if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
		responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
	}
	return &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}, nil
}


// SingleBeforeModelCallback defines the signature for a single before-model callback.
type SingleBeforeModelCallback func(callbackCtx *CallbackContext, llmReq *LlmRequest) (*LlmResponse, error)

// SingleAfterModelCallback defines the signature for a single after-model callback.
type SingleAfterModelCallback func(callbackCtx *CallbackContext, llmResp *LlmResponse) (*LlmResponse, error)

// SingleBeforeToolCallback defines the signature for a single before-tool callback.
type SingleBeforeToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext) (map[string]interface{}, error)

// SingleAfterToolCallback defines the signature for a single after-tool callback.
type SingleAfterToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext, toolResponse map[string]interface{}) (map[string]interface{}, error)

// InstructionProvider defines a function that provides an instruction string.
type InstructionProvider func(readonlyCtx *ReadonlyContext) (string, error)

// ToolUnion represents a type that can be a callable, a BaseTool, or a BaseToolset.
// In Go, this would typically be handled by interfaces or by checking types at runtime.
// For this conceptual translation, we'll use `interface{}` and runtime type assertions.
type ToolUnion interface{}

// ExamplesUnion represents types that can provide examples.
type ExamplesUnion interface{} // list[Example] or BaseExampleProvider

// LlmAgent is an LLM-based Agent.
type LlmAgent struct {
	BaseAgent
	AgentModel                  interface{} // string or *BaseLlm (actual model instance)
	AgentInstruction            interface{} // string or InstructionProvider
	AgentGlobalInstruction      interface{} // string or InstructionProvider
	AgentTools                  []ToolUnion
	AgentGenerateContentConfig  *GenerateContentConfig
	DisallowTransferToParent    bool
	DisallowTransferToPeers     bool
	IncludeContents             string // "default" or "none"
	InputSchema                 interface{} // reflect.Type of a struct, or nil
	OutputSchema                interface{} // reflect.Type of a struct, or nil
	OutputKey                   string
	AgentPlanner                Planner
	AgentCodeExecutor           CodeExecutor
	AgentExamples               ExamplesUnion
	AgentBeforeModelCallbacks   []SingleBeforeModelCallback
	AgentAfterModelCallbacks    []SingleAfterModelCallback
	AgentBeforeToolCallbacks    []SingleBeforeToolCallback
	AgentAfterToolCallbacks     []SingleAfterToolCallback

	// Internal flow strategy
	llmFlow LlmFlow
}

// LlmFlow defines the behavior of the LLM agent.
type LlmFlow interface {
	RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error)
	RunLive(invocationCtx *InvocationContext) (<-chan *Event, error)
}

// NewLlmAgent creates a new LlmAgent.
// This constructor attempts to mirror Pydantic's initialization logic.
func NewLlmAgent(name, description string, model interface{}, instruction interface{}) (*LlmAgent, error) {
	base, err := NewBaseAgent(name, description)
	if err != nil {
		return nil, err
	}

	agent := &LlmAgent{
		BaseAgent:        *base,
		AgentModel:       model,
		AgentInstruction: instruction,
		IncludeContents:  "default",
		// Default to AutoFlow which includes SingleFlow + agent transfer
		llmFlow: newAutoFlow(), // Placeholder for flow initialization
	}

	// Simulating Pydantic's model_post_init and field_validator logic
	if err := agent.validateAndProcessConfig(); err != nil {
		return nil, err
	}

	return agent, nil
}

func (a *LlmAgent) validateAndProcessConfig() error {
	if err := a.checkOutputSchema(); err != nil {
		return err
	}
	if err := a.validateGenerateContentConfig(); err != nil {
		return err
	}
	// In Python, Pydantic's model_post_init calls __set_parent_agent_for_sub_agents
	// This is handled by AddSubAgent in Go's BaseAgent.

	// Determine flow based on transfer disallowed flags
	if a.DisallowTransferToParent && a.DisallowTransferToPeers && len(a.Subs) == 0 {
		a.llmFlow = newSingleFlow() // Placeholder
	} else {
		a.llmFlow = newAutoFlow() // Placeholder
	}

	return nil
}


// RunAsync implements the Agent interface.
// It delegates to the internal LlmFlow.
func (a *LlmAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx) // from BaseAgent
	a.agentInvocationContext = ctx // Store context for this agent's run
	return a.llmFlow.RunAsync(ctx)
}

// RunLive implements the Agent interface.
func (a *LlmAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx)
	a.agentInvocationContext = ctx
	return a.llmFlow.RunLive(ctx)
}


func (a *LlmAgent) CanonicalModel() (BaseLlm, error) {
	if modelInst, ok := a.AgentModel.(BaseLlm); ok {
		return modelInst, nil
	}
	if modelStr, ok := a.AgentModel.(string); ok && modelStr != "" {
		// Placeholder for LLMRegistry.NewLlm(modelStr)
		// In a real Go ADK, this would look up and instantiate the model.
		return MockLLM{BaseLlm{ModelName: modelStr}}, nil
	}

	current := a.ParentAgent()
	for current != nil {
		if llmAgent, ok := current.(*LlmAgent); ok {
			return llmAgent.CanonicalModel()
		}
		current = current.ParentAgent()
	}
	return MockLLM{}, fmt.Errorf("no model found for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentInstruction.(func(ctx *ReadonlyContext) (string, error)); ok { // Allow func type as well
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, fmt.Errorf("invalid instruction type for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentGlobalInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentGlobalInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentGlobalInstruction.(func(ctx *ReadonlyContext) (string, error)); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, nil // Global instruction can be empty
}

func (a *LlmAgent) CanonicalTools(ctx *ReadonlyContext) ([]Tool, error) {
	var resolvedTools []Tool
	for _, toolUnion := range a.AgentTools {
		switch t := toolUnion.(type) {
		case Tool:
			resolvedTools = append(resolvedTools, t)
		case func(interface{}) interface{}: // Simplified check for a callable
			// This is a very basic check. Robustly handling arbitrary callables
			// and converting them to tools.Tool would require more reflection
			// or specific registration mechanisms.
			// For now, wrapping it in a conceptual FunctionTool.
			resolvedTools = append(resolvedTools, NewFunctionTool(t))
		case BaseToolset:
			toolsFromSet, err := t.GetTools(ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to get tools from toolset: %w", err)
			}
			resolvedTools = append(resolvedTools, toolsFromSet...)
		default:
			return nil, fmt.Errorf("unsupported tool type: %T", t)
		}
	}
	return resolvedTools, nil
}


func (a *LlmAgent) GetCanonicalBeforeModelCallbacks() []SingleBeforeModelCallback {
	return a.AgentBeforeModelCallbacks
}

func (a *LlmAgent) GetCanonicalAfterModelCallbacks() []SingleAfterModelCallback {
	return a.AgentAfterModelCallbacks
}

func (a *LlmAgent) GetCanonicalBeforeToolCallbacks() []SingleBeforeToolCallback {
	return a.AgentBeforeToolCallbacks
}

func (a *LlmAgent) GetCanonicalAfterToolCallbacks() []SingleAfterToolCallback {
	return a.AgentAfterToolCallbacks
}

func (a *LlmAgent) maybeSaveOutputToState(event *Event) {
	if a.OutputKey == "" || !eventIsFinalResponse(event) || event.Content == nil || len(event.Content.Parts) == 0 {
		return
	}

	var resultData interface{}
	var combinedText []string
	for _, part := range event.Content.Parts {
		if part.Text != "" {
			combinedText = append(combinedText, part.Text)
		}
		// In a real scenario, you'd handle other part types like function calls/responses if they constituted the "output"
	}
	fullText := strings.Join(combinedText, "\n")


	if a.OutputSchema != nil {
		// In Go, Pydantic's model_validate_json would be equivalent to json.Unmarshal into a struct.
		// This requires a.OutputSchema to be a reflect.Type of the target struct.
		// For simplicity, this placeholder assumes fullText is valid JSON for the schema.
		// A real implementation would use json.Unmarshal(byte(fullText), &targetStructInstance)
		// and then potentially convert targetStructInstance to map[string]interface{} if needed.
		var tempMap map[string]interface{}
		// This is a simplified placeholder. Real unmarshalling and validation needed.
		if err := json.Unmarshal([]byte(fullText), &tempMap); err == nil {
			resultData = tempMap
		} else {
			log.Printf("Warning: Failed to parse output for key '%s' against schema: %v. Storing raw text.", a.OutputKey, err)
			resultData = fullText // Fallback to raw text
		}
	} else {
		resultData = fullText
	}

	if event.Actions.StateDelta == nil {
		event.Actions.StateDelta = make(map[string]interface{})
	}
	event.Actions.StateDelta[a.OutputKey] = resultData
}

func (a *LlmAgent) checkOutputSchema() error {
	if a.OutputSchema == nil {
		return nil
	}

	if !a.DisallowTransferToParent || !a.DisallowTransferToPeers {
		log.Println(
			fmt.Sprintf("Warning: Invalid config for agent %s: outputSchema cannot co-exist with agent transfer configurations. Setting disallowTransferToParent=true, disallowTransferToPeers=true", a.Name),
		)
		a.DisallowTransferToParent = true
		a.DisallowTransferToPeers = true
	}

	if len(a.SubAgents()) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, subAgents must be empty to disable agent transfer", a.Name)
	}

	if len(a.AgentTools) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, tools must be empty", a.Name)
	}
	return nil
}

func (a *LlmAgent) validateGenerateContentConfig() error {
	if a.AgentGenerateContentConfig == nil {
		a.AgentGenerateContentConfig = &GenerateContentConfig{} // Ensure it's not nil
		return nil
	}
	if a.AgentGenerateContentConfig.ThinkingConfig != nil {
		return fmt.Errorf("thinkingConfig should be set via LlmAgent.Planner")
	}
	if len(a.AgentGenerateContentConfig.Tools) > 0 {
		return fmt.Errorf("all tools must be set via LlmAgent.Tools")
	}
	if a.AgentGenerateContentConfig.SystemInstruction != "" {
		return fmt.Errorf("system instruction must be set via LlmAgent.Instruction or LlmAgent.GlobalInstruction")
	}
	if a.AgentGenerateContentConfig.ResponseSchema != nil {
		return fmt.Errorf("response schema must be set via LlmAgent.OutputSchema")
	}
	return nil
}

// Helper, would be part of event logic
func eventIsFinalResponse(e *Event) bool {
	if e.Actions.StateDelta["skipSummarization"] == true { // Assuming skipSummarization is stored in StateDelta
		return true
	}
	// Simplified: In Python ADK, this checks for function calls/responses and partial flags.
	// Here, we'll assume an event is final if it's not marked partial and has content.
	return e.Content != nil && !e.Partial // Placeholder for e.Partial
}

// --- Placeholder Flow Implementations ---
// In a real Go ADK, these would be more fleshed out and likely in their own package.

type SingleFlow struct {
	// RequestProcessors  []BaseLlmRequestProcessor  // Placeholder
	// ResponseProcessors []BaseLlmResponseProcessor // Placeholder
}

func newSingleFlow() *SingleFlow {
	// Initialize processors
	return &SingleFlow{}
}

func (sf *SingleFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)
		llmReq := &LlmRequest{} // Initialize LlmRequest

		// Simulate preprocessing (would iterate through sf.RequestProcessors)
		if err := sf.preprocessAsync(invocationCtx, llmReq); err != nil {
			log.Printf("Error during preprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()} // Conceptual error event
			return
		}

		llmAgent, ok := invocationCtx.Agent.(*LlmAgent)
		if !ok {
			log.Printf("Error: agent is not an LlmAgent")
			return
		}
		llm, err := llmAgent.CanonicalModel()
		if err != nil {
			log.Printf("Error getting canonical model: %v", err)
			return
		}
		
		// Simplified LLM call for SingleFlow
		// A real implementation would handle streaming, function calling loops, etc.
		var llmResponse *LlmResponse

		// This is a simplification. The Python ADK has a loop for multiple LLM calls if tools are used.
		// For this Go conceptual translation, we'll assume a single LLM call or a very simplified tool use.
		if provider, ok := llm.(LLMProvider); ok {
			// Use GenerateContent for non-streaming for simplicity in this conceptual flow
			llmResponse, err = provider.GenerateContent(context.Background(), llmReq)
			if err != nil {
				log.Printf("Error calling LLM: %v", err)
				// outCh <- &Event{Error: err.Error()}
				return
			}
		} else {
			log.Printf("Error: LLM model does not implement LLMProvider interface")
			return
		}
		

		modelResponseEvent := &Event{
			ID:            newID(), // Generate new event ID
			InvocationID:  invocationCtx.InvocationID,
			Author:        invocationCtx.Agent.GetName(),
			Branch:        invocationCtx.Branch,
			Content:       llmResponse.Content, // Simplified
			// Actions, ErrorCode, ErrorMessage, etc. would be populated
		}

		// Simulate postprocessing (would iterate through sf.ResponseProcessors)
		if err := sf.postprocessAsync(invocationCtx, llmReq, llmResponse, modelResponseEvent, outCh); err != nil {
			log.Printf("Error during postprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()}
			return
		}
		
		// If not handled by post-processors (e.g. tool calls), yield the direct LLM response event
		if modelResponseEvent.Content != nil || modelResponseEvent.Actions.StateDelta != nil { // Yield if there's something to yield
			outCh <- modelResponseEvent
		}

	}()
	return outCh, nil
}


func (sf *SingleFlow) RunLive(invocationCtx *InvocationContext) (<-chan *Event, error) {
	// Simplified RunLive, a real one would be more complex with bidirectional streaming
	return sf.RunAsync(invocationCtx) // Fallback for now
}

func (sf *SingleFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	// Conceptual: Iterate through request_processors from Python ADK
	// Example: basic.request_processor, instructions.request_processor, etc.
	// For this Go version, we'll call helper methods directly for some core logic.
	agent := invocationCtx.Agent.(*LlmAgent) // Assuming LlmAgent

	// Basic processor logic
	modelInstance, err := agent.CanonicalModel()
	if err != nil {
		return err
	}
	llmReq.Model = modelInstance.ModelName // Assuming BaseLlm has ModelName
	if agent.AgentGenerateContentConfig != nil {
		cfgCopy := *agent.AgentGenerateContentConfig
		llmReq.Config = &cfgCopy
	} else {
		llmReq.Config = &GenerateContentConfig{}
	}
	if agent.OutputSchema != nil {
		llmReq.SetOutputSchema(agent.OutputSchema)
	}
	// ... (add live_connect_config population from RunConfig)

	// Instructions processor logic
	// Global Instruction
	rootAgent := agent.RootAgent().(*LlmAgent) // Assuming LlmAgent
	if rootAgent.AgentGlobalInstruction != nil {
		instr, _, err := rootAgent.CanonicalGlobalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		// Simplified: In Python, instructions_utils.inject_session_state is used.
		// For Go, this would involve a similar template processing.
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	// Agent Instruction
	if agent.AgentInstruction != nil {
		instr, _, err := agent.CanonicalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	
	// Identity processor
	var si []string
	si = append(si, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName()))
	if agent.GetDescription() != "" {
		si = append(si, fmt.Sprintf(" The description about you is \"%s\"", agent.GetDescription()))
	}
	llmReq.AppendInstructions(si)

	// Contents processor logic (simplified)
	if agent.IncludeContents != "none" {
		llmReq.Contents = getContentsForLlm(invocationCtx.Branch, invocationCtx.Session.Events, agent.GetName())
	}

	// Tool processing (adding declarations to llmReq.Config.Tools)
	tools, err := agent.CanonicalTools(&ReadonlyContext{InvocationContext: invocationCtx})
	if err != nil { return err}
	if len(tools) > 0 {
		if llmReq.Config.Tools == nil {
			llmReq.Config.Tools = make([]Tool, 0)
		}
	}
	for _, tool := range tools {
		// Simplified: directly appending, Python ADK has more complex logic for FunctionDeclaration
		llmReq.Config.Tools = append(llmReq.Config.Tools, tool) // This assumes tool itself is the declaration or can provide it
		
		// Store tool in llmReq.ToolsDict for later execution lookup by name
		if llmReq.ToolsDict == nil {
			llmReq.ToolsDict = make(map[string]Tool)
		}
		llmReq.ToolsDict[tool.GetName()] = tool
	}

	return nil
}

func (sf *SingleFlow) postprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest, llmResp *LlmResponse, modelResponseEvent *Event, outCh chan<- *Event) error {
	// Conceptual: Iterate through response_processors
	// Example: _nl_planning.response_processor, _code_execution.response_processor
	
	// Handle function calls (simplified from functions.py)
	if llmResp.Content != nil {
		var functionCalls []struct{ Name string; Args map[string]interface{}; ID string } // Simplified representation
		for _, part := range llmResp.Content.Parts {
			// Placeholder: In Python this uses part.function_call
			// For Go, we'd check a similar field if LlmResponse.Content.Parts had FunctionCall type
			// For this example, assume FunctionCall is identified by a specific structure in Part
			// For now, this logic is highly simplified and conceptual
			if strings.HasPrefix(part.Text, "CALL_TOOL:") { // Very basic trigger
				parts := strings.SplitN(strings.TrimPrefix(part.Text, "CALL_TOOL:"), "(", 2)
				name := parts[0]
				argsStr := ""
				if len(parts) > 1 {
					argsStr = strings.TrimSuffix(parts[1], ")")
				}
				var args map[string]interface{}
				json.Unmarshal([]byte(argsStr), &args) // Simplified arg parsing
				functionCalls = append(functionCalls, struct{ Name string; Args map[string]interface{}; ID string }{Name: name, Args: args, ID: newID()})

				// Populate client function call ID if LLM didn't provide one
				modelResponseEvent.Content.Parts[0].Text = "" // Clear text part conceptually
				// modelResponseEvent.Content.Parts[0].FunctionCall = &genai.FunctionCall{Name: name, Args: args, ID: ...}
			}
		}

		if len(functionCalls) > 0 {
			// In Python ADK, this calls functions.handle_function_calls_async
			// which then calls tool.run_async and creates a response event.
			// This is a very complex part involving callbacks, state updates, etc.
			// Here's a highly simplified conceptual flow:
			var toolResponseParts []Part
			for _, fc := range functionCalls {
				tool, ok := llmReq.ToolsDict[fc.Name]
				if !ok {
					log.Printf("Tool %s not found", fc.Name)
					toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Error: Tool %s not found", fc.Name)})
					continue
				}
				// Conceptual tool execution
				// toolResult, err := tool.Execute(context.Background(), fc.Args) // This is complex
				// Simplified:
				toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Tool %s called with %v (mocked)", fc.Name, fc.Args)})
			}
			
			// Create a new event for tool responses and send it
			toolResponseEvent := &Event{
				ID:            newID(),
				InvocationID:  invocationCtx.InvocationID,
				Author:        invocationCtx.Agent.GetName(),
				Branch:        invocationCtx.Branch,
				Content:       &Content{Role: "model", Parts: toolResponseParts}, // Simplified: role should be 'user' for function response in Gemini
			}
			outCh <- toolResponseEvent
			
			// The original LLM response event (modelResponseEvent) that *contained* the function call
			// is also yielded. Then, the flow would typically make *another* LLM call with the tool responses.
			// For simplicity, we will stop here in this conceptual snippet.
			// The python ADK has a loop in _run_one_step_async for this.
			modelResponseEvent.Content = nil // After tool call, the original content is usually superseded or followed by tool response processing
		}
	}
	
	return nil
}

type AutoFlow struct {
	SingleFlow
	// agentTransferRequestProcessor // Placeholder for agent_transfer.request_processor
}

func newAutoFlow() *AutoFlow {
	return &AutoFlow{SingleFlow: *newSingleFlow()}
}

func (af *AutoFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	if err := af.SingleFlow.preprocessAsync(invocationCtx, llmReq); err != nil {
		return err
	}
	// Agent transfer logic
	agent := invocationCtx.Agent.(*LlmAgent)
	transferTargets := getTransferTargets(agent)
	if len(transferTargets) > 0 {
		llmReq.AppendInstructions([]string{buildTargetAgentsInstructions(agent, transferTargets)})
		// Conceptually add transfer_to_agent tool
		// transferTool := NewFunctionTool(transferToAgent) // transferToAgent needs to be defined
		// ... add transferTool to llmReq.Config.Tools and llmReq.ToolsDict
	}
	return nil
}

// Helper to simulate Python's list comprehension and filtering for contents
func getContentsForLlm(currentBranch string, allEvents []*Event, agentName string) []Content {
	var contents []Content
	// This is a simplified version of Python ADK's _get_contents, which has complex logic
	// for rearranging events, handling async function responses, and filtering by branch.
	for _, event := range allEvents {
		if event.Content != nil && len(event.Content.Parts) > 0 {
			// Simplified branch check and foreign event conversion
			if isEventOnBranch(currentBranch, event) && !isAuthEvent(event) {
				if isOtherAgentReply(agentName, event) {
					contents = append(contents, *convertForeignEvent(event))
				} else {
					// Deep copy content before modifying (e.g. removing client func call ID)
					contentCopy := *event.Content 
					// removeClientFunctionCallID(&contentCopy) // Placeholder for actual logic
					contents = append(contents, contentCopy)
				}
			}
		}
	}
	return contents
}

// Simplified placeholders for helper functions from Python's contents.py
func isEventOnBranch(invocationBranch string, event *Event) bool { 
	if invocationBranch == "" || event.Branch == "" { return true }
	return strings.HasPrefix(invocationBranch, event.Branch)
}
func isAuthEvent(event *Event) bool { return false /* TODO */ }
func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return event.Author != currentAgentName && event.Author != "user"
}
func convertForeignEvent(event *Event) *Content { 
	// Simplified conversion
	return &Content{Role: "user", Parts: []Part{{Text: fmt.Sprintf("Context from %s: %s", event.Author, event.Content.Parts[0].Text)}}}
}
func newID() string { return "temp-id" } // Placeholder for ID generation

func getTransferTargets(agent *LlmAgent) []Agent {
    var targets []Agent
    targets = append(targets, agent.SubAgents()...) // Assuming SubAgents returns []Agent

    if agent.ParentAgent() != nil {
        if llmParent, ok := agent.ParentAgent().(*LlmAgent); ok {
            if !agent.DisallowTransferToParent {
                targets = append(targets, llmParent)
            }
            if !agent.DisallowTransferToPeers {
                for _, peerAgent := range llmParent.SubAgents() {
                    if peerAgent.GetName() != agent.GetName() {
                        targets = append(targets, peerAgent)
                    }
                }
            }
        }
    }
    return targets
}

func buildTargetAgentsInfo(targetAgent Agent) string {
    return fmt.Sprintf("\nAgent name: %s\nAgent description: %s\n", targetAgent.GetName(), targetAgent.GetDescription())
}

func buildTargetAgentsInstructions(agent *LlmAgent, targetAgents []Agent) string {
    var sb strings.Builder
    sb.WriteString("\nYou have a list of other agents to transfer to:\n")
    for _, target := range targetAgents {
        sb.WriteString(buildTargetAgentsInfo(target))
    }
    sb.WriteString("\nIf you are the best to answer the question according to your description, you can answer it.")
    sb.WriteString("\nIf another agent is better for answering the question according to its description, call `transfer_to_agent` function to transfer the question to that agent. When transferring, do not generate any text other than the function call.\n")

    if agent.ParentAgent() != nil {
        sb.WriteString(fmt.Sprintf("\nYour parent agent is %s. If neither the other agents nor you are best for answering the question according to the descriptions, transfer to your parent agent. If you don't have parent agent, try answer by yourself.\n", agent.ParentAgent().GetName()))
    }
    return sb.String()
}

// Placeholder for instruction template processing
func processInstructionTemplate(template string, ctx *ReadonlyContext) (string, error) {
	// In Python, this uses string.Formatter with a custom class to access state.
	// In Go, this would require a more explicit template engine or string replacement.
	// This is a very simplified version.
	processed := template
	// Regex to find {key} or {key?}
	re := regexp.MustCompile(`{([^}]+)}`)
	state := ctx.GetState() // Assuming GetState returns map[string]interface{}

	processed = re.ReplaceAllStringFunc(processed, func(match string) string {
		keyWithOptionalMarker := strings.Trim(match, "{} ")
		key := strings.TrimSuffix(keyWithOptionalMarker, "?")
		isOptional := strings.HasSuffix(keyWithOptionalMarker, "?")

		// Handle artifact.filename format
		if strings.HasPrefix(key, "artifact.") {
			// artifactName := strings.TrimPrefix(key, "artifact.")
			// artifactContent, err := loadArtifact(ctx.InvocationContext, artifactName) // Placeholder
			// if err != nil {
			// 	if isOptional { return "" }
			// 	log.Printf("Error loading artifact %s: %v", artifactName, err)
			// 	return match // return original on error
			// }
			// return artifactContent
			return "[ARTIFACT_CONTENT_PLACEHOLDER]" // Simplified
		}

		val, ok := state[key]
		if ok {
			return fmt.Sprintf("%v", val)
		}
		if isOptional {
			return ""
		}
		log.Printf("Warning: Context variable not found and not optional: `%s`", key)
		return match // Return original placeholder if not found and not optional
	})
	return processed, nil
}

# /Users/gopher/Desktop/workspace/adk-go/agents/interfaces/interfaces.go
package agents

import (
	"context"

	llmprovideriface "github.com/KennethanCeyer/adk-go/llmproviders"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools"
)

type Agent interface {
	GetName() string
	GetDescription() string
}

// LlmAgent defines agents that interact with LLMs.
type LlmAgent interface {
	Agent // Embeds base Agent capabilities
	GetModelIdentifier() string
	GetSystemInstruction() *modelstypes.Content
	GetTools() []toolstypes.Tool
	GetLLMProvider() llmprovideriface.LLMProvider

	Process(
		ctx context.Context,
		history []modelstypes.Content, // Conversation history (user, model, function turns)
		latestContent modelstypes.Content, // Latest input (user message or function response)
	) (*modelstypes.Content, error) // Agent's response (model text or next function call)
}

# /Users/gopher/Desktop/workspace/adk-go/models/types/types.go
package types

type Part struct {
	Text             *string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
}

type Content struct {
	Parts []Part
	Role  string // e.g., "user", "model"
}

type FunctionCall struct {
	Name string
	Args any // Typically map[string]any
}

type FunctionResponse struct {
	Name     string
	Response any // Typically map[string]any
}

type GenerateContentConfig struct {
	Temperature     *float32
	TopP            *float32
	TopK            *int32
	MaxOutputTokens *int32
	StopSequences   []string
	// SystemInstruction can be part of LlmRequest or set on the LLM client directly
}

type LlmRequest struct {
	ModelIdentifier string
	SystemInstruction *Content // Optional system instruction
	Contents        []Content // Conversation history + current message
	Config          *GenerateContentConfig
	// Tools are typically configured on the LLM client or passed to the GenerateContent call
}

type LlmResponse struct {
	Content *Content
	// Other fields like FinishReason, TokenCount can be added
}

# /Users/gopher/Desktop/workspace/adk-go/README.md
# ADK-Go: Agent Development Kit in Go (Runnable HelloWorld Example)

This repository provides a foundational, runnable "Hello World" agent example, demonstrating core Agent Development Kit (ADK) concepts ported from Python to Go. It adheres to the project structure `github.com/KennethanCeyer/adk-go`.

The primary goal of this example is to provide a working, minimal agent that interacts with a Large Language Model (LLM). More complex features and modules from the Python ADK can be incrementally built upon this base.

## Project Structure

```plaintext
adk-go/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ helloworld_runner/
â”‚       â””â”€â”€ main.go
â”œâ”€â”€ adk/
â”‚   â”œâ”€â”€ agent.go
â”‚   â”œâ”€â”€ runner.go
â”‚   â””â”€â”€ types.go
â”œâ”€â”€ llmproviders/
â”‚   â””â”€â”€ gemini.go
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ rolldie.go
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ helloworld/
â”‚       â””â”€â”€ agent.go
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ README.md
```

## Prerequisites

1.  **Go**: Version 1.20 or later. ([Installation Guide](https://go.dev/doc/install))
2.  **Google Cloud Project**: A Google Cloud Project with the Vertex AI API (or Generative Language API through Google AI Studio) enabled.
3.  **API Key**: An API key for Google Gemini.
    - **Important**: Secure your API key. Use environment variables.
4.  **Environment Variable**: Set the `GEMINI_API_KEY` environment variable:
    ```bash
    export GEMINI_API_KEY="YOUR_API_KEY_HERE"
    ```

## Setup & Running the HelloWorld Agent

1.  **Create Project Directory:**

    ```bash
    mkdir adk-go
    cd adk-go
    ```

2.  **Initialize Go Module:**

    ```bash
    go mod init [github.com/KennethanCeyer/adk-go](https://github.com/KennethanCeyer/adk-go)
    ```

3.  **Install Dependencies:**

    ```bash
    go get [github.com/google/generative-ai-go/genai](https://github.com/google/generative-ai-go/genai)
    go get google.golang.org/api/iterator # Usually a dependency of genai
    go get google.golang.org/api/option   # Usually a dependency of genai
    ```

4.  **Create Code Files:**
    Create the directories and `.go` files as listed in the "Project Structure" section. Copy the Go code provided in the "Code Implementation" section below into these files.

5.  **Set `GEMINI_API_KEY` Environment Variable.**

6.  **Run the HelloWorld Agent:**
    From the `adk-go` root directory:

    ```bash
    go run ./cmd/helloworld_runner/main.go
    ```

7.  **Interact:**
    The runner will prompt for input. Try:
    - `Hello`
    - `Can you roll a 6-sided die for me?`
    - `Roll a d20`
    - Type `exit` or `quit` to terminate.

# /Users/gopher/Desktop/workspace/adk-go/adk/types/types.go
package types

type Message struct {
	Role  string `json:"role"`
	Parts []Part `json:"parts"`
}

type Part struct {
	Text             *string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
}

type FunctionCall struct {
	Name string
	Args any
}

type FunctionResponse struct {
	Name     string
	Response any
}

# /Users/gopher/Desktop/workspace/adk-go/adk/runner.go
package adk

import (
	"bufio"
	"context"
	"fmt"
	"log"
	"os"
	"strings"

	adk_types "github.com/KennethanCeyer/adk-go/adk/types"
)

type SimpleCLIRunner struct {
	AgentToRun Agent
}

func NewSimpleCLIRunner(agent Agent) (*SimpleCLIRunner, error) {
	if agent == nil {
		return nil, fmt.Errorf("agent cannot be nil")
	}
	return &SimpleCLIRunner{AgentToRun: agent}, nil
}

func (r *SimpleCLIRunner) Start(ctx context.Context) {
	fmt.Printf("--- Starting Agent: %s ---\n", r.AgentToRun.Name())
	fmt.Println("Type 'exit' or 'quit' to stop.")

	var history []adk_types.Message
	scanner := bufio.NewScanner(os.Stdin)

	for {
		select {
		case <-ctx.Done():
			log.Println("Runner context cancelled, shutting down.")
			return
		default:
		}

		fmt.Print("[user]: ")
		if !scanner.Scan() {
			if err := scanner.Err(); err != nil {
				log.Printf("Error reading input: %v", err)
			}
			return // EOF or error
		}
		userInputText := strings.TrimSpace(scanner.Text())

		if strings.ToLower(userInputText) == "exit" || strings.ToLower(userInputText) == "quit" {
			fmt.Println("Exiting agent.")
			return
		}
		if userInputText == "" {
			continue
		}

		userMessage := adk_types.Message{Role: "user", Parts: []adk_types.Part{{Text: &userInputText}}}
		
		currentHistoryForCall := make([]adk_types.Message, len(history))
		copy(currentHistoryForCall, history)

		agentResponse, err := r.AgentToRun.Process(ctx, currentHistoryForCall, userMessage)
		if err != nil {
			log.Printf("Error from agent %s: %v", r.AgentToRun.Name(), err)
			fmt.Printf("[%s-error]: Sorry, I encountered an error: %v\n", r.AgentToRun.Name(), err)
			// Decide if user message causing error should be added to history
			// history = append(history, userMessage)
			// errorMsgForHist := fmt.Sprintf("System error processing last input: %v", err)
			// history = append(history, Message{Role:"model", Parts:[]Part{{Text: &errorMsgForHist}}})
			continue
		}

		history = append(history, userMessage)
		history = append(history, agentResponse)

		var responseTexts []string
		for _, part := range agentResponse.Parts {
			if part.Text != nil {
				responseTexts = append(responseTexts, *part.Text)
			}
			// The agent's Process method should handle FCs internally and not return them directly to runner
			if part.FunctionCall != nil {
				responseTexts = append(responseTexts, fmt.Sprintf("[INFO: Agent returned a raw FunctionCall: %s]", part.FunctionCall.Name))
			}
		}
		fmt.Printf("[%s]: %s\n", r.AgentToRun.Name(), strings.Join(responseTexts, "\n"))

		const maxHistoryItems = 20
		if len(history) > maxHistoryItems {
			history = history[len(history)-maxHistoryItems:]
		}
	}
}

# /Users/gopher/Desktop/workspace/adk-go/adk/agent.go
package adk

import (
	"context"
	"fmt"

	adk_types "github.com/KennethanCeyer/adk-go/adk/types"
)

// Tool defines the interface for any tool an agent can use.
type Tool interface {
	Name() string
	Description() string
	// Parameters should return a structure that can be converted to a JSON schema
	// compatible with the LLM provider (e.g., a map[string]any or a *genai.Schema).
	Parameters() any
	Execute(ctx context.Context, args any) (any, error)
}

// LLMProvider defines the interface for interacting with a Large Language Model.
type LLMProvider interface {
	GenerateContent(
		ctx context.Context,
		modelName string,
		systemInstruction string,
		tools []Tool,
		history []adk_types.Message, // Full conversation history up to the point before latestMessage
		latestMessage adk_types.Message, // The very latest message to process (user input or tool response)
	) (adk_types.Message, error)
}

// Agent defines the core interface for an agent.
type Agent interface {
	Name() string
	SystemInstruction() string
	Tools() []Tool
	ModelName() string      // Ensures this method exists for helloworld_runner/main.go
	LLMProvider() LLMProvider
	Process(ctx context.Context, history []adk_types.Message, latestMessage adk_types.Message) (adk_types.Message, error)
}

// BaseAgent provides a common structure and processing logic for agents.
type BaseAgent struct {
	AgentNameField              string // Renamed to avoid conflict with method Name()
	AgentSystemInstructionField string // Renamed
	AgentToolsField             []Tool   // Renamed
	AgentModelNameField         string // Renamed
	AgentLLMProviderField       LLMProvider // Renamed
}

// NewBaseAgent creates a new BaseAgent.
func NewBaseAgent(name, systemInstruction, modelName string, llmProvider LLMProvider, tools []Tool) *BaseAgent {
	if tools == nil {
		tools = []Tool{}
	}
	return &BaseAgent{
		AgentNameField:              name,
		AgentSystemInstructionField: systemInstruction,
		AgentModelNameField:         modelName,
		AgentLLMProviderField:       llmProvider,
		AgentToolsField:             tools,
	}
}

// Name returns the agent's name.
func (a *BaseAgent) Name() string { return a.AgentNameField }

// SystemInstruction returns the agent's system instruction.
func (a *BaseAgent) SystemInstruction() string { return a.AgentSystemInstructionField }

// Tools returns the tools available to the agent.
func (a *BaseAgent) Tools() []Tool { return a.AgentToolsField }

// ModelName returns the model name the agent is configured to use.
func (a *BaseAgent) ModelName() string { return a.AgentModelNameField }

// LLMProvider returns the LLM provider configured for the agent.
func (a *BaseAgent) LLMProvider() LLMProvider { return a.AgentLLMProviderField }

// Process handles the interaction logic for the BaseAgent.
func (a *BaseAgent) Process(ctx context.Context, history []adk_types.Message, latestMessage adk_types.Message) (adk_types.Message, error) {
	// Check if LLMProvider is available directly from the struct field
	if a.AgentLLMProviderField == nil {
		return adk_types.Message{}, fmt.Errorf("agent %s has no LLMProvider configured", a.AgentNameField)
	}

	// First call to LLM
	llmResponseMsg, err := a.AgentLLMProviderField.GenerateContent(
		ctx,
		a.AgentModelNameField,         // Use field directly
		a.AgentSystemInstructionField, // Use field directly
		a.AgentToolsField,             // Use field directly
		history,
		latestMessage,
	)
	if err != nil {
		return adk_types.Message{}, fmt.Errorf("LLMProvider.GenerateContent failed for agent %s: %w", a.AgentNameField, err)
	}

	var pendingFunctionCall *adk_types.FunctionCall
	for _, part := range llmResponseMsg.Parts {
		if part.FunctionCall != nil {
			pendingFunctionCall = part.FunctionCall
			break // Assuming one function call per LLM turn for this simplified agent
		}
	}

	if pendingFunctionCall != nil {
		var selectedTool Tool
		for _, t := range a.AgentToolsField { // Use field directly
			if t.Name() == pendingFunctionCall.Name {
				selectedTool = t
				break
			}
		}

		var toolResponseMessage adk_types.Message
		if selectedTool == nil {
			errText := fmt.Sprintf("LLM requested unknown tool '%s' for agent %s", pendingFunctionCall.Name, a.AgentNameField)
			toolResponseMessage = adk_types.Message{
				Role: "user",
				Parts: []adk_types.Part{{
					FunctionResponse: &adk_types.FunctionResponse{
						Name:     pendingFunctionCall.Name,
						Response: map[string]string{"error": errText},
					},
				}},
			}
		} else {
			toolResultData, toolErr := selectedTool.Execute(ctx, pendingFunctionCall.Args)
			toolExecutionResponse := adk_types.FunctionResponse{Name: selectedTool.Name()}
			if toolErr != nil {
				toolExecutionResponse.Response = map[string]string{"error": toolErr.Error()}
			} else {
				toolExecutionResponse.Response = toolResultData
			}
			toolResponseMessage = adk_types.Message{
				Role:  "user",
				Parts: []adk_types.Part{{FunctionResponse: &toolExecutionResponse}},
			}
		}
		
		// History for the next call should include the original user message (from latestMessage of previous turn, which is now in 'history'),
		// the LLM's response that contained the function call (llmResponseMsg),
		// and then the toolResponseMessage is the new 'latestMessage'.
		updatedHistory := append(history, latestMessage, llmResponseMsg)
		
		finalLlmResponseMsg, err := a.AgentLLMProviderField.GenerateContent( // Use field for provider
			ctx,
			a.AgentModelNameField,
			a.AgentSystemInstructionField,
			a.AgentToolsField,
			updatedHistory,      // Pass the history that led to the tool call and the call itself
			toolResponseMessage, // The latest message is now the tool's response
		)
		if err != nil {
			return adk_types.Message{}, fmt.Errorf("LLMProvider.GenerateContent after tool call failed for agent %s: %w", a.AgentNameField, err)
		}
		return finalLlmResponseMsg, nil
	}

	// No function call, return the LLM's response directly.
	return llmResponseMsg, nil
}

# /Users/gopher/Desktop/workspace/adk-go/common/types/types.go
package types

type Timestamp float64

type InvocationID string

type SessionID string

type AgentID string

type ToolID string

type EventID string

type ErrorCode string

# /Users/gopher/Desktop/workspace/adk-go/sessions/types/types.go
package types

import (
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"
)

type State map[string]any

type Session struct {
	ID             commontypes.SessionID
	AppName        string
	UserID         string
	CreationTime   commontypes.Timestamp
	LastAccessTime commontypes.Timestamp
	State          State
	Events         []*eventstypes.Event `json:"-"`
	Metadata       map[string]string    `json:",omitempty"`
}

# /Users/gopher/Desktop/workspace/adk-go/examples/helloworld/agent.go
package helloworld

import (
	"fmt"
	"log"

	"github.com/KennethanCeyer/adk-go/adk"
	"github.com/KennethanCeyer/adk-go/llmproviders"
	"github.com/KennethanCeyer/adk-go/tools"
)

// Agent is the "HelloWorld" agent instance.
var Agent adk.Agent // This is the instance, not the type

func init() {
	geminiProvider, err := llmproviders.NewGeminiLLMProvider()
	if err != nil {
		panic(fmt.Sprintf("Failed to create GeminiLLMProvider: %v. Ensure GEMINI_API_KEY is set.", err))
	}

	rollDieTool := tools.NewRollDieTool()
	agentTools := []adk.Tool{rollDieTool}

	// Use a model known for function calling, e.g., "gemini-1.5-flash-latest" or "gemini-pro"
	// Ensure your API key has access to this model.
	Agent = adk.NewBaseAgent(
		"HelloWorldAgent", // Name
		"You are a friendly and helpful assistant. You can roll dice if asked. When you use the roll_die tool, tell the user what you rolled and how many sides the die had.", // System Instruction
		"gemini-1.5-flash-latest", // Model Name
		geminiProvider,            // LLMProvider
		agentTools,                // Tools
	)
	log.Println("HelloWorldAgent initialized successfully.")
}

# /Users/gopher/Desktop/workspace/adk-go/planners/types/types.go
package types

// Forward declarations for types. Replace with actual imports when types are fully defined.
type AnyLlmRequest any
type AnyReadonlyContext any
type AnyCallbackContext any
type AnyPart any

type Planner interface {
	Name() string
	BuildPlanningInstruction(readonlyCtx AnyReadonlyContext, llmReq AnyLlmRequest) (instruction string, err error)
	ProcessPlanningResponse(callbackCtx AnyCallbackContext, responseParts []AnyPart) (processedParts []AnyPart, err error)
	ApplyThinkingConfig(llmReq AnyLlmRequest)
}

# /Users/gopher/Desktop/workspace/adk-go/events/types/types.go
package types

import (
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
)

type EventActions struct {
	StateDelta             map[string]any
	RequestedAuthConfigs []authtypes.AuthConfigName
	EscalateTo             commontypes.AgentID
	TransferToAgent        commontypes.AgentID
	SkipSummarization      bool
	ProducedArtifacts      []string
	ConsumedArtifacts      []string
	CustomMetadata         map[string]any
	TurnComplete           bool
}

type Event struct {
	ID                commontypes.EventID
	InvocationID      commontypes.InvocationID
	SessionID         commontypes.SessionID
	ParentEventID     commontypes.EventID `json:",omitempty"`
	Timestamp         commontypes.Timestamp
	Author            string
	Role              string `json:",omitempty"`
	Content           *modelstypes.Content
	Actions           *EventActions `json:",omitempty"`
	Branch            commontypes.BranchID `json:",omitempty"`
	IsPartial         bool `json:",omitempty"`
	ErrorCode         commontypes.ErrorCode `json:",omitempty"`
	ErrorMessage      string `json:",omitempty"`
	LlmResponse       *modelstypes.LlmResponse `json:",omitempty"`
	ObservedLatencyMs int64 `json:",omitempty"`
}

func (e *Event) GetFunctionCalls() []modelstypes.FunctionCall {
	if e == nil || e.Content == nil {
		return nil
	}
	var calls []modelstypes.FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

func (e *Event) GetFunctionResponses() []modelstypes.FunctionResponse {
	if e == nil || e.Content == nil {
		return nil
	}
	var responses []modelstypes.FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/base_llm_flow.go
package llmflows

import (
	"fmt"
	"log"
	"reflect"
	"runtime"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/sessions"
	// "github.com/KennethanCeyer/adk-go/utils"
)

// Placeholder types from assumed packages (if not already fully defined in processors)
type InvocationContext = processors.InvocationContext
type LlmRequest = models.LlmRequest
type LlmResponse = models.LlmResponse
type Event = events.Event
type EventActions = events.EventActions
type LlmAgent = agents.LlmAgent // This should be the primary agent interface
type Content = models.Content   // Assuming models package defines Content, Part etc.
type Part = models.Part
type FunctionCall = models.FunctionCall
type FunctionResponse = models.FunctionResponse
type State = sessions.State
type RunConfig = agents.RunConfig // Assuming RunConfig is in agents

// BaseLlmFlow is the base for LLM-based flows.
type BaseLlmFlow struct {
	RequestProcessors  []processors.BaseLlmRequestProcessor
	ResponseProcessors []processors.BaseLlmResponseProcessor
}

// NewBaseLlmFlow creates a new BaseLlmFlow.
func NewBaseLlmFlow(
	requestProcessors []processors.BaseLlmRequestProcessor,
	responseProcessors []processors.BaseLlmResponseProcessor) *BaseLlmFlow {
	return &BaseLlmFlow{
		RequestProcessors:  requestProcessors,
		ResponseProcessors: responseProcessors,
	}
}

// RunAsync executes the LLM flow.
func (f *BaseLlmFlow) RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error) {
	outputEventCh := make(chan *Event)

	go func() {
		defer close(outputEventCh)

		llmReq := &LlmRequest{} // Initialize with appropriate defaults if any
		if llmAgent.GetGenerateContentConfig() != nil { // Assuming LlmAgent has GetGenerateContentConfig
			llmReq.Config = llmAgent.GetGenerateContentConfig()
		} else {
			llmReq.Config = &models.GenerateContentConfig{} // Ensure config is not nil
		}


		// 1. Pre-LLM processing
		for _, processor := range f.RequestProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name() // Get processor name for logging
			log.Printf("Running request processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmReq)
			if err != nil {
				log.Printf("Error running request processor %s: %v", processorName, err)
				outputEventCh <- &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in request processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				}
				return
			}
			// Drain events from processor if any (though these processors typically modify llmReq)
			for event := range eventCh {
				log.Printf("Event from request processor %s: %v", processorName, event.ID)
				outputEventCh <- event
			}
		}

		// 2. Call LLM
		// In Python ADK, this is `llm_agent.predict_async(llm_req, invocation_context=invocation_ctx)`
		// which internally calls the model connection.
		// Here, we assume LlmAgent has a method to interact with the LLM.
		log.Printf("Sending request to LLM for agent: %s, model: %s", llmAgent.GetName(), llmReq.Model)
		// For debugging: log.Printf("LLM Request Contents: %+v", llmReq.Contents)
		// For debugging: log.Printf("LLM Request System Instruction: %+v", llmReq.Config.SystemInstruction)


		llmRespCh, err := llmAgent.PredictAsync(invocationCtx, llmReq) // Assuming LlmAgent has PredictAsync
		if err != nil {
			log.Printf("Error initiating LLM prediction for agent %s: %v", llmAgent.GetName(), err)
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error calling LLM: %v", err)}}},
				ErrorCode:    "llm_call_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// Wait for LLM response (event containing LlmResponse)
		modelResponseEvent, ok := <-llmRespCh
		if !ok {
			log.Printf("LLM response channel closed unexpectedly for agent %s", llmAgent.GetName())
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: "LLM response channel closed unexpectedly"}}},
				ErrorCode:    "llm_response_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// The event from PredictAsync should contain the LlmResponse.
		// We need a way to extract it. Let's assume Event has an LlmResponse field or a method.
		var llmResponse *LlmResponse
		if modelResponseEvent.LlmResponse != nil { // Assuming Event has a direct LlmResponse field
			llmResponse = modelResponseEvent.LlmResponse
		} else {
			// Fallback: if LlmResponse is embedded in Content.Parts as a special Part type,
			// or if the event itself *is* the LlmResponse structure (less likely for an Event).
			// This part needs clarification based on how PredictAsync structures its output event.
			log.Printf("Warning: LlmResponse not directly found in modelResponseEvent for agent %s. Event: %+v", llmAgent.GetName(), modelResponseEvent)
			// For now, let's assume if LlmResponse is nil, the event content itself is the raw model output.
            // This means the response processors might need to handle raw Content directly.
            // To make it work with current ResponseProcessor interface, we'll construct a dummy LlmResponse if Content exists.
            if modelResponseEvent.Content != nil && llmResponse == nil {
                llmResponse = &LlmResponse{Content: modelResponseEvent.Content}
                 // Also ensure the event itself is passed, as it contains actions, etc.
            } else if llmResponse == nil {
                 log.Printf("Error: No LlmResponse or Content in modelResponseEvent for agent %s.", llmAgent.GetName())
                 outputEventCh <- &Event{
                    ID:           utils.NewEventID(),
                    InvocationID: invocationCtx.InvocationID,
                    Author:       llmAgent.GetName(),
                    Branch:       invocationCtx.Branch,
                    Content:      &Content{Parts: []Part{{Text: "Invalid or empty LLM response event"}}},
                    ErrorCode:    "llm_response_invalid",
                    Timestamp:    utils.GetTimestamp(),
                }
                return
            }
		}


		// 3. Post-LLM processing
		// The Python ADK iterates response_processors and then sends the modelResponseEvent.
		// If processors yield further events, those are sent too.
		// The original modelResponseEvent is sent *after* all processors complete.

		var accumulatedProcessorEvents []*Event

		for _, processor := range f.ResponseProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name()
			log.Printf("Running response processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmResponse, modelResponseEvent) // Pass the original event for context (e.g., actions)
			if err != nil {
				log.Printf("Error running response processor %s: %v", processorName, err)
				// Decide if this error is fatal or if we should continue with other processors
				// For now, accumulate and proceed, then send error event.
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in response processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				})
				continue // Or return if fatal
			}
			for procEvent := range eventCh {
				log.Printf("Event from response processor %s: %v", processorName, procEvent.ID)
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, procEvent)
			}
		}

		// Send the (potentially modified by processors) modelResponseEvent first
		outputEventCh <- modelResponseEvent

		// Then send any events generated by the response processors
		for _, procEvent := range accumulatedProcessorEvents {
			outputEventCh <- procEvent
		}

	}()

	return outputEventCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/contents.go
package processors

import (
	"fmt"
	"log"
	"sort"
	"strings"
	"time"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions" // For REQUEST_EUC_FUNCTION_CALL_NAME & removeClientFunctionCallID
)

// InvocationContext defines the context for a single invocation of an agent.
type InvocationContext struct {
	InvocationID    string
	Agent           LlmAgent // Assuming LlmAgent or a more general Agent interface
	Session         *Session // Assuming this contains Events and State
	Branch          string
	UserContent     *Content
	ArtifactService interface{} // Placeholder for actual ArtifactService type
	// ... other fields like SessionService, MemoryService, RunConfig, etc.
}

// LlmAgent represents an LLM-based agent.
// This is a simplified interface based on usage in the Python code.
type LlmAgent interface {
	GetName() string
	GetIncludeContents() string // "default", "none"
	// ... other methods like GetPlanner, RootAgent, CanonicalInstruction etc.
}

// Session represents a user session.
type Session struct {
	ID     string
	Events []*Event
	State  map[string]interface{}
	AppName string
	UserID  string
	// ... other session fields
}

// Event represents an event in the system.
type Event struct {
	ID            string
	InvocationID  string
	Author        string
	Content       *Content
	Actions       *EventActions
	Timestamp     float64
	Branch        string
	// ... other event fields like TurnComplete, Partial, ErrorCode, ErrorMessage, etc.
}

// GetFunctionCalls extracts function calls from the event's content parts.
func (e *Event) GetFunctionCalls() []FunctionCall {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var calls []FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

// GetFunctionResponses extracts function responses from the event's content parts.
func (e *Event) GetFunctionResponses() []FunctionResponse {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var responses []FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}


// Content represents the content of a message.
type Content struct {
	Role  string
	Parts []Part
}

// Part represents a part of a message's content.
type Part struct {
	Text             string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
	// ... other part types like InlineData, FileData, etc.
}

// FunctionCall represents a function call requested by the LLM.
type FunctionCall struct {
	ID   string
	Name string
	Args map[string]interface{}
}

// FunctionResponse represents the result of a function call.
type FunctionResponse struct {
	ID       string
	Name     string
	Response map[string]interface{}
}

// EventActions represents actions associated with an event.
type EventActions struct {
	StateDelta             map[string]interface{}
	RequestedAuthConfigs map[string]interface{} // Placeholder for actual AuthConfig type
	// ... other actions like SkipSummarization, TransferToAgent, Escalate
}


// LlmRequest represents a request to the LLM.
type LlmRequest struct {
	Model    string
	Contents []Content
	Config   *GenerateContentConfig // Assuming GenerateContentConfig is defined elsewhere
	// ... other fields like LiveConnectConfig, ToolsDict
}

// GenerateContentConfig holds configuration for LLM content generation.
type GenerateContentConfig struct {
	// Define fields based on google.genai.types.GenerateContentConfig
	// For example:
	// Temperature *float32
	// TopP *float32
	// TopK *int32
	// CandidateCount *int32
	// MaxOutputTokens *int32
	// StopSequences []string
	// SafetySettings []*SafetySetting
	// Tools []*Tool
	// SystemInstruction *Content
	// ResponseSchema interface{}
	// ResponseMimeType string
	// ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig
}


// Placeholder, assuming it's defined in the functions package or similar
const REQUEST_EUC_FUNCTION_CALL_NAME = "adk_request_credential"

// ContentLlmRequestProcessor builds the contents for the LLM request.
type ContentLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *ContentLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		if llmAgent.GetIncludeContents() != "none" { // Assuming LlmAgent has GetIncludeContents()
			if invocationCtx.Session == nil {
				log.Println("Error: InvocationContext.Session is nil")
				return
			}
			llmReq.Contents = getContents(
				invocationCtx.Branch,
				invocationCtx.Session.Events, // Assuming Session has Events []*Event
				llmAgent.GetName(),
			)
		}
		// This processor does not yield events.
	}()
	return outCh, nil
}

func rearrangeEventsForAsyncFunctionResponsesInHistory(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	functionCallIDToResponseEventsIndex := make(map[string]int)
	for i, event := range events {
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			for _, fr := range responses {
				functionCallIDToResponseEventsIndex[fr.ID] = i
			}
		}
	}

	var resultEvents []*Event
	processedResponseIndices := make(map[int]bool) // To avoid adding same response event multiple times

	for i, event := range events {
		if _, ok := processedResponseIndices[i]; ok && len(event.GetFunctionResponses()) > 0 { //
			// This response event was already merged with its call, skip it.
			continue //
		}

		if calls := event.GetFunctionCalls(); len(calls) > 0 { //
			resultEvents = append(resultEvents, event) // Add the function call event itself

			var responseEventsToMerge []*Event
			var indicesToMarkProcessed []int

			for _, fc := range calls {
				if responseIdx, ok := functionCallIDToResponseEventsIndex[fc.ID]; ok { //
					isAlreadyProcessedByAnotherCall := false
					for _, resEvent := range resultEvents {
						if resEvent == events[responseIdx] {
							isAlreadyProcessedByAnotherCall = true
							break
						}
						// Also check if the response event's content (parts) were merged into another event.
						// For simplicity, we rely on direct event object comparison or index tracking.
					}
					if !isAlreadyProcessedByAnotherCall && !processedResponseIndices[responseIdx] { //
						responseEventsToMerge = append(responseEventsToMerge, events[responseIdx])
						indicesToMarkProcessed = append(indicesToMarkProcessed, responseIdx)
					}
				}
			}

			if len(responseEventsToMerge) > 0 {
				sort.SliceStable(responseEventsToMerge, func(i, j int) bool { //
					var idxI, idxJ int = -1, -1
					for k, e := range events {
						if e == responseEventsToMerge[i] {
							idxI = k
						}
						if e == responseEventsToMerge[j] {
							idxJ = k
						}
						if idxI != -1 && idxJ != -1 {
							break
						}
					}
					return idxI < idxJ
				})
				mergedRespEvent := mergeFunctionResponseEvents(responseEventsToMerge)
				if mergedRespEvent != nil {
					resultEvents = append(resultEvents, mergedRespEvent)
					for _, idx := range indicesToMarkProcessed { //
						processedResponseIndices[idx] = true
					}
				}
			}
		} else {
			if !(len(event.GetFunctionResponses()) > 0 && processedResponseIndices[i]) { //
				resultEvents = append(resultEvents, event)
			}
		}
	}
	return resultEvents
}

func rearrangeEventsForLatestFunctionResponse(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	lastEvent := events[len(events)-1]
	responsesInLastEvent := lastEvent.GetFunctionResponses()
	if len(responsesInLastEvent) == 0 { //
		return events
	}

	responseIDs := make(map[string]bool)
	for _, fr := range responsesInLastEvent {
		responseIDs[fr.ID] = true
	}

	if len(events) >= 2 { //
		eventBeforeLast := events[len(events)-2]
		callsInEventBeforeLast := eventBeforeLast.GetFunctionCalls()
		allCallsMatched := true
		if len(callsInEventBeforeLast) == len(responseIDs) && len(callsInEventBeforeLast) > 0 { //
			for _, fc := range callsInEventBeforeLast {
				if !responseIDs[fc.ID] {
					allCallsMatched = false
					break
				}
			}
			if allCallsMatched {
				return events
			}
		}
	}

	functionCallEventIdx := -1 //
	for i := len(events) - 2; i >= 0; i-- { //
		event := events[i]
		calls := event.GetFunctionCalls()
		if len(calls) > 0 {
			foundMatch := false
			for _, fc := range calls {
				if responseIDs[fc.ID] {
					foundMatch = true
					for _, otherFc := range calls {
						responseIDs[otherFc.ID] = true
					}
					break
				}
			}
			if foundMatch {
				functionCallEventIdx = i
				break
			}
		}
	}

	if functionCallEventIdx == -1 { //
		log.Printf("Warning: No matching function call event found for function response IDs: %v", getKeys(responseIDs))
		return events //
	}

	var functionResponseEventsToMerge []*Event
	for i := functionCallEventIdx + 1; i < len(events)-1; i++ { //
		event := events[i]
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			isRelevantResponse := false
			for _, fr := range responses {
				if responseIDs[fr.ID] {
					isRelevantResponse = true
					break
				}
			}
			if isRelevantResponse {
				functionResponseEventsToMerge = append(functionResponseEventsToMerge, event)
			}
		}
	}
	functionResponseEventsToMerge = append(functionResponseEventsToMerge, lastEvent)

	resultEvents := make([]*Event, 0, functionCallEventIdx+2) //
	resultEvents = append(resultEvents, events[:functionCallEventIdx+1]...)
	if merged := mergeFunctionResponseEvents(functionResponseEventsToMerge); merged != nil { //
		resultEvents = append(resultEvents, merged)
	}

	return resultEvents
}

func getContents(currentBranch string, historyEvents []*Event, agentName string) []Content {
	var filteredEvents []*Event
	for _, event := range historyEvents {
		if event.Content == nil || len(event.Content.Parts) == 0 { //
			continue
		}
		// Go: we assume an empty text part is still a valid part unless explicitly filtered.
		// For now, let's assume empty text part implies no meaningful content for LLM.
		hasMeaningfulText := false //
		for _, p := range event.Content.Parts {
			if p.Text != "" {
				hasMeaningfulText = true
				break
			}
		}
		if !hasMeaningfulText && len(event.GetFunctionCalls()) == 0 && len(event.GetFunctionResponses()) == 0 { //
			continue
		}

		if !isEventOnBranch(currentBranch, event) {
			continue
		}
		if isAuthEvent(event) {
			continue
		}

		if isOtherAgentReply(agentName, event) {
			filteredEvents = append(filteredEvents, convertForeignEventToGo(event))
		} else {
			filteredEvents = append(filteredEvents, event)
		}
	}

	// Python calls _rearrange_events_for_latest_function_response first, then _rearrange_events_for_async_function_responses_in_history.
	resultEventsStage1 := rearrangeEventsForLatestFunctionResponse(filteredEvents)
	resultEventsStage2 := rearrangeEventsForAsyncFunctionResponsesInHistory(resultEventsStage1)

	var contents []Content
	for _, event := range resultEventsStage2 {
		if event.Content != nil {
			contentCopy := deepCopyContent(*event.Content)
			removeClientFunctionCallID(&contentCopy) // Placeholder
			contents = append(contents, contentCopy)
		}
	}
	return contents
}

func isEventOnBranch(currentBranch string, event *Event) bool {
	// Placeholder for logic to check if event is on the current branch
	// This depends on how Branch is structured and compared.
	if event == nil {
		return false
	}
	return event.Branch == "" || currentBranch == "" || strings.HasPrefix(event.Branch, currentBranch) || strings.HasPrefix(currentBranch, event.Branch)
}

func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return currentAgentName != "" && event.Author != currentAgentName && event.Author != "user"
}

func convertForeignEventToGo(event *Event) *Event {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return event
	}

	newContent := Content{Role: "user"} //
	newContent.Parts = append(newContent.Parts, Part{Text: "For context:"})

	for _, part := range event.Content.Parts {
		if part.Text != "" {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] said: %s", event.Author, part.Text)})
		} else if part.FunctionCall != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] called tool `%s` with parameters: %v", event.Author, part.FunctionCall.Name, part.FunctionCall.Args)})
		} else if part.FunctionResponse != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] `%s` tool returned result: %v", event.Author, part.FunctionResponse.Name, part.FunctionResponse.Response)})
		} else {
			// This part needs careful consideration for how to represent non-text/non-FC/non-FR parts.
			// For simplicity, appending as-is if possible, or a placeholder.
			// newContent.Parts = append(newContent.Parts, part) // This assumes Part is copyable
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] provided non-text data.", event.Author)}) //
		}
	}

	newEvent := *event // Shallow copy for metadata
	newEvent.Author = "user" //
	newEvent.Content = &newContent
	return &newEvent
}

func mergeFunctionResponseEvents(functionResponseEvents []*Event) *Event {
	if len(functionResponseEvents) == 0 {
		return nil
	}
	if len(functionResponseEvents) == 1 {
		return functionResponseEvents[0]
	}

	baseEvent := functionResponseEvents[0]
	// mergedParts := make([]Part, 0) // Not used directly in Go like Python

	// Map to store the latest response part for each function call ID
	latestResponsesForID := make(map[string]Part)
	var nonResponseParts []Part // To collect text parts or other non-FR parts in order

	for _, event := range functionResponseEvents {
		if event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil {
				latestResponsesForID[part.FunctionResponse.ID] = part
			} else {
				nonResponseParts = append(nonResponseParts, part)
			}
		}
	}

	// The Python code implicitly relies on the order of parts in the *first* event
	// if it contained multiple function responses, and then updates them.
	// For Go, if the first event had multiple FCs, its FRs would be the base.
	// If subsequent events provide FRs for *different* FC IDs not in the first event, they are appended.
	// This simplified version will collect all unique FRs based on their latest appearance.
	tempParts := make([]Part, 0, len(latestResponsesForID)+len(nonResponseParts)) //
	tempParts = append(tempParts, nonResponseParts...)                            //
	// To maintain order similar to Python (where original order of FCs in the *first* event matters,
	// and then other unique FRs are appended/merged), we need a more sophisticated merge.
	// For now, this collects unique FRs; their order relative to nonResponseParts and each other might differ from Python's.
	// A better approach would be to iterate through the function calls of the *triggering* event (if available)
	// and append responses in that order, or sort collected FRs by some original call order.

	// Example: Iterate through first event's FRs if they exist as a base order
	if baseEvent.Content != nil {
		for _, part := range baseEvent.Content.Parts {
			if part.FunctionResponse != nil {
				if respPart, ok := latestResponsesForID[part.FunctionResponse.ID]; ok {
					tempParts = append(tempParts, respPart)
					delete(latestResponsesForID, part.FunctionResponse.ID) // Mark as added
				}
			}
		}
	}
	// Append any remaining unique FRs (those not in the first event's call order)
	for _, part := range latestResponsesForID { // Order from map is not guaranteed.
		tempParts = append(tempParts, part)
	}

	mergedEvent := &Event{ //
		ID:           newEventID(), // Assuming newEventID() is defined
		InvocationID: baseEvent.InvocationID,
		Author:       baseEvent.Author,
		Branch:       baseEvent.Branch,
		Content:      &Content{Role: "user", Parts: tempParts}, // Gemini expects FRs to be role "user"
		Actions:      &EventActions{},                          // Initialize to avoid nil pointer if baseEvent.Actions is nil
		Timestamp:    baseEvent.Timestamp,                      // Or use latest timestamp
	}

	// Merge actions from all events (simplified)
	for _, event := range functionResponseEvents {
		if event.Actions != nil && event.Actions.StateDelta != nil { //
			if mergedEvent.Actions.StateDelta == nil {
				mergedEvent.Actions.StateDelta = make(map[string]interface{})
			}
			for k, v := range event.Actions.StateDelta {
				mergedEvent.Actions.StateDelta[k] = v
			}
		}
		// ... merge other actions like RequestedAuthConfigs, ArtifactDelta etc.
	}

	return mergedEvent
}

// newEventID generates a new unique ID for an event.
// This is a placeholder; a robust UUID generation should be used.
func newEventID() string {
	// In a real implementation, use something like github.com/google/uuid
	return fmt.Sprintf("evt_%d", time.Now().UnixNano())
}


func isAuthEvent(event *Event) bool {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return false
	}
	for _, part := range event.Content.Parts {
		if part.FunctionCall != nil && part.FunctionCall.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
		if part.FunctionResponse != nil && part.FunctionResponse.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
	}
	return false
}

// removeClientFunctionCallID placeholder
func removeClientFunctionCallID(content *Content) {
	// In Python, this removes IDs starting with AF_FUNCTION_CALL_ID_PREFIX
	// For Go, this would iterate through content.Parts and nil out IDs if they match a pattern.
	// For example:
	// const afFunctionCallIDPrefix = "adk-"
	// if content == nil || content.Parts == nil {
	// 	return
	// }
	// for i := range content.Parts {
	// 	if content.Parts[i].FunctionCall != nil && strings.HasPrefix(content.Parts[i].FunctionCall.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionCall.ID = "" // Or based on how Gemini handles empty vs nil
	// 	}
	// 	if content.Parts[i].FunctionResponse != nil && strings.HasPrefix(content.Parts[i].FunctionResponse.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionResponse.ID = ""
	// 	}
	// }
}

// deepCopyContent performs a deep copy of the Content struct.
func deepCopyContent(original Content) Content { //
	// A real deep copy is needed here. This is a placeholder.
	// For Go structs, manual copying or a library might be needed for true deep copies.
	copied := original //
	if original.Parts != nil {
		copied.Parts = make([]Part, len(original.Parts))
		for i, p := range original.Parts {
			// Deep copy each part. This is simplified.
			// If Part contains pointers or slices, those need deep copying too.
			copiedPart := p
			if p.FunctionCall != nil {
				fcCopy := *p.FunctionCall
				if p.FunctionCall.Args != nil {
					fcCopy.Args = deepCopyMap(p.FunctionCall.Args)
				}
				copiedPart.FunctionCall = &fcCopy
			}
			if p.FunctionResponse != nil {
				frCopy := *p.FunctionResponse
				if p.FunctionResponse.Response != nil {
					frCopy.Response = deepCopyMap(p.FunctionResponse.Response)
				}
				copiedPart.FunctionResponse = &frCopy
			}
			// Add deep copy for other Part fields if necessary
			copied.Parts[i] = copiedPart
		}
	}
	return copied
}

// deepCopyMap creates a deep copy of a map[string]interface{}.
// This is a simplified version; a robust deep copy library might be better for complex nested structures.
func deepCopyMap(originalMap map[string]interface{}) map[string]interface{} {
	if originalMap == nil {
		return nil
	}
	copyMap := make(map[string]interface{})
	for key, value := range originalMap {
		switch v := value.(type) {
		case map[string]interface{}:
			copyMap[key] = deepCopyMap(v)
		case []interface{}:
			copyMap[key] = deepCopySlice(v)
		default:
			copyMap[key] = v // Assumes primitive types are copyable by value
		}
	}
	return copyMap
}

// deepCopySlice creates a deep copy of a slice of interface{}.
func deepCopySlice(originalSlice []interface{}) []interface{} {
	if originalSlice == nil {
		return nil
	}
	copySlice := make([]interface{}, len(originalSlice))
	for i, value := range originalSlice {
		switch v := value.(type) {
		case map[string]interface{}:
			copySlice[i] = deepCopyMap(v)
		case []interface{}:
			copySlice[i] = deepCopySlice(v)
		default:
			copySlice[i] = v
		}
	}
	return copySlice
}


func getKeys(m map[string]bool) []string { //
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}

// Placeholder for LlmAgent.GetIncludeContents() method.
// This should be part of the LlmAgent interface/struct definition.
// func (a *LlmAgentStruct) GetIncludeContents() string { return a.IncludeContents }

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/instructions.go
package processors

import (
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/utils" // For instructions_utils
)

// ReadonlyContext provides a read-only view of the invocation context.
type ReadonlyContext struct { //
	InvocationContext *InvocationContext
	// Potentially other fields if ReadonlyContext offers more than just InvocationContext access
}


// InstructionsLlmRequestProcessor handles instructions and global instructions for LLM flow.
type InstructionsLlmRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *InstructionsLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in InstructionsLlmRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx} //

		// Append global instructions if set.
		// In Python ADK, global_instruction is on the root agent.
		// Assuming LlmAgent interface has a RootAgent() method that returns the root agent (LlmAgent type).
		var rootLlmAgent LlmAgent
		if ra := llmAgent.RootAgent(); ra != nil {
			var rOk bool
			rootLlmAgent, rOk = ra.(LlmAgent)
			if !rOk {
				log.Println("Warning: Root agent is not an LlmAgent, cannot process global instructions.")
			}
		}


		if rootLlmAgent != nil { //
			// Assuming LlmAgent interface has CanonicalGlobalInstruction
			globalInstructionStr, bypassGlobal, err := rootLlmAgent.CanonicalGlobalInstruction(readonlyCtx) //
			if err != nil {
				log.Printf("Error getting canonical global instruction: %v", err) //
			}
			if globalInstructionStr != "" {
				var processedGlobalInstr string
				if !bypassGlobal {
					processedGlobalInstr, err = injectSessionState(globalInstructionStr, readonlyCtx) //
					if err != nil {
						log.Printf("Error injecting state into global instruction: %v", err) //
						processedGlobalInstr = globalInstructionStr                               // Use raw on error
					}
				} else {
					processedGlobalInstr = globalInstructionStr
				}
				// Assuming LlmRequest has AppendInstructions
				if llmReq.Config == nil {
					llmReq.Config = &GenerateContentConfig{}
				}
				if llmReq.Config.SystemInstruction == nil {
					llmReq.Config.SystemInstruction = &Content{}
				}
				// This needs a proper AppendInstructions method on LlmRequest or its Config
				currentSysInstruction := ""
				if len(llmReq.Config.SystemInstruction.Parts) > 0 {
					currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
				}
				if currentSysInstruction != "" {
					currentSysInstruction += "\n\n"
				}
				llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedGlobalInstr}}

			}
		}

		// Append agent instructions if set.
		// Assuming LlmAgent interface has CanonicalInstruction
		agentInstructionStr, bypassAgent, err := llmAgent.CanonicalInstruction(readonlyCtx) //
		if err != nil {
			log.Printf("Error getting canonical agent instruction: %v", err) //
		}
		if agentInstructionStr != "" {
			var processedAgentInstr string
			if !bypassAgent {
				processedAgentInstr, err = injectSessionState(agentInstructionStr, readonlyCtx) //
				if err != nil {
					log.Printf("Error injecting state into agent instruction: %v", err) //
					processedAgentInstr = agentInstructionStr                             // Use raw on error
				}
			} else {
				processedAgentInstr = agentInstructionStr
			}
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedAgentInstr}}
		}
		// No events are yielded by this specific processor.
	}()
	return outCh, nil
}

// injectSessionState populates values in the instruction template.
// This is a Go equivalent of instructions_utils.inject_session_state.
func injectSessionState(template string, readonlyCtx *ReadonlyContext) (string, error) {
	if readonlyCtx == nil || readonlyCtx.InvocationContext == nil || readonlyCtx.InvocationContext.Session == nil {
		return template, fmt.Errorf("readonlyCtx or its internal fields are nil")
	}
	invocationCtx := readonlyCtx.InvocationContext //

	// Go's regexp package doesn't support direct async replacement functions.
	// This will be synchronous for now.
	// Pattern to find {var_name} or {var_name?} or {artifact.file_name}
	// For simplicity, a basic version is used here. A more robust parser might be needed.
	re := regexp.MustCompile(`{([^}]+)}`) //

	var replacementErr error

	result := re.ReplaceAllStringFunc(template, func(match string) string { //
		if replacementErr != nil {
			return match
		}

		varNameWithMarker := strings.Trim(match, "{} ")
		isOptional := strings.HasSuffix(varNameWithMarker, "?")
		varName := strings.TrimSuffix(varNameWithMarker, "?")

		if strings.HasPrefix(varName, "artifact.") { //
			artifactName := strings.TrimPrefix(varName, "artifact.")
			if invocationCtx.ArtifactService == nil { //
				replacementErr = fmt.Errorf("artifact service is not initialized")
				return match
			}
			// Conceptual: ArtifactService would need a synchronous LoadArtifact or this needs to be async.
			// For this snippet, assuming a conceptual synchronous load or pre-loaded artifact map.
			// This part is commented out as ArtifactService and its methods are not fully defined here.
			/*
				artifactPart, err := invocationCtx.ArtifactService.LoadArtifactSync( //
					invocationCtx.Session.AppName, // Assuming AppName field
					invocationCtx.Session.UserID,  // Assuming UserID field
					invocationCtx.Session.ID,      // Assuming ID field
					artifactName,
				)
				if err != nil {
					if isOptional { return "" }
					replacementErr = fmt.Errorf("artifact '%s' not found: %w", artifactName, err)
					return match
				}
				if artifactPart != nil {
					return artifactPart.Text // Assuming text artifact for simplicity
				}
			*/
			return fmt.Sprintf("[ARTIFACT:%s]", artifactName) // Placeholder
		}

		if !isValidStateName(varName) { //
			return match
		}

		if val, ok := invocationCtx.Session.State[varName]; ok { //
			return fmt.Sprintf("%v", val)
		}
		if isOptional { //
			return ""
		}
		replacementErr = fmt.Errorf("context variable not found: `%s`", varName) //
		return match
	})

	if replacementErr != nil {
		return "", replacementErr
	}
	return result, nil
}

// isValidStateName (simplified) checks if a variable name could be a state key.
func isValidStateName(varName string) bool {
	parts := strings.Split(varName, ":")
	if len(parts) == 1 {
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(varName) //
	}
	if len(parts) == 2 {
		// In Python ADK, prefixes are "app:", "user:", "temp:"
		// Simplified check for now.
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[0]) &&
			regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[1]) //
	}
	return false
}

// LlmAgent interface needs to be fully defined, including these methods for the above to compile:
// RootAgent() BaseAgent // Assuming BaseAgent is a common interface for all agents
// CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error)
// CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error)

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/base_llm_processors.go
package agents

import (
	"context"
	"fmt"
	"log"

	llmprovideriface "github.com/KennethanCeyer/adk-go/llmproviders"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools"
)

// BaseLlmAgent provides a standard implementation for LlmAgent.
type BaseLlmAgent struct {
	AgentName         string
	AgentDescription  string
	ModelName         string
	SystemInstruction *modelstypes.Content // Use Content type for system instruction
	AgentTools        []toolstypes.Tool
	Provider          llmprovideriface.LLMProvider
}

// NewBaseLlmAgent creates a new BaseLlmAgent.
func NewBaseLlmAgent(
	name, description, modelName string,
	systemInstruction *modelstypes.Content, // Changed to *modelstypes.Content
	provider llmprovideriface.LLMProvider,
	agentTools []toolstypes.Tool,
) *BaseLlmAgent {
	if agentTools == nil {
		agentTools = []toolstypes.Tool{}
	}
	return &BaseLlmAgent{
		AgentName:         name,
		AgentDescription:  description,
		ModelName:         modelName,
		SystemInstruction: systemInstruction, // Store as Content
		AgentTools:        agentTools,
		Provider:          provider,
	}
}

// GetName returns the agent's name.
func (a *BaseLlmAgent) GetName() string { return a.AgentName }
// GetDescription returns the agent's description.
func (a *BaseLlmAgent) GetDescription() string { return a.AgentDescription }
// GetModelIdentifier returns the LLM model name.
func (a *BaseLlmAgent) GetModelIdentifier() string { return a.ModelName }
// GetSystemInstruction returns the agent's system instruction.
func (a *BaseLlmAgent) GetSystemInstruction() *modelstypes.Content { return a.SystemInstruction }
// GetTools returns the tools available to the agent.
func (a *BaseLlmAgent) GetTools() []toolstypes.Tool { return a.AgentTools }
// GetLLMProvider returns the LLM provider for this agent.
func (a *BaseLlmAgent) GetLLMProvider() llmprovideriface.LLMProvider { return a.Provider }

// Process handles a turn of conversation.
func (a *BaseLlmAgent) Process(
	ctx context.Context,
	history []modelstypes.Content,
	latestContent modelstypes.Content,
) (*modelstypes.Content, error) {
	if a.Provider == nil {
		return nil, fmt.Errorf("BaseLlmAgent Process: agent %s has no LLMProvider configured", a.AgentName)
	}

	currentHistory := history
	currentInputContent := latestContent
	maxToolCalls := 5 // Limit recursive tool calls

	for i := 0; i < maxToolCalls; i++ {
		llmResponseContent, err := a.Provider.GenerateContent(
			ctx,
			a.ModelName,
			a.SystemInstruction,
			a.AgentTools,
			currentHistory,
			currentInputContent,
		)
		if err != nil {
			return nil, fmt.Errorf("BaseLlmAgent Process: LLMProvider.GenerateContent failed for agent %s: %w", a.AgentName, err)
		}

		if llmResponseContent == nil {
			log.Printf("BaseLlmAgent Process: Agent %s received nil content from LLM.", a.AgentName)
			return &modelstypes.Content{Role: "model", Parts: []modelstypes.Part{{Text: new(string)}}}, nil // Empty text part
		}

		// Append the input that led to this LLM response, and the LLM response itself to history for the next turn.
		currentHistory = append(currentHistory, currentInputContent)
		currentHistory = append(currentHistory, *llmResponseContent)

		var pendingFunctionCall *modelstypes.FunctionCall
		for _, part := range llmResponseContent.Parts {
			if part.FunctionCall != nil {
				pendingFunctionCall = part.FunctionCall
				break
			}
		}

		if pendingFunctionCall == nil {
			return llmResponseContent, nil // No function call, this is the final response for this turn.
		}

		log.Printf("BaseLlmAgent Process: Agent %s attempting to call tool '%s'", a.AgentName, pendingFunctionCall.Name)
		var selectedTool toolstypes.Tool
		for _, t := range a.AgentTools {
			if t.Name() == pendingFunctionCall.Name {
				selectedTool = t
				break
			}
		}

		var toolExecutionResult modelstypes.Part
		if selectedTool == nil {
			errMsg := fmt.Sprintf("tool '%s' not found for agent %s", pendingFunctionCall.Name, a.AgentName)
			log.Println(errMsg)
			toolExecutionResult.FunctionResponse = &modelstypes.FunctionResponse{
				Name:     pendingFunctionCall.Name,
				Response: map[string]any{"error": errMsg},
			}
		} else {
			toolResultData, toolErr := selectedTool.Execute(ctx, pendingFunctionCall.Args)
			fr := &modelstypes.FunctionResponse{Name: selectedTool.Name()}
			if toolErr != nil {
				log.Printf("BaseLlmAgent Process: Error executing tool %s for agent %s: %v", selectedTool.Name(), a.AgentName, toolErr)
				fr.Response = map[string]any{"error": toolErr.Error()}
			} else {
				fr.Response = toolResultData
			}
			toolExecutionResult.FunctionResponse = fr
		}
		
		// Next input for the LLM is the function response.
		// Role for function response content should be "function" or "tool".
		currentInputContent = modelstypes.Content{
			Role:  "function", // Use "function" as per typical GenAI API function calling flows
			Parts: []modelstypes.Part{toolExecutionResult},
		}
		// History for the next LLM call already contains the previous user/model turns, including the FC.
	}

	return nil, fmt.Errorf("BaseLlmAgent Process: agent %s exceeded maximum tool call iterations (%d)", a.AgentName, maxToolCalls)
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/basic.go
package processors

import (
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, etc.
)

// BaseLlm defines the interface for a Large Language Model.
type BaseLlm interface {
	ModelName() string
	// Potentially other methods like GenerateContent, Connect etc.
}

// MockLLMForFlow is a placeholder for testing, representing a specific LLM implementation.
type MockLLMForFlow struct {
	ModelName string
}

func (m *MockLLMForFlow) ModelName() string {
	return m.ModelName
}
// Ensure MockLLMForFlow implements BaseLlm
var _ BaseLlm = &MockLLMForFlow{}


// RunConfig defines configuration for an agent run.
type RunConfig struct {
	// Define fields based on google.adk.agents.run_config.RunConfig
	// Example fields (conceptual):
	// ResponseModalities []string
	// SpeechConfig interface{}
	// OutputAudioTranscription bool
	// InputAudioTranscription bool
	// MaxLlmCalls int
	// StreamingMode string
}


// LlmAgent interface part related to BasicRequestProcessor
type LlmAgentForBasicProcessor interface {
	LlmAgent // Embed the general LlmAgent interface
	CanonicalModel() (BaseLlm, error)
	GetGenerateContentConfig() *GenerateContentConfig
	GetOutputSchema() interface{}
}


// BasicRequestProcessor handles basic information to build the LLM request.
type BasicRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *BasicRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) { //
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in BasicRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgentForBasicProcessor) // Use the more specific interface
		if !ok {
			// If agent is not an LlmAgent, this processor might not apply,
			// or it might indicate an issue with the agent setup.
			// For now, we simply return without error.
			log.Println("Error: Agent in InvocationContext is not of type LlmAgentForBasicProcessor or compatible interface")
			return
		}

		canonicalModel, err := llmAgent.CanonicalModel() //
		if err != nil {
			log.Printf("Error getting canonical model for agent %s: %v", llmAgent.GetName(), err) //
			return
		}

		if canonicalModel != nil { //
			llmReq.Model = canonicalModel.ModelName() //
		} else {
			log.Printf("Warning: CanonicalModel is nil for agent %s, model name might not be set correctly in LlmRequest.", llmAgent.GetName()) //
		}


		if cfg := llmAgent.GetGenerateContentConfig(); cfg != nil { //
			cfgCopy := *cfg // Shallow copy
			llmReq.Config = &cfgCopy //
		} else {
			llmReq.Config = &GenerateContentConfig{} // Ensure config is not nil
		}

		if outputSchema := llmAgent.GetOutputSchema(); outputSchema != nil { //
			// Assuming LlmRequest has a SetOutputSchema method
			// llmReq.SetOutputSchema(outputSchema) // This depends on LlmRequest's definition
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			llmReq.Config.ResponseSchema = outputSchema
			llmReq.Config.ResponseMimeType = "application/json" // Typically for structured output

		}

		if invocationCtx.RunConfig != nil { //
			// Conceptual assignments - actual fields depend on RunConfig and LlmRequest.LiveConnectConfig definitions
			// if llmReq.LiveConnectConfig == nil { llmReq.LiveConnectConfig = &LiveConnectConfig{} }
			// llmReq.LiveConnectConfig.ResponseModalities = invocationCtx.RunConfig.ResponseModalities
			// llmReq.LiveConnectConfig.SpeechConfig = invocationCtx.RunConfig.SpeechConfig
			// llmReq.LiveConnectConfig.OutputAudioTranscription = invocationCtx.RunConfig.OutputAudioTranscription
			// llmReq.LiveConnectConfig.InputAudioTranscription = invocationCtx.RunConfig.InputAudioTranscription
			// These would be direct assignments if LiveConnectConfig fields match RunConfig fields.
		}

		// No events are yielded by this specific processor, it only modifies llmReq.
	}()
	return outCh, nil
}

// These GetGenerateContentConfig and GetOutputSchema methods should be part of the LlmAgent implementation.
// Example implementation for a hypothetical LlmAgentStruct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentGenerateContentConfig *GenerateContentConfig
    OutputSchema interface{}
}

func (a *LlmAgentStruct) GetGenerateContentConfig() *GenerateContentConfig { //
    return a.AgentGenerateContentConfig
}

func (a *LlmAgentStruct) GetOutputSchema() interface{} { //
    return a.OutputSchema
}
*/

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/identity.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
)

// IdentityLlmRequestProcessor gives the agent identity from the framework.
type IdentityLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *IdentityLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in IdentityLlmRequestProcessor")
			return
		}
		agent := invocationCtx.Agent // Assuming Agent interface has GetName() and GetDescription()

		var instructions []string
		instructions = append(instructions, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName())) //
		if desc := agent.GetDescription(); desc != "" { //
			instructions = append(instructions, fmt.Sprintf(" The description about you is \"%s\"", desc)) //
		}

		// Assuming LlmRequest has AppendInstructions method or similar mechanism
		if llmReq.Config == nil {
			llmReq.Config = &GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &Content{}
		}
		currentSysInstruction := ""
		if len(llmReq.Config.SystemInstruction.Parts) > 0 {
			currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
		}

		addedInstruction := strings.Join(instructions, "")
		if currentSysInstruction != "" {
			currentSysInstruction += "\n\n" // Ensure separation if there's existing instruction
		}
		llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + addedInstruction}}
		// This processor does not yield events, it modifies llmReq.
	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/nlplanning.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/planners" // For PlanReActPlanner, BuiltInPlanner
	// "github.com/google/generative-ai-go/genai" // For genai.Part, genai.ThinkingConfig
)

// Planner defines the interface for agent planners.
type Planner interface { // [cite: 2133]
	BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) // [cite: 2133]
	ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) // [cite: 2133]
	// ApplyThinkingConfig is specific to BuiltInPlanner in Python, might be part of a more specific interface in Go
	ApplyThinkingConfig(llmReq *LlmRequest)
}

// BuiltInPlanner is a placeholder for planners.BuiltInPlanner [cite: 2133]
type BuiltInPlanner struct {
	ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig [cite: 2133]
}

// ApplyThinkingConfig applies the thinking configuration to the LLM request.
func (bip *BuiltInPlanner) ApplyThinkingConfig(llmReq *LlmRequest) { // [cite: 2133]
	if bip.ThinkingConfig != nil && llmReq.Config != nil {
		// llmReq.Config.ThinkingConfig = bip.ThinkingConfig // This depends on GenerateContentConfig definition
		// For example, if GenerateContentConfig has a field `ThinkingConfig interface{}`
		// then the above line would work.
		log.Println("BuiltInPlanner.ApplyThinkingConfig: ThinkingConfig application is conceptual.")
	}
}
// BuildPlanningInstruction for BuiltInPlanner.
func (bip *BuiltInPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) { // [cite: 2133]
	return "", nil // Python version returns None, so empty string and no error for Go
}
// ProcessPlanningResponse for BuiltInPlanner.
func (bip *BuiltInPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) { // [cite: 2133, 2134]
	return responseParts, nil
}

// PlanReActPlanner is a placeholder for planners.PlanReActPlanner [cite: 2134]
type PlanReActPlanner struct{}

// Constants for PlanReActPlanner tags [cite: 2134]
const (
	PlanningTag    = "/*PLANNING*/"    // [cite: 2134]
	RePlanningTag  = "/*REPLANNING*/"  // [cite: 2134]
	ReasoningTag   = "/*REASONING*/"   // [cite: 2134]
	ActionTag      = "/*ACTION*/"      // [cite: 2134]
	FinalAnswerTag = "/*FINAL_ANSWER*/" // [cite: 2134]
)

// BuildPlanningInstruction for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) {
	// This directly translates the Python _build_nl_planner_instruction method [cite: 2134]
	highLevelPreamble := fmt.Sprintf(` When answering the question, try to leverage the available tools to gather the information instead of your memorized knowledge. [cite: 2134, 2135]
Follow this process when answering the question: (1) first come up with a plan in natural language text format; [cite: 2135, 2136]
(2) Then use tools to execute the plan and provide reasoning between tool code snippets to make a summary of current state and next step. [cite: 2136, 2137]
Tool code snippets and reasoning should be interleaved with each other. (3) In the end, return one final answer. [cite: 2137]
Follow this format when answering the question: (1) The planning part should be under %s. [cite: 2138]
(2) The tool code snippets should be under %s, and the reasoning parts should be under %s. [cite: 2139]
(3) The final answer part should be under %s. `, PlanningTag, ActionTag, ReasoningTag, FinalAnswerTag) // [cite: 2139, 2140]

	planningPreamble := fmt.Sprintf(` Below are the requirements for the planning: The plan is made to answer the user query if following the plan. The plan is coherent and covers all aspects of information from user query, and only involves the tools that are accessible by the agent. The plan contains the decomposed steps as a numbered list where each step should use one or multiple available tools. By reading the plan, you can intuitively know which tools to trigger or what actions to take. [cite: 2140, 2141]
If the initial plan cannot be successfully executed, you should learn from previous execution results and revise your plan. The revised plan should be be under %s. Then use tools to follow the new plan. `, RePlanningTag) // [cite: 2141]

	// The Python original has reasoning_preamble, final_answer_preamble, tool_code_without_python_libraries_preamble, user_input_preamble
	// These would be similarly defined and concatenated. For brevity, I'm showing the pattern.
	// Assuming those other preamble parts are defined similarly:
	reasoningPreamble := ` Below are the requirements for the reasoning: The reasoning makes a summary of the current trajectory based on the user query and tool outputs. Based on the tool outputs and plan, the reasoning also comes up with instructions to the next steps, making the trajectory closer to the final answer. `
	finalAnswerPreamble := ` Below are the requirements for the final answer: The final answer should be precise and follow query formatting requirements. Some queries may not be answerable with the available tools and information. In those cases, inform the user why you cannot process their query and ask for more information. `
	toolCodeWithoutPythonLibrariesPreamble := ` Below are the requirements for the tool code: **Custom Tools:** The available tools are described in the context and can be directly used. - Code must be valid self-contained Python snippets with no imports and no references to tools or Python libraries that are not in the context. - You cannot use any parameters or fields that are not explicitly defined in the APIs in the context. - The code snippets should be readable, efficient, and directly relevant to the user query and reasoning steps. - When using the tools, you should use the library name together with the function name, e.g., vertex_search.search(). - If Python libraries are not provided in the context, NEVER write your own code other than the function calls using the provided tools. `
	userInputPreamble := ` VERY IMPORTANT instruction that you MUST follow in addition to the above instructions: You should ask for clarification if you need more information to answer the question. You should prefer using the information available in the context instead of repeated tool use. `

	return strings.Join([]string{
		highLevelPreamble,
		planningPreamble,
		reasoningPreamble,
		finalAnswerPreamble,
		toolCodeWithoutPythonLibrariesPreamble,
		userInputPreamble,
	}, "\n\n"), nil
}

// markAsThought marks a response part as a thought.
// In Go, `Part` doesn't have a direct `Thought` field like in Python's genai types.
// This might be handled by a wrapper struct or by convention (e.g., specific role or metadata).
// For this conceptual translation, we'll assume a way to mark it, or it's implicit in the planner's logic.
func (prp *PlanReActPlanner) markAsThought(part *Part) {
	// Conceptual: If Part struct had a Thought field: part.Thought = true
	// Or, this could modify the Part's role or add metadata if the Go ADK defines such conventions.
	// For now, this is a no-op unless Part struct is extended.
	log.Printf("Marking part as thought (conceptual): %v", part.Text)
}


// ProcessPlanningResponse for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) {
	if responseParts == nil { // [cite: 2134]
		return nil, nil
	}

	var preservedParts []Part
	firstFCPartIndex := -1

	for i, part := range responseParts {
		if part.FunctionCall != nil { // [cite: 2134]
			if part.FunctionCall.Name == "" {
				continue
			}
			preservedParts = append(preservedParts, part)
			if firstFCPartIndex == -1 {
				firstFCPartIndex = i
			}
		} else {
			// Handle non-function call parts based on tags
			if part.Text != "" {
				// Split by FINAL_ANSWER_TAG first
				if strings.Contains(part.Text, FINAL_ANSWER_TAG) {
					splits := strings.SplitN(part.Text, FINAL_ANSWER_TAG, 2)
					reasoningText := strings.TrimSpace(splits[0])
					finalAnswerText := ""
					if len(splits) > 1 {
						finalAnswerText = strings.TrimSpace(splits[1])
					}

					if reasoningText != "" {
						reasoningPart := Part{Text: reasoningText}
						prp.markAsThought(&reasoningPart) // Conceptual
						preservedParts = append(preservedParts, reasoningPart)
					}
					if finalAnswerText != "" {
						preservedParts = append(preservedParts, Part{Text: finalAnswerText})
					}
				} else {
					// If no FINAL_ANSWER_TAG, check for other thought tags
					isThought := false
					for _, tag := range []string{PlanningTag, ReasoningTag, ActionTag, RePlanningTag} {
						if strings.HasPrefix(part.Text, tag) {
							isThought = true
							break
						}
					}
					currentPart := part
					if isThought {
						prp.markAsThought(&currentPart) // Conceptual
					}
					preservedParts = append(preservedParts, currentPart)
				}
			}
		}
	}
	return preservedParts, nil
}

// ApplyThinkingConfig for PlanReActPlanner (not directly applicable as it doesn't use genai.ThinkingConfig)
func (prp *PlanReActPlanner) ApplyThinkingConfig(llmReq *LlmRequest) {
	// PlanReActPlanner builds instructions directly, doesn't use genai.ThinkingConfig
}


// LlmAgent interface needs to be fully defined for the below to be valid outside this file.
// For example, within an LlmAgent struct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentPlanner Planner
}

func (a *LlmAgentStruct) GetPlanner() Planner {
    return a.AgentPlanner
}
*/

// NlPlanningRequestProcessor handles NL planning for LLM flow.
type NlPlanningRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *NlPlanningRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in NlPlanningRequestProcessor")
			return
		}

		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}
		// Python has: if isinstance(planner, BuiltInPlanner): planner.apply_thinking_config(llm_req)
		// In Go, this would typically be handled by type assertion or checking an interface method.
		if bp, ok := planner.(*BuiltInPlanner); ok { // [cite: 2133]
			bp.ApplyThinkingConfig(llmReq) // [cite: 2133]
		}


		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx}
		instruction, err := planner.BuildPlanningInstruction(readonlyCtx, llmReq) // [cite: 2133]
		if err != nil {
			log.Printf("Error building planning instruction: %v", err)
			// Optionally send an error event or handle error
			return
		}
		if instruction != "" {
			// Assuming LlmRequest has AppendInstructions
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + instruction}}
		}
	}()
	return outCh, nil
}

// NlPlanningResponseProcessor processes LLM responses for NL planning.
type NlPlanningResponseProcessor struct{}

// RunAsync implements the BaseLlmResponseProcessor interface (conceptual).
func (p *NlPlanningResponseProcessor) RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || llmResp == nil || llmResp.Content == nil {
			log.Println("Error: InvocationContext, Agent, or LlmResponse content is nil in NlPlanningResponseProcessor")
			return
		}
		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}

		callbackCtx := &CallbackContext{InvocationContext: invocationCtx, EventActions: modelResponseEvent.Actions} // [cite: 2133]
		if callbackCtx.EventActions == nil { // Ensure EventActions is not nil
		    callbackCtx.EventActions = &EventActions{}
		}


		processedParts, err := planner.ProcessPlanningResponse(callbackCtx, llmResp.Content.Parts) // [cite: 2134]
		if err != nil {
			log.Printf("Error processing planning response: %v", err)
			// Optionally send an error event or handle error
			return
		}

		if processedParts != nil {
			llmResp.Content.Parts = processedParts
		}

		// If callbackCtx.State has changed (meaning planner updated state via EventActions)
		// and an event needs to be yielded. In Python, this is handled by the framework.
		// Here, we might need to explicitly yield an event if the state delta implies one.
		// However, the processor's role is usually to modify the llmResp or yield new events based on llmResp processing.
		// If state changes in callbackCtx are meant to produce a new event, that logic would go here.
		// For now, this processor primarily modifies the llmResp. If an event with updated actions
		// is needed, it would be constructed and sent to outCh.
		// This part of the Python code doesn't explicitly yield an Event from this processor.

	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/auto_flow.go
package llmflows

import (
	"fmt"
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
)

// AutoFlow dynamically selects and runs the appropriate flow for an agent.
// In Go, the dynamic creation of the LlmAgent itself as done in Python's AutoFlow
// (creating an LlmAgent on the fly with specific tools/planners based on config)
// is less straightforward due to static typing.
// Typically, the LlmAgent would already be constructed with its configuration.
// This AutoFlow equivalent will focus on selecting the flow and running it.
type AutoFlow struct {
	// Configuration for AutoFlow, if any, can go here.
	// For example, a map of agent types to flow constructors.
}

// NewAutoFlow creates a new AutoFlow.
func NewAutoFlow() *AutoFlow {
	return &AutoFlow{}
}

// RunAsync executes the appropriate flow for the given agent.
func (af *AutoFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	if invocationCtx == nil || invocationCtx.Agent == nil {
		err := fmt.Errorf("AutoFlow: InvocationContext or Agent is nil")
		log.Println(err)
		// Return a channel that immediately closes or sends an error event
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	agent := invocationCtx.Agent // Agent from InvocationContext

	// The Python AutoFlow can dynamically create an LlmAgent if the provided agent
	// is just a configuration. In Go, we'd expect `agent` to already be a runnable LlmAgent.
	// If `agent` is a configuration struct, a factory would be needed here to build the LlmAgent.

	llmAgent, ok := agent.(LlmAgent) // Assuming the agent in context is already an LlmAgent
	if !ok {
		err := fmt.Errorf("AutoFlow: agent in InvocationContext is not an LlmAgent, but %T", agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	// --- Flow Selection Logic ---
	// In Python, this checks agent.flow or agent.tools, agent.planner etc.
	// For Go, we'll assume a simple logic: if the agent is an LlmAgent, use SingleFlow.
	// More complex logic could be based on LlmAgent's properties or specific type.

	var selectedFlow interface { // Using interface{} to represent a generic Flow type
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	}

	// Example: If LlmAgent has a GetFlowType() method or similar configuration.
	// flowType := llmAgent.GetFlowType() // Hypothetical method
	// switch flowType {
	// case "single":
	//  selectedFlow = NewSingleFlow()
	// case "sequential":
	//  selectedFlow = NewSequentialFlow() // If SequentialFlow exists
	// default:
	//  selectedFlow = NewSingleFlow() // Default
	// }

	// For now, directly use SingleFlow if it's an LlmAgent
	if _, isLlmAgent := agent.(LlmAgent); isLlmAgent {
		log.Printf("AutoFlow: Using SingleFlow for LlmAgent: %s", llmAgent.GetName())
		selectedFlow = NewSingleFlow()
	} else {
		// Handle other agent types if necessary, or default
		err := fmt.Errorf("AutoFlow: Unhandled agent type %T for flow selection. Defaulting to SingleFlow if possible or erroring.", agent)
		log.Println(err)
        // Attempt to use SingleFlow if the base agent can be cast.
        // This part is tricky without knowing all agent types.
        // For robustnes, it's better to ensure llmAgent is correctly typed above.
		log.Println("AutoFlow: Defaulting to SingleFlow (or error if not LlmAgent).")
		selectedFlow = NewSingleFlow() // This will run if the initial cast to LlmAgent succeeded.
	}


	if selectedFlow == nil {
		err := fmt.Errorf("AutoFlow: Could not determine a flow for agent: %s (%T)", llmAgent.GetName(), agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}

	// Type assert selectedFlow to the expected Flow interface/type to call RunAsync
	// This assumes all flows (SingleFlow, etc.) implement this method signature.
	concreteFlow, flowOk := selectedFlow.(interface {
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	})
	if !flowOk {
		err := fmt.Errorf("AutoFlow: Selected flow does not implement the required RunAsync method signature.")
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}


	log.Printf("AutoFlow: Running flow for agent: %s", llmAgent.GetName())
	return concreteFlow.RunAsync(invocationCtx, llmAgent)
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/single_flow.go
package llmflows

import (
	"github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
)

// SingleFlow represents a simple LLM flow with a standard set of processors.
type SingleFlow struct {
	*BaseLlmFlow
}

// NewSingleFlow creates a new SingleFlow.
func NewSingleFlow() *SingleFlow {
	requestProcessors := []processors.BaseLlmRequestProcessor{
		&processors.BasicRequestProcessor{},
		&processors.IdentityLlmRequestProcessor{},
		&processors.ToolLlmRequestProcessor{}, // Assuming ToolLlmRequestProcessor is defined
		&processors.NlPlanningRequestProcessor{},
		&processors.InstructionsLlmRequestProcessor{},
		&processors.ContentLlmRequestProcessor{},
		&processors.CodeExecutionRequestProcessor{}, // Assuming CodeExecutionRequestProcessor is defined
	}

	responseProcessors := []processors.BaseLlmResponseProcessor{
		&processors.NlPlanningResponseProcessor{},
		&processors.FunctionCallResponseProcessor{}, // Assuming FunctionCallResponseProcessor is defined
		&processors.CodeExecutionResponseProcessor{},// Assuming CodeExecutionResponseProcessor is defined
		&processors.ToolUsageResponseProcessor{},    // Assuming ToolUsageResponseProcessor is defined
	}

	baseFlow := NewBaseLlmFlow(requestProcessors, responseProcessors)
	return &SingleFlow{BaseLlmFlow: baseFlow}
}

// Ensure SingleFlow implements an expected Flow interface if one exists
// type Flow interface {
//    RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error)
// }
// var _ Flow = &SingleFlow{}

# /Users/gopher/Desktop/workspace/adk-go/golang_code.txt
# /Users/gopher/Desktop/workspace/adk-go/cmd/echoagent_runner/main.go
package main

import (
	"bufio"
	"context"
	"fmt"
	"log"
	"os"
	"os/signal"
	"strings"
	"syscall"

	"github.com/KennethanCeyer/adk-go/examples/echoagent"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
)

func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	go func() {
		<-sigChan
		log.Println("Shutdown signal received, cancelling context...")
		cancel()
		// Allow time for cleanup or force exit if needed after a delay
		// time.Sleep(1 * time.Second)
		// os.Exit(1)
	}()

	if echoagent.ConcreteLlmAgent == nil {
		log.Fatal("EchoAgent runner: ConcreteLlmAgent is not initialized. Check init() in examples/echoagent/agent.go and ensure GEMINI_API_KEY is set.")
	}
	agentToRun := echoagent.ConcreteLlmAgent

	fmt.Printf("--- Starting Agent: %s ---\n", agentToRun.GetName())
	if agentToRun.GetDescription() != "" {
		fmt.Printf("Description: %s\n", agentToRun.GetDescription())
	}
	fmt.Printf("Model: %s\n", agentToRun.GetModelIdentifier())
	if len(agentToRun.GetTools()) > 0 {
		fmt.Println("Available Tools:")
		for _, tool := range agentToRun.GetTools() {
			fmt.Printf("  - %s: %s\n", tool.Name(), tool.Description())
		}
	}
	fmt.Println("Type 'exit' or 'quit' to stop.")
	fmt.Println("------------------------------------")

	var history []modelstypes.Content
	scanner := bufio.NewScanner(os.Stdin)

	for {
		if ctx.Err() != nil { // Check if context was cancelled (e.g., by SIGINT)
			log.Println("Context cancelled, exiting runner loop.")
			break
		}
		fmt.Print("[user]: ")
		if !scanner.Scan() {
			if err := scanner.Err(); err != nil {
				log.Printf("Input error: %v", err)
			} else {
				log.Println("EOF received, exiting.")
			}
			break
		}
		userInputText := strings.TrimSpace(scanner.Text())

		if strings.ToLower(userInputText) == "exit" || strings.ToLower(userInputText) == "quit" {
			fmt.Println("Exiting agent.")
			break
		}
		if userInputText == "" {
			continue
		}

		currentUserContent := modelstypes.Content{
			Role:  "user",
			Parts: []modelstypes.Part{{Text: &userInputText}},
		}

		agentResponseContent, err := agentToRun.Process(ctx, history, currentUserContent)
		if err != nil {
			log.Printf("Error from agent.Process: %v", err)
			fmt.Printf("[%s-error]: I encountered an issue: %v\n", agentToRun.GetName(), err)
			history = append(history, currentUserContent) // Add user message for context
			continue
		}

		history = append(history, currentUserContent)
		if agentResponseContent != nil {
			history = append(history, *agentResponseContent)
		}

		if agentResponseContent != nil && len(agentResponseContent.Parts) > 0 {
			var responseTexts []string
			for _, part := range agentResponseContent.Parts {
				if part.Text != nil {
					responseTexts = append(responseTexts, *part.Text)
				}
				if part.FunctionCall != nil {
					log.Printf("Runner: Agent response contained FunctionCall: Name=%s. This should have been handled by agent's Process loop.", part.FunctionCall.Name)
					responseTexts = append(responseTexts, fmt.Sprintf("[Debug: Agent yielded unhandled FunctionCall to tool '%s']", part.FunctionCall.Name))
				}
			}
			fmt.Printf("[%s]: %s\n", agentToRun.GetName(), strings.Join(responseTexts, "\n"))
		} else {
			fmt.Printf("[%s]: (Agent returned no displayable content)\n", agentToRun.GetName())
		}

		const maxHistoryTurns = 10 // Keep last 10 pairs of user/model messages
		if len(history) > maxHistoryTurns*2 {
			history = history[len(history)-(maxHistoryTurns*2):]
		}
	}
	log.Println("EchoAgent Runner finished.")
}

# /Users/gopher/Desktop/workspace/adk-go/cmd/helloworld_runner/main.go
// File: github.com/KennethanCeyer/adk-go/cmd/helloworld_runner/main.go
package main

import (
	"bufio"
	"context"
	"fmt"
	"log"
	"os"
	"os/signal"
	"strings"
	"syscall"

	"github.com/KennethanCeyer/adk-go/examples/helloworld"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
)

func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	go func() {
		<-sigChan
		log.Println("Shutdown signal received, cancelling context...")
		cancel()
	}()

	if helloworld.ConcreteLlmAgent == nil {
		log.Fatal("HelloWorld agent (ConcreteLlmAgent) is not initialized. Check init() in examples/helloworld/agent.go and ensure GEMINI_API_KEY is set.")
	}
	agentToRun := helloworld.ConcreteLlmAgent

	fmt.Printf("Starting agent: %s (%s)\n", agentToRun.GetName(), agentToRun.GetDescription())
	fmt.Println("Model:", agentToRun.GetModelIdentifier())
	fmt.Println("Available tools:", agentToRun.GetTools())
	fmt.Println("Type 'exit' or 'quit' to stop.")

	var history []modelstypes.Content // History stores Content directly
	scanner := bufio.NewScanner(os.Stdin)

	for {
		fmt.Print("[user]: ")
		if !scanner.Scan() {
			if err := scanner.Err(); err != nil {
				log.Printf("Error reading input: %v", err)
			}
			break // EOF or error
		}
		userInputText := strings.TrimSpace(scanner.Text())

		if strings.ToLower(userInputText) == "exit" || strings.ToLower(userInputText) == "quit" {
			fmt.Println("Exiting agent.")
			break
		}
		if userInputText == "" {
			continue
		}

		currentUserContent := modelstypes.Content{
			Role:  "user",
			Parts: []modelstypes.Part{{Text: &userInputText}},
		}

		// Pass history and current user content to the agent's Process method
		agentResponseContent, err := agentToRun.Process(ctx, history, currentUserContent)
		if err != nil {
			log.Printf("Error processing message with agent %s: %v", agentToRun.GetName(), err)
			// Add user message to history even on error for context
			history = append(history, currentUserContent)
			// Display error to user
			errorMsg := fmt.Sprintf("Sorry, I encountered an error: %v", err)
			fmt.Printf("[%s-error]: %s\n", agentToRun.GetName(), errorMsg)
			// Optionally add an error "model" response to history
			// history = append(history, modelstypes.Content{Role: "model", Parts: []modelstypes.Part{{Text: &errorMsg}}})
			continue
		}

		// Add current user input and agent's response to history
		history = append(history, currentUserContent)
		if agentResponseContent != nil { // Ensure response is not nil before appending
			history = append(history, *agentResponseContent)
		}


		// Display agent's response
		if agentResponseContent != nil && len(agentResponseContent.Parts) > 0 {
			var responseTexts []string
			for _, part := range agentResponseContent.Parts {
				if part.Text != nil {
					responseTexts = append(responseTexts, *part.Text)
				}
				if part.FunctionCall != nil { // The agent's Process method should handle FCs internally.
					responseTexts = append(responseTexts, fmt.Sprintf("[Agent wants to call tool: %s with args: %+v. This is unexpected here.]", part.FunctionCall.Name, part.FunctionCall.Args))
				}
			}
			fmt.Printf("[%s]: %s\n", agentToRun.GetName(), strings.Join(responseTexts, "\n"))
		} else {
			fmt.Printf("[%s]: (Received no parsable content from agent)\n", agentToRun.GetName())
		}


		const maxHistoryItems = 10 // Keep last 5 turns (user + model)
		if len(history) > maxHistoryItems {
			history = history[len(history)-maxHistoryItems:]
		}
	}
	log.Println("HelloWorld Agent Runner finished.")
}

# /Users/gopher/Desktop/workspace/adk-go/tools/types/types.go
package types

import (
	"context"
)

type Tool interface {
	Name() string
	Description() string
	Parameters() any // e.g., map[string]any for JSON schema
	Execute(ctx context.Context, args any) (result any, err error)
}

# /Users/gopher/Desktop/workspace/adk-go/tools/example/echo_tool.go
package example

import (
	"context"
	"fmt"

	"github.com/google/generative-ai-go/genai"
)

// EchoTool is a simple tool that echoes input.
type EchoTool struct{}

// NewEchoTool creates an EchoTool.
func NewEchoTool() *EchoTool {
	return &EchoTool{}
}

// Name returns the tool's name.
func (t *EchoTool) Name() string {
	return "echo_tool"
}

// Description returns the tool's purpose.
func (t *EchoTool) Description() string {
	return "Echoes back the input message provided to it. Useful for testing function calling."
}

// Parameters defines the schema for arguments.
func (t *EchoTool) Parameters() any {
	return &genai.Schema{
		Type: genai.TypeObject,
		Properties: map[string]*genai.Schema{
			"message_to_echo": {
				Type:        genai.TypeString,
				Description: "The exact message that the tool should echo back.",
			},
		},
		Required: []string{"message_to_echo"},
	}
}

// Execute runs the tool.
func (t *EchoTool) Execute(ctx context.Context, args any) (any, error) {
	argsMap, ok := args.(map[string]any)
	if !ok {
		return nil, fmt.Errorf("echo_tool: invalid arguments format, expected map[string]any, got %T", args)
	}

	messageVal, ok := argsMap["message_to_echo"]
	if !ok {
		return nil, fmt.Errorf("echo_tool: missing 'message_to_echo' argument")
	}

	messageStr, ok := messageVal.(string)
	if !ok {
		return nil, fmt.Errorf("echo_tool: 'message_to_echo' argument must be a string, got %T", messageVal)
	}

	return map[string]any{"echoed_message": "Echo: " + messageStr}, nil
}

# /Users/gopher/Desktop/workspace/adk-go/tools/interface.go
package tools

import (
	"context"
)

// Tool defines the interface for agent capabilities.
type Tool interface {
	Name() string
	Description() string
	Parameters() any // JSON schema as map[string]any or *genai.Schema.
	Execute(ctx context.Context, args any) (result any, err error)
}

# /Users/gopher/Desktop/workspace/adk-go/tools/rolldie.go
package tools

import (
	"context"
	"crypto/rand"
	"fmt"
	"log"
	"math/big"

	"github.com/google/generative-ai-go/genai" // For genai.Schema used in Parameters()
)

// RollDieTool is a simple tool that simulates rolling a die.
type RollDieTool struct{}

// NewRollDieTool creates a new RollDieTool.
func NewRollDieTool() *RollDieTool {
	return &RollDieTool{}
}

func (t *RollDieTool) Name() string {
	return "roll_die"
}

func (t *RollDieTool) Description() string {
	return "Rolls a die with a specified number of sides and returns the result."
}

// Parameters defines the JSON schema for the arguments this tool accepts.
func (t *RollDieTool) Parameters() any {
	// Using genai.Schema for compatibility with GeminiLLMProvider
	// This defines an object with one integer property "sides".
	return &genai.Schema{
		Type: genai.TypeObject,
		Properties: map[string]*genai.Schema{
			"sides": {
				Type:        genai.TypeInteger,
				Description: "The number of sides on the die (e.g., 6 for a standard die, 20 for a d20). Must be a positive integer.",
			},
		},
		Required: []string{"sides"},
	}
}

// Execute performs the die roll.
// args is expected to be map[string]any as per JSON object schema.
func (t *RollDieTool) Execute(ctx context.Context, args any) (any, error) {
	log.Printf("RollDieTool: Executing with args: %+v (type: %T)", args, args)

	argsMap, ok := args.(map[string]any)
	if !ok {
		return nil, fmt.Errorf("invalid arguments format for RollDieTool: expected map[string]any, got %T", args)
	}

	sidesVal, ok := argsMap["sides"]
	if !ok {
		return nil, fmt.Errorf("missing 'sides' argument in RollDieTool")
	}

	var sides int64
	switch v := sidesVal.(type) {
	case float64: // JSON numbers often parse as float64
		if v <= 0 || v != float64(int64(v)) { // Check if positive and whole number
			return nil, fmt.Errorf("'sides' must be a positive integer, got %f", v)
		}
		sides = int64(v)
	case int:
		if v <= 0 {
			return nil, fmt.Errorf("'sides' must be a positive integer, got %d", v)
		}
		sides = int64(v)
	case int32:
		if v <= 0 {
			return nil, fmt.Errorf("'sides' must be a positive integer, got %d", v)
		}
		sides = int64(v)
	case int64:
		if v <= 0 {
			return nil, fmt.Errorf("'sides' must be a positive integer, got %d", v)
		}
		sides = v
	default:
		return nil, fmt.Errorf("'sides' argument must be an integer, got type %T", sidesVal)
	}

	if sides <= 0 { // Redundant check if above type checks are thorough, but good for safety.
		return nil, fmt.Errorf("number of sides must be a positive integer, got %d", sides)
	}

	// Using crypto/rand for better randomness if needed, math/rand is simpler for non-security critical tasks.
	// math/rand needs seeding (e.g., rand.Seed(time.Now().UnixNano())) once at program start.
	// crypto/rand is preferred for production.
	nBig, err := rand.Int(rand.Reader, big.NewInt(sides))
	if err != nil {
		return nil, fmt.Errorf("failed to generate random number for die roll: %w", err)
	}
	// nBig is in [0, sides-1], so add 1 for [1, sides]
	result := nBig.Int64() + 1

	log.Printf("RollDieTool: Rolled a %d (d%d)", result, sides)
	// The result should be structured as defined by the tool's output schema (if any).
	// For Gemini, it often expects a map.
	return map[string]any{"result": result, "sides_rolled": sides}, nil
}

# /Users/gopher/Desktop/workspace/adk-go/llmproviders/gemini.go
package llmproviders

import (
	"context"
	"fmt"
	"log"
	"os"

	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools/types"

	"github.com/google/generative-ai-go/genai"
	"google.golang.org/api/option"
)

type GeminiLLMProvider struct {
	apiKey string
}

func NewGeminiLLMProvider() (*GeminiLLMProvider, error) {
	apiKey := os.Getenv("GEMINI_API_KEY")
	if apiKey == "" {
		return nil, fmt.Errorf("GEMINI_API_KEY environment variable not set")
	}
	return &GeminiLLMProvider{apiKey: apiKey}, nil
}

func convertADKToolsToGenaiTools(adkTools []toolstypes.Tool) []*genai.Tool {
	if len(adkTools) == 0 {
		return nil
	}
	genaiTools := make([]*genai.Tool, len(adkTools))
	for i, t := range adkTools {
		var paramSchema *genai.Schema
		paramSchemaData := t.Parameters() // Expects JSON schema as map[string]any or *genai.Schema
		if paramSchemaData != nil {
			if schemaMap, ok := paramSchemaData.(map[string]any); ok {
				paramSchema = &genai.Schema{Type: genai.TypeObject, Properties: make(map[string]*genai.Schema)}
				if props, ok := schemaMap["properties"].(map[string]any); ok {
					for k, v := range props {
						propSchemaMap, _ := v.(map[string]any)
						propTypeStr, _ := propSchemaMap["type"].(string)
						propDesc, _ := propSchemaMap["description"].(string)
						var propType genai.SchemaType
						switch propTypeStr {
						case "string":
							propType = genai.TypeString
						case "integer", "number":
							propType = genai.TypeNumber // Use TypeNumber for broader compatibility
						case "boolean":
							propType = genai.TypeBoolean
						default:
							propType = genai.TypeString
						}
						paramSchema.Properties[k] = &genai.Schema{Type: propType, Description: propDesc}
					}
				}
				if required, ok := schemaMap["required"].([]any); ok {
					for _, req := range required {
						if rStr, rOk := req.(string); rOk {
							paramSchema.Required = append(paramSchema.Required, rStr)
						}
					}
				}
			} else if schemaGenai, ok := paramSchemaData.(*genai.Schema); ok {
				paramSchema = schemaGenai
			}
		}

		genaiTools[i] = &genai.Tool{
			FunctionDeclarations: []*genai.FunctionDeclaration{{
				Name:        t.Name(),
				Description: t.Description(),
				Parameters:  paramSchema,
			}},
		}
	}
	return genaiTools
}

func convertADKContentToGenaiContent(adkContents []modelstypes.Content) []*genai.Content {
	var genaiContents []*genai.Content
	for _, adkContent := range adkContents {
		genaiC := &genai.Content{Role: adkContent.Role}
		for _, adkPart := range adkContent.Parts {
			var genaiP genai.Part
			if adkPart.Text != nil {
				genaiP = genai.Text(*adkPart.Text)
			} else if adkPart.FunctionCall != nil {
				if adkContent.Role == "model" { // FunctionCall parts in history are from the model
					genaiP = genai.FunctionCall{Name: adkPart.FunctionCall.Name, Args: adkPart.FunctionCall.Args}
				} else {
					continue // Skip user-originated FC parts in history for genai format
				}
			} else if adkPart.FunctionResponse != nil {
				// FunctionResponse parts for genai are typically role "function" or "tool"
				// but genai.Content uses "user" role for FunctionResponse inputs.
				if adkContent.Role != "user" && adkContent.Role != "function" && adkContent.Role != "tool" {
					// Forcing role to "user" if it's a function response part, as per genai SDK's current chat expectations
					// This might need adjustment if genai SDK changes or for different models.
					// Or the input adkContent should already have the correct role.
					log.Printf("Warning: FunctionResponse part with role %s, genai expects 'user' or 'tool' for FR inputs. Adjusting role for this part context.", adkContent.Role)
				}
				genaiP = genai.FunctionResponse{Name: adkPart.FunctionResponse.Name, Response: adkPart.FunctionResponse.Response}
			} else {
				continue
			}
			genaiC.Parts = append(genaiC.Parts, genaiP)
		}
		if len(genaiC.Parts) > 0 {
			genaiContents = append(genaiContents, genaiC)
		}
	}
	return genaiContents
}

func convertGenaiCandidateToADKContent(candidate *genai.Candidate) *modelstypes.Content {
	adkContent := &modelstypes.Content{Role: "model"} // LLM responses are from "model"
	if candidate == nil || candidate.Content == nil {
		text := "Error: LLM returned no content."
		adkContent.Parts = []modelstypes.Part{{Text: &text}}
		return adkContent
	}

	for _, genaiP := range candidate.Content.Parts {
		var adkP modelstypes.Part
		switch v := genaiP.(type) {
		case genai.Text:
			text := string(v)
			adkP.Text = &text
		case genai.FunctionCall:
			adkP.FunctionCall = &modelstypes.FunctionCall{Name: v.Name, Args: v.Args}
		default:
			unsupportedText := fmt.Sprintf("Unsupported genai.Part type received: %T", v)
			adkP.Text = &unsupportedText
		}
		adkContent.Parts = append(adkContent.Parts, adkP)
	}
	return adkContent
}

func (g *GeminiLLMProvider) GenerateContent(
	ctx context.Context,
	modelName string,
	systemInstruction *modelstypes.Content,
	tools []toolstypes.Tool,
	history []modelstypes.Content,
	latestContent modelstypes.Content,
) (*modelstypes.Content, error) {
	client, err := genai.NewClient(ctx, option.WithAPIKey(g.apiKey))
	if err != nil {
		return nil, fmt.Errorf("failed to create genai client: %w", err)
	}
	defer client.Close()

	model := client.GenerativeModel(modelName)

	if systemInstruction != nil && len(systemInstruction.Parts) > 0 {
		// Assuming system instruction is primarily text
		if systemInstruction.Parts[0].Text != nil {
			model.SystemInstruction = genai.Text(*systemInstruction.Parts[0].Text)
		}
	}

	if len(tools) > 0 {
		model.Tools = convertADKToolsToGenaiTools(tools)
	}

	chatHistory := convertADKContentToGenaiContent(history)
	convertedLatestContent := convertADKContentToGenaiContent([]modelstypes.Content{latestContent})

	if len(convertedLatestContent) == 0 || len(convertedLatestContent[0].Parts) == 0 {
		return nil, fmt.Errorf("latest message resulted in no usable content for LLM")
	}

	session := model.StartChat()
	if len(chatHistory) > 0 {
		session.History = chatHistory
	}
	
	// Use SendMessage for non-streaming to get full response at once
	resp, err := session.SendMessage(ctx, convertedLatestContent[0].Parts...)
	if err != nil {
		// Check for specific block reasons
		var blockedText string
		if resp != nil && resp.PromptFeedback != nil && resp.PromptFeedback.BlockReason != genai.BlockReasonUnspecified {
			blockedText = fmt.Sprintf("LLM response blocked. Reason: %s. Message: %s",
				resp.PromptFeedback.BlockReason.String(),
				resp.PromptFeedback.BlockReasonMessage)
			return &modelstypes.Content{Role: "model", Parts: []modelstypes.Part{{Text: &blockedText}}}, nil
		}
		return nil, fmt.Errorf("genai SendMessage failed: %w", err)
	}
	

	if resp == nil || len(resp.Candidates) == 0 {
		noCandidateText := "LLM returned no candidates."
		// Check for finish reason if available
		if resp != nil && len(resp.Candidates) > 0 && resp.Candidates[0].FinishReason != genai.FinishReasonUnspecified && resp.Candidates[0].FinishReason != genai.FinishReasonStop {
			noCandidateText = fmt.Sprintf("LLM generation finished with reason: %s", resp.Candidates[0].FinishReason.String())
		} else if resp != nil && resp.PromptFeedback != nil && resp.PromptFeedback.BlockReason != genai.BlockReasonUnspecified {
            noCandidateText = fmt.Sprintf("LLM response blocked. Reason: %s. Message: %s",
                resp.PromptFeedback.BlockReason.String(),
                resp.PromptFeedback.BlockReasonMessage)
        }
		return &modelstypes.Content{Role: "model", Parts: []modelstypes.Part{{Text: &noCandidateText}}}, nil
	}

	return convertGenaiCandidateToADKContent(resp.Candidates[0]), nil
}

# /Users/gopher/Desktop/workspace/adk-go/llmproviders/interfaces/interfaces.go
package llmproviders

import (
	"context"

	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools"
)

type LLMProvider interface {
	GenerateContent(
		ctx context.Context,
		modelName string,
		systemInstruction *modelstypes.Content,
		tools []toolstypes.Tool,
		history []modelstypes.Content,
		latestContent modelstypes.Content,
	) (*modelstypes.Content, error)
}

# /Users/gopher/Desktop/workspace/adk-go/auth/types/types.go
package types

type AuthConfigName string

type AuthConfig struct {
	Name         AuthConfigName
	AuthScheme   string
	Scope        string
	ClientID     string
	ClientSecret string
	// Other scheme-specific fields (e.g., auth_url, token_url)
}

type AuthCredential struct {
	Name         AuthConfigName
	AccessToken  string
	RefreshToken string
	ExpiresIn    int64 // Seconds
	ObtainedAt   int64 // Unix timestamp
	Scope        string
	IDToken      string
}

func (ac *AuthCredential) IsExpired() bool {
	if ac.AccessToken == "" || ac.ExpiresIn <= 0 || ac.ObtainedAt <= 0 {
		return true
	}
	// Needs actual time checking logic, e.g. using time.Now().Unix()
	// return time.Now().Unix() >= (ac.ObtainedAt + ac.ExpiresIn - 60) // 60s buffer
	return false // Placeholder
}

// AuthHandler interface (can also be in auth/interfaces/interfaces.go)
// To avoid too many "types" packages, interfaces can live with their functional code or in a dedicated interfaces sub-package.
// For now, keeping it here for simplicity of this correction step.
// If AuthHandler needs to be used by other packages that auth might import,
// it would need to be in a more common or dedicated interface package.
type AuthHandler interface {
	GetAllAuthConfigs() []AuthConfig
	GetAuthConfig(name AuthConfigName) (*AuthConfig, error)
	// Session type will be from github.com/KennethanCeyer/adk-go/sessions/types
	GetCredential(name AuthConfigName, session any) (AuthCredential, error)
	// IsToolAuthorized(toolName string, authConfigsUsedByTool []AuthConfigName, session any) (bool, error)
	// GetAuthTool() any // Would return a toolstypes.Tool
}

# /Users/gopher/Desktop/workspace/adk-go/auth/auth_preprocessor.go
package auth

import (
	"fmt"
	"log"
	"strings"
	"time"

	agentiface "github.com/KennethanCeyer/adk-go/agents"
	"github.com/KennethanCeyer/adk-go/agents/invocation"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"

	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions"
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

const defaultAuthInstructionTemplate = `
You have access to a set of tools that may require user authorization.
If you need to use a tool that requires authorization, you must first request authorization by calling the "%s" function.
The function call will return one of the following:
- A success message: "Successfully obtained credential."
- An error message: "Failed to obtain credential. <reason>"
- A message indicating that user interaction is required: "User interaction is required to obtain credential. <auth_url>"
Only if the function call returns a success message, you can then proceed to use the tool that requires authorization.
Otherwise, you must inform the user about the error or the required user interaction.

Current user:
<User>
%s
</User>
`

type AuthLlmRequestProcessor struct{}

// BaseLlmRequestProcessor interface (should be defined elsewhere, e.g., flows/llmflows/processors/interfaces.go)
type BaseLlmRequestProcessor interface {
	RunAsync(invocationCtx *invocation.InvocationContext, llmReq *modelstypes.LlmRequest) (<-chan *eventstypes.Event, error)
}

var _ BaseLlmRequestProcessor = (*AuthLlmRequestProcessor)(nil)

func (p *AuthLlmRequestProcessor) RunAsync(
	invocationCtx *invocation.InvocationContext,
	llmReq *modelstypes.LlmRequest,
) (<-chan *eventstypes.Event, error) {
	outCh := make(chan *eventstypes.Event)

	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || invocationCtx.Session == nil {
			log.Println("AuthLlmRequestProcessor: InvocationContext, Agent, or Session is nil.")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(agentiface.LlmAgent)
		if !ok {
			// invocationCtx.Agent is agentiface.Agent. We need to check if it's also an LlmAgent.
			// This might require a type assertion from the specific AgentInterface used in InvocationContext.
			// For now, assuming InvocationContext.Agent can be directly asserted if it's set with an LlmAgent.
			log.Printf("AuthLlmRequestProcessor: Agent in InvocationContext is not an LlmAgent.")
			return
		}

		authHandler := llmAgent.GetAuthHandler()
		if authHandler == nil {
			return
		}

		authConfigs := authHandler.GetAllAuthConfigs()
		if len(authConfigs) == 0 {
			return
		}

		// Cast session from any to *sessionstypes.Session for GetCredential
		// This relies on AuthHandler's GetCredential taking `any` and handling the cast,
		// or defining GetCredential more specifically. The authtypes.AuthHandler definition
		// uses `any` for session, so it's consistent for now.
		userInfo := p.buildUserInfo(invocationCtx.Session, authConfigs, authHandler)
		authInstruction := p.buildAuthInstruction(userInfo)

		if llmReq.Config == nil {
			llmReq.Config = &modelstypes.GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &modelstypes.Content{Parts: []modelstypes.Part{}}
		}

		var currentSysInstructionText string
		if len(llmReq.Config.SystemInstruction.Parts) > 0 && llmReq.Config.SystemInstruction.Parts[0].Text != nil {
			currentSysInstructionText = *llmReq.Config.SystemInstruction.Parts[0].Text
		}

		if currentSysInstructionText != "" {
			currentSysInstructionText += "\n\n"
		}
		newInstructionText := currentSysInstructionText + authInstruction
		
		if len(llmReq.Config.SystemInstruction.Parts) == 0 {
		    llmReq.Config.SystemInstruction.Parts = append(llmReq.Config.SystemInstruction.Parts, modelstypes.Part{Text: &newInstructionText})
		} else {
		    llmReq.Config.SystemInstruction.Parts[0].Text = &newInstructionText
		}

		if p.isAuthFlowCompletedInPriorTurn(invocationCtx.Session.Events) {
			log.Println("AuthLlmRequestProcessor: Auth flow was recently completed or attempted in a prior turn.")
		}
	}()

	return outCh, nil
}

func (p *AuthLlmRequestProcessor) buildUserInfo(
	session *sessionstypes.Session,
	authConfigs []authtypes.AuthConfig,
	authHandler authtypes.AuthHandler,
) string {
	if session == nil {
		return "User information not available (session is nil)."
	}
	var userInfoParts []string
	userInfoParts = append(userInfoParts, fmt.Sprintf("  User ID: %s", session.UserID))
	userInfoParts = append(userInfoParts, fmt.Sprintf("  App ID: %s", session.AppName))

	for _, authConfig := range authConfigs {
		credential, err := authHandler.GetCredential(authConfig.Name, session) // Pass session directly
		status := "Not authorized"

		if err != nil {
			log.Printf("AuthLlmRequestProcessor: Error getting credential for %s: %v", authConfig.Name, err)
			status = fmt.Sprintf("Error checking authorization for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
		} else {
			if credential.AccessToken != "" {
				if !credential.IsExpired() { // IsExpired defined on authtypes.AuthCredential
					status = fmt.Sprintf("Authorized for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
				} else {
					status = fmt.Sprintf("Authorization expired for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
				}
			} else {
				status = fmt.Sprintf("Not authorized for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
			}
		}
		userInfoParts = append(userInfoParts, "  "+status)
	}
	return strings.Join(userInfoParts, "\n")
}


func (p *AuthLlmRequestProcessor) buildAuthInstruction(userInfo string) string {
	return fmt.Sprintf(defaultAuthInstructionTemplate, functions.REQUEST_EUC_FUNCTION_CALL_NAME, userInfo)
}

func (p *AuthLlmRequestProcessor) isAuthFlowCompletedInPriorTurn(historyEvents []*eventstypes.Event) bool {
	if len(historyEvents) == 0 {
		return false
	}
	const turnsToLookBack = 2
	maxEventsToCheck := turnsToLookBack * 2
	if maxEventsToCheck > len(historyEvents) {
		maxEventsToCheck = len(historyEvents)
	}

	for i := len(historyEvents) - 1; i >= len(historyEvents)-maxEventsToCheck; i-- {
		event := historyEvents[i]
		if event == nil || event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil && part.FunctionResponse.Name == functions.REQUEST_EUC_FUNCTION_CALL_NAME {
				return true
			}
		}
	}
	return false
}

func newEventID() commontypes.EventID {
	return commontypes.EventID(fmt.Sprintf("evt-%d", time.Now().UnixNano()))
}

func getTimestamp() commontypes.Timestamp {
	return commontypes.Timestamp(float64(time.Now().UnixNano()) / 1e9)
}

# /Users/gopher/Desktop/workspace/adk-go/agents/invocation/types/types.go
package types

import (
	"context"

	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	// agentiface "github.com/KennethanCeyer/adk-go/agents" // This would cause import cycle if agents/interfaces.go imports this.
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

// AgentInterfacePlaceholder is used to avoid import cycle with agents package.
// Replace with actual `agents.Agent` type if structure allows.
type AgentInterfacePlaceholder interface {
	GetName() string
	GetDescription() string
}

// RunConfig holds configuration for a specific agent execution.
type RunConfig struct {
	// Add fields as needed, e.g., MaxToolCalls, Streaming.
}

// InvocationContext provides context for a single agent invocation.
type InvocationContext struct {
	Ctx          context.Context
	InvocationID commontypes.InvocationID
	Agent        AgentInterfacePlaceholder // Use placeholder
	Session      *sessionstypes.Session
	UserContent  *modelstypes.Content
	RunConfig    *RunConfig
}

// ReadonlyContext offers a read-only view of InvocationContext.
type ReadonlyContext struct {
	invocationCtx *InvocationContext
}

// NewReadonlyContext creates a ReadonlyContext.
func NewReadonlyContext(invCtx *InvocationContext) *ReadonlyContext {
	return &ReadonlyContext{invocationCtx: invCtx}
}

// GetSession returns the session from the context.
func (roc *ReadonlyContext) GetSession() *sessionstypes.Session {
	if roc.invocationCtx == nil {
		return nil
	}
	return roc.invocationCtx.Session
}

# /Users/gopher/Desktop/workspace/adk-go/agents/invocation/inovocation.go
package invocation

import (
	"context"

	agentiface "github.com/KennethanCeyer/adk-go/agents"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

type RunConfig struct {
	// Fields for run-specific configurations if needed.
}

type InvocationContext struct {
	Ctx          context.Context
	InvocationID commontypes.InvocationID
	Agent        agentiface.Agent // The agent being invoked
	Session      *sessionstypes.Session
	UserContent  *modelstypes.Content // Initial user input for this specific invocation
	RunConfig    *RunConfig
}

type ReadonlyContext struct {
	invocationCtx *InvocationContext
}

func NewReadonlyContext(invCtx *InvocationContext) *ReadonlyContext {
	return &ReadonlyContext{invocationCtx: invCtx}
}

// Add getter methods for ReadonlyContext if specific fields need to be exposed.

# /Users/gopher/Desktop/workspace/adk-go/agents/base_agent.go
package agents

import (
	"fmt"
	"regexp"
	// Hypothetical Go ADK packages
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/genai/types" // For genai.Content
	// "github.com/KennethanCeyer/adk-go/telemetry"
)

// Forward declarations for types that would be defined in other packages.
// In a real Go ADK, these would be imported.
type Content struct { // Placeholder for genai.types.Content
	Role  string `json:"role,omitempty"`
	Parts []Part `json:"parts,omitempty"`
}

type Part struct { // Placeholder for genai.types.Part
	Text string `json:"text,omitempty"`
	// Other fields like FunctionCall, FunctionResponse, InlineData would be here
}

type Event struct { // Placeholder for events.Event
	InvocationID string       `json:"invocationId,omitempty"`
	Author       string       `json:"author,omitempty"`
	Branch       string       `json:"branch,omitempty"`
	Content      *Content     `json:"content,omitempty"`
	Actions      EventActions `json:"actions,omitempty"` // Placeholder
	ID           string       `json:"id,omitempty"`
	Timestamp    float64      `json:"timestamp,omitempty"`
	// ... other Event fields
}

type EventActions struct { // Placeholder for events.EventActions
	StateDelta map[string]interface{} `json:"stateDelta,omitempty"`
	// ... other EventActions fields
}

type InvocationContext struct { // Placeholder
	Agent            Agent
	Branch           string
	InvocationID     string
	Session          *Session // Placeholder
	EndInvocation    bool
	UserContent      *Content
	ArtifactService  interface{} // Placeholder
	SessionService   interface{} // Placeholder
	MemoryService    interface{} // Placeholder
	LiveRequestQueue interface{} // Placeholder for LiveRequestQueue
	RunConfig        *RunConfig  // Placeholder
}

type Session struct { // Placeholder for sessions.Session
	State map[string]interface{}
	// ... other Session fields
}

type RunConfig struct { // Placeholder for RunConfig
	// ... RunConfig fields
}

type CallbackContext struct { // Placeholder
	InvocationContext *InvocationContext
	EventActions      EventActions
	State             MutableState // Placeholder for a state map that tracks deltas
}

type MutableState map[string]interface{} // Simplified placeholder

func (ms MutableState) HasDelta() bool {
	// In a real implementation, this would check if _delta has items
	return len(ms) > 0 // Simplified
}

// Agent is the base interface for all agents in the Agent Development Kit.
type Agent interface {
	GetName() string
	GetDescription() string
	ParentAgent() Agent
	SetParentAgent(parent Agent) error
	SubAgents() []Agent
	AddSubAgent(subAgent Agent)
	RunAsync(parentCtx *InvocationContext) (<-chan *Event, error)
	RunLive(parentCtx *InvocationContext) (<-chan *Event, error)
	RootAgent() Agent
	FindAgent(name string) Agent
	FindSubAgent(name string) Agent
	GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback
	GetCanonicalAfterAgentCallbacks() []SingleAgentCallback
}

// SingleAgentCallback defines the signature for agent lifecycle callbacks.
type SingleAgentCallback func(callbackCtx *CallbackContext) (*Content, error)

// BaseAgent provides a common structure for agents.
type BaseAgent struct {
	Name                   string                `json:"name"`
	Description            string                `json:"description,omitempty"`
	Parent                 Agent                 `json:"-"` // Exclude from JSON, handle manually
	Subs                   []Agent               `json:"subAgents,omitempty"`
	BeforeAgentCallback    []SingleAgentCallback `json:"-"` // Callbacks are not typically part of serialized state
	AfterAgentCallback     []SingleAgentCallback `json:"-"`
	agentInvocationContext *InvocationContext    // Internal context for the current run
}

// NewBaseAgent creates a new BaseAgent.
// Name must be a valid Go identifier and not "user".
func NewBaseAgent(name string, description string) (*BaseAgent, error) {
	if !isValidIdentifier(name) {
		return nil, fmt.Errorf("invalid agent name: `%s`. Agent name must be a valid identifier", name)
	}
	if name == "user" {
		return nil, fmt.Errorf("agent name cannot be `user`. `user` is reserved for end-user's input")
	}
	return &BaseAgent{
		Name:        name,
		Description: description,
		Subs:        make([]Agent, 0),
	}, nil
}

func (ba *BaseAgent) GetName() string {
	return ba.Name
}

func (ba *BaseAgent) GetDescription() string {
	return ba.Description
}

func (ba *BaseAgent) ParentAgent() Agent {
	return ba.Parent
}

func (ba *BaseAgent) SetParentAgent(parent Agent) error {
	if ba.Parent != nil && ba.Parent != parent {
		return fmt.Errorf("agent `%s` already has a parent agent: `%s`, cannot add to: `%s`", ba.Name, ba.Parent.GetName(), parent.GetName())
	}
	ba.Parent = parent
	return nil
}

func (ba *BaseAgent) SubAgents() []Agent {
	return ba.Subs
}

func (ba *BaseAgent) AddSubAgent(subAgent Agent) {
	if err := subAgent.SetParentAgent(ba); err != nil {
		// Or handle error as appropriate, e.g. log and skip
		fmt.Printf("Error setting parent for sub-agent %s: %v\n", subAgent.GetName(), err)
		return
	}
	ba.Subs = append(ba.Subs, subAgent)
}

// RunAsync is the entry method to run an agent via text-based conversation.
func (ba *BaseAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	// In Go, we'd typically use channels for async generators.
	// The actual implementation of _runAsyncImpl would push events to this channel.
	// The telemetry.tracer logic would wrap the core processing.
	// For this snippet, we'll simulate the channel.

	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run [%s]", ba.Name)):
		
		event, err := ba.handleBeforeAgentCallback(ctx)
		if err != nil {
			// ì–´ë–»ê²Œ ì—ëŸ¬ë¥¼ outChìœ¼ë¡œ ë³´ë‚¼ ì§€ ê³ ë¯¼ í•„ìš”. Eventì— Error í•„ë“œ ì¶”ê°€?
			// For now, log and potentially send an error event.
			fmt.Printf("Error in beforeAgentCallback for %s: %v\n", ba.Name, err)
			// outCh <- &events.Event{Error: err.Error()} // Conceptual
			return
		}
		if event != nil {
			outCh <- event
		}
		if ctx.EndInvocation {
			return
		}

		err = ba.runAsyncImpl(ctx, outCh) // Pass channel to implementation
		if err != nil {
			fmt.Printf("Error in runAsyncImpl for %s: %v\n", ba.Name, err)
			return
		}

		if ctx.EndInvocation {
			return
		}

		event, err = ba.handleAfterAgentCallback(ctx)
		if err != nil {
			fmt.Printf("Error in afterAgentCallback for %s: %v\n", ba.Name, err)
			return
		}
		if event != nil {
			outCh <- event
		}
	}()

	return outCh, nil
}

// runAsyncImpl is the core logic to run this agent via text-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runAsyncImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	// This is where the Python _run_async_impl's `yield Event(...)` would happen.
	// In Go, we send to the channel.
	// Example: outCh <- &events.Event{Author: ba.Name, Content: ...}
	return fmt.Errorf("_runAsyncImpl for %T is not implemented", ba)
}

// RunLive is the entry method to run an agent via video/audio-based conversation.
func (ba *BaseAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run_live [%s]", ba.Name)):
		err := ba.runLiveImpl(ctx, outCh) // Pass channel
		if err != nil {
			fmt.Printf("Error in runLiveImpl for %s: %v\n", ba.Name, err)
			// Potentially send an error event
		}
	}()
	return outCh, nil
}

// runLiveImpl is the core logic to run this agent via video/audio-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runLiveImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	return fmt.Errorf("_runLiveImpl for %T is not implemented", ba)
}

func (ba *BaseAgent) RootAgent() Agent {
	current := ba
	for current.ParentAgent() != nil {
		current = current.ParentAgent().(*BaseAgent) // Assuming BaseAgent for simplicity
	}
	return current
}

func (ba *BaseAgent) FindAgent(name string) Agent {
	if ba.Name == name {
		return ba
	}
	return ba.FindSubAgent(name)
}

func (ba *BaseAgent) FindSubAgent(name string) Agent {
	for _, subAgent := range ba.Subs {
		if found := subAgent.FindAgent(name); found != nil {
			return found
		}
	}
	return nil
}

func (ba *BaseAgent) createInvocationContext(parentCtx *InvocationContext) *InvocationContext {
	// Create a copy and update agent and branch
	// This is a simplified deep copy. Real implementation needs care.
	ctxCopy := *parentCtx
	ctxCopy.Agent = ba
	if parentCtx.Branch != "" {
		ctxCopy.Branch = fmt.Sprintf("%s.%s", parentCtx.Branch, ba.Name)
	} else {
		ctxCopy.Branch = ba.Name
	}
	return &ctxCopy
}

func (ba *BaseAgent) GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback {
	return ba.BeforeAgentCallback
}

func (ba *BaseAgent) GetCanonicalAfterAgentCallbacks() []SingleAgentCallback {
	return ba.AfterAgentCallback
}

func (ba *BaseAgent) handleBeforeAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalBeforeAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalBeforeAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("beforeAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions, // Assuming EventActions is part of CallbackContext
			}
			ctx.EndInvocation = true
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			Actions:      callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

func (ba *BaseAgent) handleAfterAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalAfterAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalAfterAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("afterAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions,
			}
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			// Content might be nil if callback only changed state
			Actions: callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

// isValidIdentifier checks if a string is a valid Go identifier (simplified).
// Go identifiers are more restrictive than Python's.
func isValidIdentifier(name string) bool {
	if name == "" {
		return false
	}
	// Regex for a simplified Go identifier: starts with a letter, followed by letters or digits.
	// Go allows Unicode letters/digits. This regex is simplified.
	// Actual Go spec: letter { letter | unicode_digit } .
	// For ADK agent names, we might enforce stricter ASCII for interoperability or simplicity.
	matched, _ := regexp.MatchString(`^[a-zA-Z_][a-zA-Z0-9_]*$`, name)
	return matched
}

# /Users/gopher/Desktop/workspace/adk-go/agents/llm_agent.go
package agents

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/tools" // Placeholder for tools package
	// "github.com/KennethanCeyer/adk-go/models" // Placeholder for models package
	// "github.com/KennethanCeyer/adk-go/examples" // Placeholder for examples package
	// "github.com/KennethanCeyer/adk-go/planners" // Placeholder for planners package
	// "github.com/KennethanCeyer/adk-go/codeexecutors" // Placeholder for code executors
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, genai.Content etc.
)

// --- Forward declarations / Placeholders ---
// These would be actual types in a full Go ADK.

type BaseLlm struct { // Placeholder for models.BaseLlm
	ModelName string
}

type GenerateContentConfig struct { // Placeholder for genai.GenerateContentConfig
	SystemInstruction   string        `json:"systemInstruction,omitempty"`
	Tools               []Tool        `json:"tools,omitempty"`         // Placeholder for genai.Tool
	ResponseSchema      interface{}   `json:"responseSchema,omitempty"` // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string        `json:"responseMimeType,omitempty"`
	ThinkingConfig      interface{}   `json:"thinkingConfig,omitempty"`   // Placeholder for genai.ThinkingConfig
	Temperature         *float32      `json:"temperature,omitempty"`
	SafetySettings      []interface{} `json:"safetySettings,omitempty"` // Placeholder for genai.SafetySetting
}

type LlmRequest struct { // Placeholder for models.LlmRequest
	Model               string
	Config              *GenerateContentConfig
	Contents            []Content
	ToolsDict           map[string]Tool // Placeholder
	LiveConnectConfig   interface{}     // Placeholder for genai.LiveConnectConfig
	SystemInstruction   string          // Added for simplicity, original ADK appends to config
	OutputSchema        interface{}     // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string
}

func (lr *LlmRequest) AppendInstructions(instructions []string) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	if lr.Config.SystemInstruction != "" {
		lr.Config.SystemInstruction += "\n\n"
	}
	lr.Config.SystemInstruction += strings.Join(instructions, "\n\n")
}

func (lr *LlmRequest) SetOutputSchema(schema interface{}) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	lr.Config.ResponseSchema = schema
	lr.Config.ResponseMIMEType = "application/json"
}

type LlmResponse struct { // Placeholder for models.LlmResponse
	Content         *Content    `json:"content,omitempty"`
	ErrorCode       string      `json:"errorCode,omitempty"`
	ErrorMessage    string      `json:"errorMessage,omitempty"`
	Partial         bool        `json:"partial,omitempty"`
	CustomMetadata  map[string]interface{} `json:"customMetadata,omitempty"`
}


type Tool interface { // Placeholder for tools.BaseTool
	GetName() string
	GetDescription() string
	GetDeclaration() (*ToolDeclaration, error) // Placeholder for genai.FunctionDeclaration
	ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error
}

type ToolDeclaration struct { // Placeholder for genai.FunctionDeclaration
	Name        string      `json:"name"`
	Description string      `json:"description"`
	Parameters  interface{} `json:"parameters"`
}

type FunctionTool struct { // Placeholder for tools.FunctionTool
	BaseToolImpl
	fn interface{}
}

func NewFunctionTool(fn interface{}) *FunctionTool {
	// Simplified name/description extraction
	name := "unknown_function"
	// In Go, reflection for function name/doc is more involved
	// For now, users might need to provide this explicitly if FunctionTool wraps a raw func
	return &FunctionTool{BaseToolImpl: BaseToolImpl{ToolName: name}, fn: fn}
}

func (ft *FunctionTool) GetName() string { return ft.ToolName }
func (ft *FunctionTool) GetDescription() string { return ft.ToolDescription }
func (ft *FunctionTool) GetDeclaration() (*ToolDeclaration, error) { /* TODO */ return nil, nil }
func (ft *FunctionTool) ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error { /* TODO */ return nil }


type BaseToolset interface { // Placeholder for tools.BaseToolset
	GetTools(readonlyCtx *ReadonlyContext) ([]Tool, error)
}

type Example struct { // Placeholder for examples.Example
	Input  Content   `json:"input"`
	Output []Content `json:"output"`
}

type BaseExampleProvider interface { // Placeholder for examples.BaseExampleProvider
	GetExamples(query string) ([]Example, error)
}

type Planner interface { // Placeholder for planners.BasePlanner
	// Methods would be defined here
}

type CodeExecutor interface { // Placeholder for codeexecutors.BaseCodeExecutor
	// Methods would be defined here
}

type ToolContext struct { // Placeholder for tools.ToolContext
	InvocationContext *InvocationContext
	State             MutableState
	// ... other fields like FunctionCallID, EventActions
}

// Represents ReadonlyContext
type ReadonlyContext struct {
	InvocationContext *InvocationContext
}

func (rc *ReadonlyContext) GetState() map[string]interface{} {
	// Return a read-only copy or wrapper
	// For simplicity, returning the direct map for now, assuming no modification.
	if rc.InvocationContext != nil && rc.InvocationContext.Session != nil {
		return rc.InvocationContext.Session.State
	}
	return make(map[string]interface{})
}


// LLMProvider interface (simplified from previous example)
type LLMProvider interface {
    GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error)
    GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error)
    // Potentially a Connect method for bidirectional streaming if needed for live mode
}

// MockLLMProvider for LlmAgent
type MockLLM struct {
	BaseLlm
}

func (m *MockLLM) GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error) {
	// Simulate LLM call
	log.Printf("MockLLM GenerateContentStream called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	ch := make(chan *LlmResponse, 1)
	go func() {
		defer close(ch)
		// Simplified: just echo back part of the first user message if available
		responseText := "Default mock response"
		if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
			responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
		}
		ch <- &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}
	}()
	return ch, nil
}

func (m *MockLLM) GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error) {
	log.Printf("MockLLM GenerateContent called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	responseText := "Default mock response"
	if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
		responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
	}
	return &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}, nil
}


// SingleBeforeModelCallback defines the signature for a single before-model callback.
type SingleBeforeModelCallback func(callbackCtx *CallbackContext, llmReq *LlmRequest) (*LlmResponse, error)

// SingleAfterModelCallback defines the signature for a single after-model callback.
type SingleAfterModelCallback func(callbackCtx *CallbackContext, llmResp *LlmResponse) (*LlmResponse, error)

// SingleBeforeToolCallback defines the signature for a single before-tool callback.
type SingleBeforeToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext) (map[string]interface{}, error)

// SingleAfterToolCallback defines the signature for a single after-tool callback.
type SingleAfterToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext, toolResponse map[string]interface{}) (map[string]interface{}, error)

// InstructionProvider defines a function that provides an instruction string.
type InstructionProvider func(readonlyCtx *ReadonlyContext) (string, error)

// ToolUnion represents a type that can be a callable, a BaseTool, or a BaseToolset.
// In Go, this would typically be handled by interfaces or by checking types at runtime.
// For this conceptual translation, we'll use `interface{}` and runtime type assertions.
type ToolUnion interface{}

// ExamplesUnion represents types that can provide examples.
type ExamplesUnion interface{} // list[Example] or BaseExampleProvider

// LlmAgent is an LLM-based Agent.
type LlmAgent struct {
	BaseAgent
	AgentModel                  interface{} // string or *BaseLlm (actual model instance)
	AgentInstruction            interface{} // string or InstructionProvider
	AgentGlobalInstruction      interface{} // string or InstructionProvider
	AgentTools                  []ToolUnion
	AgentGenerateContentConfig  *GenerateContentConfig
	DisallowTransferToParent    bool
	DisallowTransferToPeers     bool
	IncludeContents             string // "default" or "none"
	InputSchema                 interface{} // reflect.Type of a struct, or nil
	OutputSchema                interface{} // reflect.Type of a struct, or nil
	OutputKey                   string
	AgentPlanner                Planner
	AgentCodeExecutor           CodeExecutor
	AgentExamples               ExamplesUnion
	AgentBeforeModelCallbacks   []SingleBeforeModelCallback
	AgentAfterModelCallbacks    []SingleAfterModelCallback
	AgentBeforeToolCallbacks    []SingleBeforeToolCallback
	AgentAfterToolCallbacks     []SingleAfterToolCallback

	// Internal flow strategy
	llmFlow LlmFlow
}

// LlmFlow defines the behavior of the LLM agent.
type LlmFlow interface {
	RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error)
	RunLive(invocationCtx *InvocationContext) (<-chan *Event, error)
}

// NewLlmAgent creates a new LlmAgent.
// This constructor attempts to mirror Pydantic's initialization logic.
func NewLlmAgent(name, description string, model interface{}, instruction interface{}) (*LlmAgent, error) {
	base, err := NewBaseAgent(name, description)
	if err != nil {
		return nil, err
	}

	agent := &LlmAgent{
		BaseAgent:        *base,
		AgentModel:       model,
		AgentInstruction: instruction,
		IncludeContents:  "default",
		// Default to AutoFlow which includes SingleFlow + agent transfer
		llmFlow: newAutoFlow(), // Placeholder for flow initialization
	}

	// Simulating Pydantic's model_post_init and field_validator logic
	if err := agent.validateAndProcessConfig(); err != nil {
		return nil, err
	}

	return agent, nil
}

func (a *LlmAgent) validateAndProcessConfig() error {
	if err := a.checkOutputSchema(); err != nil {
		return err
	}
	if err := a.validateGenerateContentConfig(); err != nil {
		return err
	}
	// In Python, Pydantic's model_post_init calls __set_parent_agent_for_sub_agents
	// This is handled by AddSubAgent in Go's BaseAgent.

	// Determine flow based on transfer disallowed flags
	if a.DisallowTransferToParent && a.DisallowTransferToPeers && len(a.Subs) == 0 {
		a.llmFlow = newSingleFlow() // Placeholder
	} else {
		a.llmFlow = newAutoFlow() // Placeholder
	}

	return nil
}


// RunAsync implements the Agent interface.
// It delegates to the internal LlmFlow.
func (a *LlmAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx) // from BaseAgent
	a.agentInvocationContext = ctx // Store context for this agent's run
	return a.llmFlow.RunAsync(ctx)
}

// RunLive implements the Agent interface.
func (a *LlmAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx)
	a.agentInvocationContext = ctx
	return a.llmFlow.RunLive(ctx)
}


func (a *LlmAgent) CanonicalModel() (BaseLlm, error) {
	if modelInst, ok := a.AgentModel.(BaseLlm); ok {
		return modelInst, nil
	}
	if modelStr, ok := a.AgentModel.(string); ok && modelStr != "" {
		// Placeholder for LLMRegistry.NewLlm(modelStr)
		// In a real Go ADK, this would look up and instantiate the model.
		return MockLLM{BaseLlm{ModelName: modelStr}}, nil
	}

	current := a.ParentAgent()
	for current != nil {
		if llmAgent, ok := current.(*LlmAgent); ok {
			return llmAgent.CanonicalModel()
		}
		current = current.ParentAgent()
	}
	return MockLLM{}, fmt.Errorf("no model found for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentInstruction.(func(ctx *ReadonlyContext) (string, error)); ok { // Allow func type as well
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, fmt.Errorf("invalid instruction type for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentGlobalInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentGlobalInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentGlobalInstruction.(func(ctx *ReadonlyContext) (string, error)); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, nil // Global instruction can be empty
}

func (a *LlmAgent) CanonicalTools(ctx *ReadonlyContext) ([]Tool, error) {
	var resolvedTools []Tool
	for _, toolUnion := range a.AgentTools {
		switch t := toolUnion.(type) {
		case Tool:
			resolvedTools = append(resolvedTools, t)
		case func(interface{}) interface{}: // Simplified check for a callable
			// This is a very basic check. Robustly handling arbitrary callables
			// and converting them to tools.Tool would require more reflection
			// or specific registration mechanisms.
			// For now, wrapping it in a conceptual FunctionTool.
			resolvedTools = append(resolvedTools, NewFunctionTool(t))
		case BaseToolset:
			toolsFromSet, err := t.GetTools(ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to get tools from toolset: %w", err)
			}
			resolvedTools = append(resolvedTools, toolsFromSet...)
		default:
			return nil, fmt.Errorf("unsupported tool type: %T", t)
		}
	}
	return resolvedTools, nil
}


func (a *LlmAgent) GetCanonicalBeforeModelCallbacks() []SingleBeforeModelCallback {
	return a.AgentBeforeModelCallbacks
}

func (a *LlmAgent) GetCanonicalAfterModelCallbacks() []SingleAfterModelCallback {
	return a.AgentAfterModelCallbacks
}

func (a *LlmAgent) GetCanonicalBeforeToolCallbacks() []SingleBeforeToolCallback {
	return a.AgentBeforeToolCallbacks
}

func (a *LlmAgent) GetCanonicalAfterToolCallbacks() []SingleAfterToolCallback {
	return a.AgentAfterToolCallbacks
}

func (a *LlmAgent) maybeSaveOutputToState(event *Event) {
	if a.OutputKey == "" || !eventIsFinalResponse(event) || event.Content == nil || len(event.Content.Parts) == 0 {
		return
	}

	var resultData interface{}
	var combinedText []string
	for _, part := range event.Content.Parts {
		if part.Text != "" {
			combinedText = append(combinedText, part.Text)
		}
		// In a real scenario, you'd handle other part types like function calls/responses if they constituted the "output"
	}
	fullText := strings.Join(combinedText, "\n")


	if a.OutputSchema != nil {
		// In Go, Pydantic's model_validate_json would be equivalent to json.Unmarshal into a struct.
		// This requires a.OutputSchema to be a reflect.Type of the target struct.
		// For simplicity, this placeholder assumes fullText is valid JSON for the schema.
		// A real implementation would use json.Unmarshal(byte(fullText), &targetStructInstance)
		// and then potentially convert targetStructInstance to map[string]interface{} if needed.
		var tempMap map[string]interface{}
		// This is a simplified placeholder. Real unmarshalling and validation needed.
		if err := json.Unmarshal([]byte(fullText), &tempMap); err == nil {
			resultData = tempMap
		} else {
			log.Printf("Warning: Failed to parse output for key '%s' against schema: %v. Storing raw text.", a.OutputKey, err)
			resultData = fullText // Fallback to raw text
		}
	} else {
		resultData = fullText
	}

	if event.Actions.StateDelta == nil {
		event.Actions.StateDelta = make(map[string]interface{})
	}
	event.Actions.StateDelta[a.OutputKey] = resultData
}

func (a *LlmAgent) checkOutputSchema() error {
	if a.OutputSchema == nil {
		return nil
	}

	if !a.DisallowTransferToParent || !a.DisallowTransferToPeers {
		log.Println(
			fmt.Sprintf("Warning: Invalid config for agent %s: outputSchema cannot co-exist with agent transfer configurations. Setting disallowTransferToParent=true, disallowTransferToPeers=true", a.Name),
		)
		a.DisallowTransferToParent = true
		a.DisallowTransferToPeers = true
	}

	if len(a.SubAgents()) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, subAgents must be empty to disable agent transfer", a.Name)
	}

	if len(a.AgentTools) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, tools must be empty", a.Name)
	}
	return nil
}

func (a *LlmAgent) validateGenerateContentConfig() error {
	if a.AgentGenerateContentConfig == nil {
		a.AgentGenerateContentConfig = &GenerateContentConfig{} // Ensure it's not nil
		return nil
	}
	if a.AgentGenerateContentConfig.ThinkingConfig != nil {
		return fmt.Errorf("thinkingConfig should be set via LlmAgent.Planner")
	}
	if len(a.AgentGenerateContentConfig.Tools) > 0 {
		return fmt.Errorf("all tools must be set via LlmAgent.Tools")
	}
	if a.AgentGenerateContentConfig.SystemInstruction != "" {
		return fmt.Errorf("system instruction must be set via LlmAgent.Instruction or LlmAgent.GlobalInstruction")
	}
	if a.AgentGenerateContentConfig.ResponseSchema != nil {
		return fmt.Errorf("response schema must be set via LlmAgent.OutputSchema")
	}
	return nil
}

// Helper, would be part of event logic
func eventIsFinalResponse(e *Event) bool {
	if e.Actions.StateDelta["skipSummarization"] == true { // Assuming skipSummarization is stored in StateDelta
		return true
	}
	// Simplified: In Python ADK, this checks for function calls/responses and partial flags.
	// Here, we'll assume an event is final if it's not marked partial and has content.
	return e.Content != nil && !e.Partial // Placeholder for e.Partial
}

// --- Placeholder Flow Implementations ---
// In a real Go ADK, these would be more fleshed out and likely in their own package.

type SingleFlow struct {
	// RequestProcessors  []BaseLlmRequestProcessor  // Placeholder
	// ResponseProcessors []BaseLlmResponseProcessor // Placeholder
}

func newSingleFlow() *SingleFlow {
	// Initialize processors
	return &SingleFlow{}
}

func (sf *SingleFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)
		llmReq := &LlmRequest{} // Initialize LlmRequest

		// Simulate preprocessing (would iterate through sf.RequestProcessors)
		if err := sf.preprocessAsync(invocationCtx, llmReq); err != nil {
			log.Printf("Error during preprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()} // Conceptual error event
			return
		}

		llmAgent, ok := invocationCtx.Agent.(*LlmAgent)
		if !ok {
			log.Printf("Error: agent is not an LlmAgent")
			return
		}
		llm, err := llmAgent.CanonicalModel()
		if err != nil {
			log.Printf("Error getting canonical model: %v", err)
			return
		}
		
		// Simplified LLM call for SingleFlow
		// A real implementation would handle streaming, function calling loops, etc.
		var llmResponse *LlmResponse

		// This is a simplification. The Python ADK has a loop for multiple LLM calls if tools are used.
		// For this Go conceptual translation, we'll assume a single LLM call or a very simplified tool use.
		if provider, ok := llm.(LLMProvider); ok {
			// Use GenerateContent for non-streaming for simplicity in this conceptual flow
			llmResponse, err = provider.GenerateContent(context.Background(), llmReq)
			if err != nil {
				log.Printf("Error calling LLM: %v", err)
				// outCh <- &Event{Error: err.Error()}
				return
			}
		} else {
			log.Printf("Error: LLM model does not implement LLMProvider interface")
			return
		}
		

		modelResponseEvent := &Event{
			ID:            newID(), // Generate new event ID
			InvocationID:  invocationCtx.InvocationID,
			Author:        invocationCtx.Agent.GetName(),
			Branch:        invocationCtx.Branch,
			Content:       llmResponse.Content, // Simplified
			// Actions, ErrorCode, ErrorMessage, etc. would be populated
		}

		// Simulate postprocessing (would iterate through sf.ResponseProcessors)
		if err := sf.postprocessAsync(invocationCtx, llmReq, llmResponse, modelResponseEvent, outCh); err != nil {
			log.Printf("Error during postprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()}
			return
		}
		
		// If not handled by post-processors (e.g. tool calls), yield the direct LLM response event
		if modelResponseEvent.Content != nil || modelResponseEvent.Actions.StateDelta != nil { // Yield if there's something to yield
			outCh <- modelResponseEvent
		}

	}()
	return outCh, nil
}


func (sf *SingleFlow) RunLive(invocationCtx *InvocationContext) (<-chan *Event, error) {
	// Simplified RunLive, a real one would be more complex with bidirectional streaming
	return sf.RunAsync(invocationCtx) // Fallback for now
}

func (sf *SingleFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	// Conceptual: Iterate through request_processors from Python ADK
	// Example: basic.request_processor, instructions.request_processor, etc.
	// For this Go version, we'll call helper methods directly for some core logic.
	agent := invocationCtx.Agent.(*LlmAgent) // Assuming LlmAgent

	// Basic processor logic
	modelInstance, err := agent.CanonicalModel()
	if err != nil {
		return err
	}
	llmReq.Model = modelInstance.ModelName // Assuming BaseLlm has ModelName
	if agent.AgentGenerateContentConfig != nil {
		cfgCopy := *agent.AgentGenerateContentConfig
		llmReq.Config = &cfgCopy
	} else {
		llmReq.Config = &GenerateContentConfig{}
	}
	if agent.OutputSchema != nil {
		llmReq.SetOutputSchema(agent.OutputSchema)
	}
	// ... (add live_connect_config population from RunConfig)

	// Instructions processor logic
	// Global Instruction
	rootAgent := agent.RootAgent().(*LlmAgent) // Assuming LlmAgent
	if rootAgent.AgentGlobalInstruction != nil {
		instr, _, err := rootAgent.CanonicalGlobalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		// Simplified: In Python, instructions_utils.inject_session_state is used.
		// For Go, this would involve a similar template processing.
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	// Agent Instruction
	if agent.AgentInstruction != nil {
		instr, _, err := agent.CanonicalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	
	// Identity processor
	var si []string
	si = append(si, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName()))
	if agent.GetDescription() != "" {
		si = append(si, fmt.Sprintf(" The description about you is \"%s\"", agent.GetDescription()))
	}
	llmReq.AppendInstructions(si)

	// Contents processor logic (simplified)
	if agent.IncludeContents != "none" {
		llmReq.Contents = getContentsForLlm(invocationCtx.Branch, invocationCtx.Session.Events, agent.GetName())
	}

	// Tool processing (adding declarations to llmReq.Config.Tools)
	tools, err := agent.CanonicalTools(&ReadonlyContext{InvocationContext: invocationCtx})
	if err != nil { return err}
	if len(tools) > 0 {
		if llmReq.Config.Tools == nil {
			llmReq.Config.Tools = make([]Tool, 0)
		}
	}
	for _, tool := range tools {
		// Simplified: directly appending, Python ADK has more complex logic for FunctionDeclaration
		llmReq.Config.Tools = append(llmReq.Config.Tools, tool) // This assumes tool itself is the declaration or can provide it
		
		// Store tool in llmReq.ToolsDict for later execution lookup by name
		if llmReq.ToolsDict == nil {
			llmReq.ToolsDict = make(map[string]Tool)
		}
		llmReq.ToolsDict[tool.GetName()] = tool
	}

	return nil
}

func (sf *SingleFlow) postprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest, llmResp *LlmResponse, modelResponseEvent *Event, outCh chan<- *Event) error {
	// Conceptual: Iterate through response_processors
	// Example: _nl_planning.response_processor, _code_execution.response_processor
	
	// Handle function calls (simplified from functions.py)
	if llmResp.Content != nil {
		var functionCalls []struct{ Name string; Args map[string]interface{}; ID string } // Simplified representation
		for _, part := range llmResp.Content.Parts {
			// Placeholder: In Python this uses part.function_call
			// For Go, we'd check a similar field if LlmResponse.Content.Parts had FunctionCall type
			// For this example, assume FunctionCall is identified by a specific structure in Part
			// For now, this logic is highly simplified and conceptual
			if strings.HasPrefix(part.Text, "CALL_TOOL:") { // Very basic trigger
				parts := strings.SplitN(strings.TrimPrefix(part.Text, "CALL_TOOL:"), "(", 2)
				name := parts[0]
				argsStr := ""
				if len(parts) > 1 {
					argsStr = strings.TrimSuffix(parts[1], ")")
				}
				var args map[string]interface{}
				json.Unmarshal([]byte(argsStr), &args) // Simplified arg parsing
				functionCalls = append(functionCalls, struct{ Name string; Args map[string]interface{}; ID string }{Name: name, Args: args, ID: newID()})

				// Populate client function call ID if LLM didn't provide one
				modelResponseEvent.Content.Parts[0].Text = "" // Clear text part conceptually
				// modelResponseEvent.Content.Parts[0].FunctionCall = &genai.FunctionCall{Name: name, Args: args, ID: ...}
			}
		}

		if len(functionCalls) > 0 {
			// In Python ADK, this calls functions.handle_function_calls_async
			// which then calls tool.run_async and creates a response event.
			// This is a very complex part involving callbacks, state updates, etc.
			// Here's a highly simplified conceptual flow:
			var toolResponseParts []Part
			for _, fc := range functionCalls {
				tool, ok := llmReq.ToolsDict[fc.Name]
				if !ok {
					log.Printf("Tool %s not found", fc.Name)
					toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Error: Tool %s not found", fc.Name)})
					continue
				}
				// Conceptual tool execution
				// toolResult, err := tool.Execute(context.Background(), fc.Args) // This is complex
				// Simplified:
				toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Tool %s called with %v (mocked)", fc.Name, fc.Args)})
			}
			
			// Create a new event for tool responses and send it
			toolResponseEvent := &Event{
				ID:            newID(),
				InvocationID:  invocationCtx.InvocationID,
				Author:        invocationCtx.Agent.GetName(),
				Branch:        invocationCtx.Branch,
				Content:       &Content{Role: "model", Parts: toolResponseParts}, // Simplified: role should be 'user' for function response in Gemini
			}
			outCh <- toolResponseEvent
			
			// The original LLM response event (modelResponseEvent) that *contained* the function call
			// is also yielded. Then, the flow would typically make *another* LLM call with the tool responses.
			// For simplicity, we will stop here in this conceptual snippet.
			// The python ADK has a loop in _run_one_step_async for this.
			modelResponseEvent.Content = nil // After tool call, the original content is usually superseded or followed by tool response processing
		}
	}
	
	return nil
}

type AutoFlow struct {
	SingleFlow
	// agentTransferRequestProcessor // Placeholder for agent_transfer.request_processor
}

func newAutoFlow() *AutoFlow {
	return &AutoFlow{SingleFlow: *newSingleFlow()}
}

func (af *AutoFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	if err := af.SingleFlow.preprocessAsync(invocationCtx, llmReq); err != nil {
		return err
	}
	// Agent transfer logic
	agent := invocationCtx.Agent.(*LlmAgent)
	transferTargets := getTransferTargets(agent)
	if len(transferTargets) > 0 {
		llmReq.AppendInstructions([]string{buildTargetAgentsInstructions(agent, transferTargets)})
		// Conceptually add transfer_to_agent tool
		// transferTool := NewFunctionTool(transferToAgent) // transferToAgent needs to be defined
		// ... add transferTool to llmReq.Config.Tools and llmReq.ToolsDict
	}
	return nil
}

// Helper to simulate Python's list comprehension and filtering for contents
func getContentsForLlm(currentBranch string, allEvents []*Event, agentName string) []Content {
	var contents []Content
	// This is a simplified version of Python ADK's _get_contents, which has complex logic
	// for rearranging events, handling async function responses, and filtering by branch.
	for _, event := range allEvents {
		if event.Content != nil && len(event.Content.Parts) > 0 {
			// Simplified branch check and foreign event conversion
			if isEventOnBranch(currentBranch, event) && !isAuthEvent(event) {
				if isOtherAgentReply(agentName, event) {
					contents = append(contents, *convertForeignEvent(event))
				} else {
					// Deep copy content before modifying (e.g. removing client func call ID)
					contentCopy := *event.Content 
					// removeClientFunctionCallID(&contentCopy) // Placeholder for actual logic
					contents = append(contents, contentCopy)
				}
			}
		}
	}
	return contents
}

// Simplified placeholders for helper functions from Python's contents.py
func isEventOnBranch(invocationBranch string, event *Event) bool { 
	if invocationBranch == "" || event.Branch == "" { return true }
	return strings.HasPrefix(invocationBranch, event.Branch)
}
func isAuthEvent(event *Event) bool { return false /* TODO */ }
func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return event.Author != currentAgentName && event.Author != "user"
}
func convertForeignEvent(event *Event) *Content { 
	// Simplified conversion
	return &Content{Role: "user", Parts: []Part{{Text: fmt.Sprintf("Context from %s: %s", event.Author, event.Content.Parts[0].Text)}}}
}
func newID() string { return "temp-id" } // Placeholder for ID generation

func getTransferTargets(agent *LlmAgent) []Agent {
    var targets []Agent
    targets = append(targets, agent.SubAgents()...) // Assuming SubAgents returns []Agent

    if agent.ParentAgent() != nil {
        if llmParent, ok := agent.ParentAgent().(*LlmAgent); ok {
            if !agent.DisallowTransferToParent {
                targets = append(targets, llmParent)
            }
            if !agent.DisallowTransferToPeers {
                for _, peerAgent := range llmParent.SubAgents() {
                    if peerAgent.GetName() != agent.GetName() {
                        targets = append(targets, peerAgent)
                    }
                }
            }
        }
    }
    return targets
}

func buildTargetAgentsInfo(targetAgent Agent) string {
    return fmt.Sprintf("\nAgent name: %s\nAgent description: %s\n", targetAgent.GetName(), targetAgent.GetDescription())
}

func buildTargetAgentsInstructions(agent *LlmAgent, targetAgents []Agent) string {
    var sb strings.Builder
    sb.WriteString("\nYou have a list of other agents to transfer to:\n")
    for _, target := range targetAgents {
        sb.WriteString(buildTargetAgentsInfo(target))
    }
    sb.WriteString("\nIf you are the best to answer the question according to your description, you can answer it.")
    sb.WriteString("\nIf another agent is better for answering the question according to its description, call `transfer_to_agent` function to transfer the question to that agent. When transferring, do not generate any text other than the function call.\n")

    if agent.ParentAgent() != nil {
        sb.WriteString(fmt.Sprintf("\nYour parent agent is %s. If neither the other agents nor you are best for answering the question according to the descriptions, transfer to your parent agent. If you don't have parent agent, try answer by yourself.\n", agent.ParentAgent().GetName()))
    }
    return sb.String()
}

// Placeholder for instruction template processing
func processInstructionTemplate(template string, ctx *ReadonlyContext) (string, error) {
	// In Python, this uses string.Formatter with a custom class to access state.
	// In Go, this would require a more explicit template engine or string replacement.
	// This is a very simplified version.
	processed := template
	// Regex to find {key} or {key?}
	re := regexp.MustCompile(`{([^}]+)}`)
	state := ctx.GetState() // Assuming GetState returns map[string]interface{}

	processed = re.ReplaceAllStringFunc(processed, func(match string) string {
		keyWithOptionalMarker := strings.Trim(match, "{} ")
		key := strings.TrimSuffix(keyWithOptionalMarker, "?")
		isOptional := strings.HasSuffix(keyWithOptionalMarker, "?")

		// Handle artifact.filename format
		if strings.HasPrefix(key, "artifact.") {
			// artifactName := strings.TrimPrefix(key, "artifact.")
			// artifactContent, err := loadArtifact(ctx.InvocationContext, artifactName) // Placeholder
			// if err != nil {
			// 	if isOptional { return "" }
			// 	log.Printf("Error loading artifact %s: %v", artifactName, err)
			// 	return match // return original on error
			// }
			// return artifactContent
			return "[ARTIFACT_CONTENT_PLACEHOLDER]" // Simplified
		}

		val, ok := state[key]
		if ok {
			return fmt.Sprintf("%v", val)
		}
		if isOptional {
			return ""
		}
		log.Printf("Warning: Context variable not found and not optional: `%s`", key)
		return match // Return original placeholder if not found and not optional
	})
	return processed, nil
}

# /Users/gopher/Desktop/workspace/adk-go/agents/interfaces/interfaces.go
package agents

import (
	"context"

	llmprovideriface "github.com/KennethanCeyer/adk-go/llmproviders"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools"
)

type Agent interface {
	GetName() string
	GetDescription() string
}

// LlmAgent defines agents that interact with LLMs.
type LlmAgent interface {
	Agent // Embeds base Agent capabilities
	GetModelIdentifier() string
	GetSystemInstruction() *modelstypes.Content
	GetTools() []toolstypes.Tool
	GetLLMProvider() llmprovideriface.LLMProvider

	Process(
		ctx context.Context,
		history []modelstypes.Content, // Conversation history (user, model, function turns)
		latestContent modelstypes.Content, // Latest input (user message or function response)
	) (*modelstypes.Content, error) // Agent's response (model text or next function call)
}

# /Users/gopher/Desktop/workspace/adk-go/models/types/types.go
package types

type Part struct {
	Text             *string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
}

type Content struct {
	Parts []Part
	Role  string // e.g., "user", "model"
}

type FunctionCall struct {
	Name string
	Args any // Typically map[string]any
}

type FunctionResponse struct {
	Name     string
	Response any // Typically map[string]any
}

type GenerateContentConfig struct {
	Temperature     *float32
	TopP            *float32
	TopK            *int32
	MaxOutputTokens *int32
	StopSequences   []string
	// SystemInstruction can be part of LlmRequest or set on the LLM client directly
}

type LlmRequest struct {
	ModelIdentifier string
	SystemInstruction *Content // Optional system instruction
	Contents        []Content // Conversation history + current message
	Config          *GenerateContentConfig
	// Tools are typically configured on the LLM client or passed to the GenerateContent call
}

type LlmResponse struct {
	Content *Content
	// Other fields like FinishReason, TokenCount can be added
}

# /Users/gopher/Desktop/workspace/adk-go/README.md
# ADK-Go: Agent Development Kit in Go (Runnable HelloWorld Example)

This repository provides a foundational, runnable "Hello World" agent example, demonstrating core Agent Development Kit (ADK) concepts ported from Python to Go. It adheres to the project structure `github.com/KennethanCeyer/adk-go`.

The primary goal of this example is to provide a working, minimal agent that interacts with a Large Language Model (LLM). More complex features and modules from the Python ADK can be incrementally built upon this base.

## Project Structure

```plaintext
adk-go
â”œâ”€â”€ common/
â”‚ â””â”€â”€ types/
â”‚ â””â”€â”€ types.go # Common, basic data types
â”œâ”€â”€ models/
â”‚ â””â”€â”€ types/
â”‚ â””â”€â”€ types.go # Types for LLM request/response (Content, Part, etc.)
â”œâ”€â”€ llmproviders/ # LLM interaction implementations
â”‚ â”œâ”€â”€ interfaces.go # LLMProvider interface
â”‚ â””â”€â”€ gemini.go # Google Gemini LLM provider
â”œâ”€â”€ tools/
â”‚ â”œâ”€â”€ types/
â”‚ â”‚ â””â”€â”€ types.go # Tool interface
â”‚ â””â”€â”€ example/ # Example tool implementations
â”‚ â””â”€â”€ echo_tool.go
â”œâ”€â”€ agents/
â”‚ â”œâ”€â”€ interfaces.go # Agent, LlmAgent interfaces
â”‚ â”œâ”€â”€ base_llm_agent.go # Base implementation for LlmAgent
â”‚ â””â”€â”€ invocation/
â”‚ â””â”€â”€ invocation.go # InvocationContext, RunConfig, ReadonlyContext
â”œâ”€â”€ examples/
â”‚ â””â”€â”€ helloworld/
â”‚ â””â”€â”€ agent.go # HelloWorld agent definition and initialization
â”œâ”€â”€ cmd/
â”‚ â””â”€â”€ helloworld_runner/
â”‚ â””â”€â”€ main.go # Executable for the HelloWorld agent
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ README.md
```

## Prerequisites

1.  **Go**: Version 1.20 or later. ([Installation Guide](https://go.dev/doc/install))
2.  **Google Cloud Project & API Key**:
    - A Google Cloud Project with the Generative Language API (for Gemini) enabled.
    - An API key for Google Gemini. Obtain this from [Google AI Studio](https://aistudio.google.com/app/apikey) or your Google Cloud console.
    - **Security**: Treat your API key as a secret. Do not commit it to your repository.
3.  **Environment Variable**: Set the `GEMINI_API_KEY` environment variable:
    ```bash
    export GEMINI_API_KEY="YOUR_API_KEY_HERE"
    ```

## Setup & Running the HelloWorld Agent

1.  **Create Project Directory:**

    ```bash
    mkdir -p adk-go
    cd adk-go
    ```

2.  **Initialize Go Module:**

    ```bash
    go mod init [github.com/KennethanCeyer/adk-go](https://github.com/KennethanCeyer/adk-go)
    ```

3.  **Install Dependencies:**
    This example uses Google's generative AI Go SDK for Gemini.

    ```bash
    go get [github.com/google/generative-ai-go/genai](https://github.com/google/generative-ai-go/genai)
    go get google.golang.org/api/option // Indirect dependency often needed
    ```

4.  **Create the Code Files:**
    Manually create the directory structure and files as listed in the "Project Structure" section above. Populate them with the Go code provided in the subsequent sections of this response. Ensure all file paths and package declarations are correct.

5.  **Set `GEMINI_API_KEY`**:
    Make sure the `GEMINI_API_KEY` environment variable is set in your terminal session before running.

6.  **Run the HelloWorld Agent:**
    From the root of your `adk-go` project directory:

    ```bash
    go run ./cmd/helloworld_runner/main.go
    ```

7.  **Interact with the Agent:**
    The runner will prompt you for input (`[user]: `). Try typing:
    - `Hello, world!`
    - `Use the echo tool to say: testing 123`
    - Type `exit` or `quit` to terminate.

# /Users/gopher/Desktop/workspace/adk-go/common/types/types.go
package types

type Timestamp float64

type InvocationID string

type SessionID string

type AgentID string

type ToolID string

type EventID string

type ErrorCode string

# /Users/gopher/Desktop/workspace/adk-go/sessions/types/types.go
package types

import (
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"
)

type State map[string]any

type Session struct {
	ID             commontypes.SessionID
	AppName        string
	UserID         string
	CreationTime   commontypes.Timestamp
	LastAccessTime commontypes.Timestamp
	State          State
	Events         []*eventstypes.Event `json:"-"`
	Metadata       map[string]string    `json:",omitempty"`
}

# /Users/gopher/Desktop/workspace/adk-go/examples/helloworld/agent.go
// File: github.com/KennethanCeyer/adk-go/examples/helloworld/agent.go
package helloworld

import (
	// Added for panic
	"log"

	"github.com/KennethanCeyer/adk-go/agents"
	llmproviders "github.com/KennethanCeyer/adk-go/llmproviders"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	exampletools "github.com/KennethanCeyer/adk-go/tools/example"
	toolstypes "github.com/KennethanCeyer/adk-go/tools/types"
)

var ConcreteLlmAgent agents.LlmAgent

func init() {
	geminiProvider, err := llmproviders.NewGeminiLLMProvider()
	if err != nil {
		log.Fatalf("Failed to create GeminiLLMProvider: %v. Ensure GEMINI_API_KEY is set.", err)
	}

	echoTool := exampletools.NewEchoTool()
	agentTools := []toolstypes.Tool{echoTool}

	systemInstructionText := "You are a helpful assistant named HelloWorldAgent. You can use tools provided to you. If asked to echo, use the echo_tool."
	systemInstruction := &modelstypes.Content{
		Parts: []modelstypes.Part{{Text: &systemInstructionText}},
	}

	// Ensure this model supports function calling effectively.
	// "gemini-1.5-flash-latest" is a good candidate. "gemini-pro" also works.
	modelName := "gemini-1.5-flash-latest"

	ConcreteLlmAgent = agents.NewBaseLlmAgent(
		"HelloWorldAgent",
		"A simple agent that can echo messages using a tool.",
		modelName,
		systemInstruction,
		geminiProvider,
		agentTools,
	)
	if ConcreteLlmAgent == nil { // Should not happen if NewBaseLlmAgent doesn't return nil on error
		panic("Failed to initialize HelloWorldAgent")
	}
	log.Println("HelloWorldAgent initialized successfully.")
}

# /Users/gopher/Desktop/workspace/adk-go/examples/echoagent/agent.go
package echoagent

import (
	"log"

	"github.com/KennethanCeyer/adk-go/agents"
	"github.com/KennethanCeyer/adk-go/llmproviders"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools"
	exampletools "github.com/KennethanCeyer/adk-go/tools/example"
)

// ConcreteLlmAgent is the exported agent instance.
var ConcreteLlmAgent agents.LlmAgent

func init() {
	geminiProvider, err := llmproviders.NewGeminiLLMProvider()
	if err != nil {
		log.Fatalf("EchoAgent init: Failed to create GeminiLLMProvider: %v. Ensure GEMINI_API_KEY is set.", err)
	}

	echoTool := exampletools.NewEchoTool()
	agentTools := []toolstypes.Tool{echoTool}

	systemInstructionText := "You are EchoBot, a friendly assistant. When a user explicitly asks you to use the 'echo_tool' to say something, you MUST use the 'echo_tool' with their message. Otherwise, just respond normally."
	systemInstruction := &modelstypes.Content{
		// Role for system instruction is often implicit or handled by the LLM provider.
		// If needed, set "system", but genai.GenerativeModel.SystemInstruction takes a *genai.Content directly.
		Parts: []modelstypes.Part{{Text: &systemInstructionText}},
	}

	// Recommended model for function calling with Gemini API.
	modelName := "gemini-1.5-flash-latest" // Or "gemini-pro"

	ConcreteLlmAgent = agents.NewBaseLlmAgent(
		"EchoAgent",
		"An agent that can use an echo_tool to repeat messages.",
		modelName,
		systemInstruction,
		geminiProvider,
		agentTools,
	)
	log.Println("EchoAgent initialized successfully.")
}

# /Users/gopher/Desktop/workspace/adk-go/planners/types/types.go
package types

// Forward declarations for types. Replace with actual imports when types are fully defined.
type AnyLlmRequest any
type AnyReadonlyContext any
type AnyCallbackContext any
type AnyPart any

type Planner interface {
	Name() string
	BuildPlanningInstruction(readonlyCtx AnyReadonlyContext, llmReq AnyLlmRequest) (instruction string, err error)
	ProcessPlanningResponse(callbackCtx AnyCallbackContext, responseParts []AnyPart) (processedParts []AnyPart, err error)
	ApplyThinkingConfig(llmReq AnyLlmRequest)
}

# /Users/gopher/Desktop/workspace/adk-go/events/types/types.go
package types

import (
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
)

type EventActions struct {
	StateDelta             map[string]any
	RequestedAuthConfigs []authtypes.AuthConfigName
	EscalateTo             commontypes.AgentID
	TransferToAgent        commontypes.AgentID
	SkipSummarization      bool
	ProducedArtifacts      []string
	ConsumedArtifacts      []string
	CustomMetadata         map[string]any
	TurnComplete           bool
}

type Event struct {
	ID                commontypes.EventID
	InvocationID      commontypes.InvocationID
	SessionID         commontypes.SessionID
	ParentEventID     commontypes.EventID `json:",omitempty"`
	Timestamp         commontypes.Timestamp
	Author            string
	Role              string `json:",omitempty"`
	Content           *modelstypes.Content
	Actions           *EventActions `json:",omitempty"`
	Branch            commontypes.BranchID `json:",omitempty"`
	IsPartial         bool `json:",omitempty"`
	ErrorCode         commontypes.ErrorCode `json:",omitempty"`
	ErrorMessage      string `json:",omitempty"`
	LlmResponse       *modelstypes.LlmResponse `json:",omitempty"`
	ObservedLatencyMs int64 `json:",omitempty"`
}

func (e *Event) GetFunctionCalls() []modelstypes.FunctionCall {
	if e == nil || e.Content == nil {
		return nil
	}
	var calls []modelstypes.FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

func (e *Event) GetFunctionResponses() []modelstypes.FunctionResponse {
	if e == nil || e.Content == nil {
		return nil
	}
	var responses []modelstypes.FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/base_llm_flow.go
package llmflows

import (
	"fmt"
	"log"
	"reflect"
	"runtime"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/sessions"
	// "github.com/KennethanCeyer/adk-go/utils"
)

// Placeholder types from assumed packages (if not already fully defined in processors)
type InvocationContext = processors.InvocationContext
type LlmRequest = models.LlmRequest
type LlmResponse = models.LlmResponse
type Event = events.Event
type EventActions = events.EventActions
type LlmAgent = agents.LlmAgent // This should be the primary agent interface
type Content = models.Content   // Assuming models package defines Content, Part etc.
type Part = models.Part
type FunctionCall = models.FunctionCall
type FunctionResponse = models.FunctionResponse
type State = sessions.State
type RunConfig = agents.RunConfig // Assuming RunConfig is in agents

// BaseLlmFlow is the base for LLM-based flows.
type BaseLlmFlow struct {
	RequestProcessors  []processors.BaseLlmRequestProcessor
	ResponseProcessors []processors.BaseLlmResponseProcessor
}

// NewBaseLlmFlow creates a new BaseLlmFlow.
func NewBaseLlmFlow(
	requestProcessors []processors.BaseLlmRequestProcessor,
	responseProcessors []processors.BaseLlmResponseProcessor) *BaseLlmFlow {
	return &BaseLlmFlow{
		RequestProcessors:  requestProcessors,
		ResponseProcessors: responseProcessors,
	}
}

// RunAsync executes the LLM flow.
func (f *BaseLlmFlow) RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error) {
	outputEventCh := make(chan *Event)

	go func() {
		defer close(outputEventCh)

		llmReq := &LlmRequest{} // Initialize with appropriate defaults if any
		if llmAgent.GetGenerateContentConfig() != nil { // Assuming LlmAgent has GetGenerateContentConfig
			llmReq.Config = llmAgent.GetGenerateContentConfig()
		} else {
			llmReq.Config = &models.GenerateContentConfig{} // Ensure config is not nil
		}


		// 1. Pre-LLM processing
		for _, processor := range f.RequestProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name() // Get processor name for logging
			log.Printf("Running request processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmReq)
			if err != nil {
				log.Printf("Error running request processor %s: %v", processorName, err)
				outputEventCh <- &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in request processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				}
				return
			}
			// Drain events from processor if any (though these processors typically modify llmReq)
			for event := range eventCh {
				log.Printf("Event from request processor %s: %v", processorName, event.ID)
				outputEventCh <- event
			}
		}

		// 2. Call LLM
		// In Python ADK, this is `llm_agent.predict_async(llm_req, invocation_context=invocation_ctx)`
		// which internally calls the model connection.
		// Here, we assume LlmAgent has a method to interact with the LLM.
		log.Printf("Sending request to LLM for agent: %s, model: %s", llmAgent.GetName(), llmReq.Model)
		// For debugging: log.Printf("LLM Request Contents: %+v", llmReq.Contents)
		// For debugging: log.Printf("LLM Request System Instruction: %+v", llmReq.Config.SystemInstruction)


		llmRespCh, err := llmAgent.PredictAsync(invocationCtx, llmReq) // Assuming LlmAgent has PredictAsync
		if err != nil {
			log.Printf("Error initiating LLM prediction for agent %s: %v", llmAgent.GetName(), err)
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error calling LLM: %v", err)}}},
				ErrorCode:    "llm_call_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// Wait for LLM response (event containing LlmResponse)
		modelResponseEvent, ok := <-llmRespCh
		if !ok {
			log.Printf("LLM response channel closed unexpectedly for agent %s", llmAgent.GetName())
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: "LLM response channel closed unexpectedly"}}},
				ErrorCode:    "llm_response_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// The event from PredictAsync should contain the LlmResponse.
		// We need a way to extract it. Let's assume Event has an LlmResponse field or a method.
		var llmResponse *LlmResponse
		if modelResponseEvent.LlmResponse != nil { // Assuming Event has a direct LlmResponse field
			llmResponse = modelResponseEvent.LlmResponse
		} else {
			// Fallback: if LlmResponse is embedded in Content.Parts as a special Part type,
			// or if the event itself *is* the LlmResponse structure (less likely for an Event).
			// This part needs clarification based on how PredictAsync structures its output event.
			log.Printf("Warning: LlmResponse not directly found in modelResponseEvent for agent %s. Event: %+v", llmAgent.GetName(), modelResponseEvent)
			// For now, let's assume if LlmResponse is nil, the event content itself is the raw model output.
            // This means the response processors might need to handle raw Content directly.
            // To make it work with current ResponseProcessor interface, we'll construct a dummy LlmResponse if Content exists.
            if modelResponseEvent.Content != nil && llmResponse == nil {
                llmResponse = &LlmResponse{Content: modelResponseEvent.Content}
                 // Also ensure the event itself is passed, as it contains actions, etc.
            } else if llmResponse == nil {
                 log.Printf("Error: No LlmResponse or Content in modelResponseEvent for agent %s.", llmAgent.GetName())
                 outputEventCh <- &Event{
                    ID:           utils.NewEventID(),
                    InvocationID: invocationCtx.InvocationID,
                    Author:       llmAgent.GetName(),
                    Branch:       invocationCtx.Branch,
                    Content:      &Content{Parts: []Part{{Text: "Invalid or empty LLM response event"}}},
                    ErrorCode:    "llm_response_invalid",
                    Timestamp:    utils.GetTimestamp(),
                }
                return
            }
		}


		// 3. Post-LLM processing
		// The Python ADK iterates response_processors and then sends the modelResponseEvent.
		// If processors yield further events, those are sent too.
		// The original modelResponseEvent is sent *after* all processors complete.

		var accumulatedProcessorEvents []*Event

		for _, processor := range f.ResponseProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name()
			log.Printf("Running response processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmResponse, modelResponseEvent) // Pass the original event for context (e.g., actions)
			if err != nil {
				log.Printf("Error running response processor %s: %v", processorName, err)
				// Decide if this error is fatal or if we should continue with other processors
				// For now, accumulate and proceed, then send error event.
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in response processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				})
				continue // Or return if fatal
			}
			for procEvent := range eventCh {
				log.Printf("Event from response processor %s: %v", processorName, procEvent.ID)
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, procEvent)
			}
		}

		// Send the (potentially modified by processors) modelResponseEvent first
		outputEventCh <- modelResponseEvent

		// Then send any events generated by the response processors
		for _, procEvent := range accumulatedProcessorEvents {
			outputEventCh <- procEvent
		}

	}()

	return outputEventCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/contents.go
package processors

import (
	"fmt"
	"log"
	"sort"
	"strings"
	"time"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions" // For REQUEST_EUC_FUNCTION_CALL_NAME & removeClientFunctionCallID
)

// InvocationContext defines the context for a single invocation of an agent.
type InvocationContext struct {
	InvocationID    string
	Agent           LlmAgent // Assuming LlmAgent or a more general Agent interface
	Session         *Session // Assuming this contains Events and State
	Branch          string
	UserContent     *Content
	ArtifactService interface{} // Placeholder for actual ArtifactService type
	// ... other fields like SessionService, MemoryService, RunConfig, etc.
}

// LlmAgent represents an LLM-based agent.
// This is a simplified interface based on usage in the Python code.
type LlmAgent interface {
	GetName() string
	GetIncludeContents() string // "default", "none"
	// ... other methods like GetPlanner, RootAgent, CanonicalInstruction etc.
}

// Session represents a user session.
type Session struct {
	ID     string
	Events []*Event
	State  map[string]interface{}
	AppName string
	UserID  string
	// ... other session fields
}

// Event represents an event in the system.
type Event struct {
	ID            string
	InvocationID  string
	Author        string
	Content       *Content
	Actions       *EventActions
	Timestamp     float64
	Branch        string
	// ... other event fields like TurnComplete, Partial, ErrorCode, ErrorMessage, etc.
}

// GetFunctionCalls extracts function calls from the event's content parts.
func (e *Event) GetFunctionCalls() []FunctionCall {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var calls []FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

// GetFunctionResponses extracts function responses from the event's content parts.
func (e *Event) GetFunctionResponses() []FunctionResponse {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var responses []FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}


// Content represents the content of a message.
type Content struct {
	Role  string
	Parts []Part
}

// Part represents a part of a message's content.
type Part struct {
	Text             string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
	// ... other part types like InlineData, FileData, etc.
}

// FunctionCall represents a function call requested by the LLM.
type FunctionCall struct {
	ID   string
	Name string
	Args map[string]interface{}
}

// FunctionResponse represents the result of a function call.
type FunctionResponse struct {
	ID       string
	Name     string
	Response map[string]interface{}
}

// EventActions represents actions associated with an event.
type EventActions struct {
	StateDelta             map[string]interface{}
	RequestedAuthConfigs map[string]interface{} // Placeholder for actual AuthConfig type
	// ... other actions like SkipSummarization, TransferToAgent, Escalate
}


// LlmRequest represents a request to the LLM.
type LlmRequest struct {
	Model    string
	Contents []Content
	Config   *GenerateContentConfig // Assuming GenerateContentConfig is defined elsewhere
	// ... other fields like LiveConnectConfig, ToolsDict
}

// GenerateContentConfig holds configuration for LLM content generation.
type GenerateContentConfig struct {
	// Define fields based on google.genai.types.GenerateContentConfig
	// For example:
	// Temperature *float32
	// TopP *float32
	// TopK *int32
	// CandidateCount *int32
	// MaxOutputTokens *int32
	// StopSequences []string
	// SafetySettings []*SafetySetting
	// Tools []*Tool
	// SystemInstruction *Content
	// ResponseSchema interface{}
	// ResponseMimeType string
	// ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig
}


// Placeholder, assuming it's defined in the functions package or similar
const REQUEST_EUC_FUNCTION_CALL_NAME = "adk_request_credential"

// ContentLlmRequestProcessor builds the contents for the LLM request.
type ContentLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *ContentLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		if llmAgent.GetIncludeContents() != "none" { // Assuming LlmAgent has GetIncludeContents()
			if invocationCtx.Session == nil {
				log.Println("Error: InvocationContext.Session is nil")
				return
			}
			llmReq.Contents = getContents(
				invocationCtx.Branch,
				invocationCtx.Session.Events, // Assuming Session has Events []*Event
				llmAgent.GetName(),
			)
		}
		// This processor does not yield events.
	}()
	return outCh, nil
}

func rearrangeEventsForAsyncFunctionResponsesInHistory(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	functionCallIDToResponseEventsIndex := make(map[string]int)
	for i, event := range events {
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			for _, fr := range responses {
				functionCallIDToResponseEventsIndex[fr.ID] = i
			}
		}
	}

	var resultEvents []*Event
	processedResponseIndices := make(map[int]bool) // To avoid adding same response event multiple times

	for i, event := range events {
		if _, ok := processedResponseIndices[i]; ok && len(event.GetFunctionResponses()) > 0 { //
			// This response event was already merged with its call, skip it.
			continue //
		}

		if calls := event.GetFunctionCalls(); len(calls) > 0 { //
			resultEvents = append(resultEvents, event) // Add the function call event itself

			var responseEventsToMerge []*Event
			var indicesToMarkProcessed []int

			for _, fc := range calls {
				if responseIdx, ok := functionCallIDToResponseEventsIndex[fc.ID]; ok { //
					isAlreadyProcessedByAnotherCall := false
					for _, resEvent := range resultEvents {
						if resEvent == events[responseIdx] {
							isAlreadyProcessedByAnotherCall = true
							break
						}
						// Also check if the response event's content (parts) were merged into another event.
						// For simplicity, we rely on direct event object comparison or index tracking.
					}
					if !isAlreadyProcessedByAnotherCall && !processedResponseIndices[responseIdx] { //
						responseEventsToMerge = append(responseEventsToMerge, events[responseIdx])
						indicesToMarkProcessed = append(indicesToMarkProcessed, responseIdx)
					}
				}
			}

			if len(responseEventsToMerge) > 0 {
				sort.SliceStable(responseEventsToMerge, func(i, j int) bool { //
					var idxI, idxJ int = -1, -1
					for k, e := range events {
						if e == responseEventsToMerge[i] {
							idxI = k
						}
						if e == responseEventsToMerge[j] {
							idxJ = k
						}
						if idxI != -1 && idxJ != -1 {
							break
						}
					}
					return idxI < idxJ
				})
				mergedRespEvent := mergeFunctionResponseEvents(responseEventsToMerge)
				if mergedRespEvent != nil {
					resultEvents = append(resultEvents, mergedRespEvent)
					for _, idx := range indicesToMarkProcessed { //
						processedResponseIndices[idx] = true
					}
				}
			}
		} else {
			if !(len(event.GetFunctionResponses()) > 0 && processedResponseIndices[i]) { //
				resultEvents = append(resultEvents, event)
			}
		}
	}
	return resultEvents
}

func rearrangeEventsForLatestFunctionResponse(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	lastEvent := events[len(events)-1]
	responsesInLastEvent := lastEvent.GetFunctionResponses()
	if len(responsesInLastEvent) == 0 { //
		return events
	}

	responseIDs := make(map[string]bool)
	for _, fr := range responsesInLastEvent {
		responseIDs[fr.ID] = true
	}

	if len(events) >= 2 { //
		eventBeforeLast := events[len(events)-2]
		callsInEventBeforeLast := eventBeforeLast.GetFunctionCalls()
		allCallsMatched := true
		if len(callsInEventBeforeLast) == len(responseIDs) && len(callsInEventBeforeLast) > 0 { //
			for _, fc := range callsInEventBeforeLast {
				if !responseIDs[fc.ID] {
					allCallsMatched = false
					break
				}
			}
			if allCallsMatched {
				return events
			}
		}
	}

	functionCallEventIdx := -1 //
	for i := len(events) - 2; i >= 0; i-- { //
		event := events[i]
		calls := event.GetFunctionCalls()
		if len(calls) > 0 {
			foundMatch := false
			for _, fc := range calls {
				if responseIDs[fc.ID] {
					foundMatch = true
					for _, otherFc := range calls {
						responseIDs[otherFc.ID] = true
					}
					break
				}
			}
			if foundMatch {
				functionCallEventIdx = i
				break
			}
		}
	}

	if functionCallEventIdx == -1 { //
		log.Printf("Warning: No matching function call event found for function response IDs: %v", getKeys(responseIDs))
		return events //
	}

	var functionResponseEventsToMerge []*Event
	for i := functionCallEventIdx + 1; i < len(events)-1; i++ { //
		event := events[i]
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			isRelevantResponse := false
			for _, fr := range responses {
				if responseIDs[fr.ID] {
					isRelevantResponse = true
					break
				}
			}
			if isRelevantResponse {
				functionResponseEventsToMerge = append(functionResponseEventsToMerge, event)
			}
		}
	}
	functionResponseEventsToMerge = append(functionResponseEventsToMerge, lastEvent)

	resultEvents := make([]*Event, 0, functionCallEventIdx+2) //
	resultEvents = append(resultEvents, events[:functionCallEventIdx+1]...)
	if merged := mergeFunctionResponseEvents(functionResponseEventsToMerge); merged != nil { //
		resultEvents = append(resultEvents, merged)
	}

	return resultEvents
}

func getContents(currentBranch string, historyEvents []*Event, agentName string) []Content {
	var filteredEvents []*Event
	for _, event := range historyEvents {
		if event.Content == nil || len(event.Content.Parts) == 0 { //
			continue
		}
		// Go: we assume an empty text part is still a valid part unless explicitly filtered.
		// For now, let's assume empty text part implies no meaningful content for LLM.
		hasMeaningfulText := false //
		for _, p := range event.Content.Parts {
			if p.Text != "" {
				hasMeaningfulText = true
				break
			}
		}
		if !hasMeaningfulText && len(event.GetFunctionCalls()) == 0 && len(event.GetFunctionResponses()) == 0 { //
			continue
		}

		if !isEventOnBranch(currentBranch, event) {
			continue
		}
		if isAuthEvent(event) {
			continue
		}

		if isOtherAgentReply(agentName, event) {
			filteredEvents = append(filteredEvents, convertForeignEventToGo(event))
		} else {
			filteredEvents = append(filteredEvents, event)
		}
	}

	// Python calls _rearrange_events_for_latest_function_response first, then _rearrange_events_for_async_function_responses_in_history.
	resultEventsStage1 := rearrangeEventsForLatestFunctionResponse(filteredEvents)
	resultEventsStage2 := rearrangeEventsForAsyncFunctionResponsesInHistory(resultEventsStage1)

	var contents []Content
	for _, event := range resultEventsStage2 {
		if event.Content != nil {
			contentCopy := deepCopyContent(*event.Content)
			removeClientFunctionCallID(&contentCopy) // Placeholder
			contents = append(contents, contentCopy)
		}
	}
	return contents
}

func isEventOnBranch(currentBranch string, event *Event) bool {
	// Placeholder for logic to check if event is on the current branch
	// This depends on how Branch is structured and compared.
	if event == nil {
		return false
	}
	return event.Branch == "" || currentBranch == "" || strings.HasPrefix(event.Branch, currentBranch) || strings.HasPrefix(currentBranch, event.Branch)
}

func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return currentAgentName != "" && event.Author != currentAgentName && event.Author != "user"
}

func convertForeignEventToGo(event *Event) *Event {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return event
	}

	newContent := Content{Role: "user"} //
	newContent.Parts = append(newContent.Parts, Part{Text: "For context:"})

	for _, part := range event.Content.Parts {
		if part.Text != "" {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] said: %s", event.Author, part.Text)})
		} else if part.FunctionCall != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] called tool `%s` with parameters: %v", event.Author, part.FunctionCall.Name, part.FunctionCall.Args)})
		} else if part.FunctionResponse != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] `%s` tool returned result: %v", event.Author, part.FunctionResponse.Name, part.FunctionResponse.Response)})
		} else {
			// This part needs careful consideration for how to represent non-text/non-FC/non-FR parts.
			// For simplicity, appending as-is if possible, or a placeholder.
			// newContent.Parts = append(newContent.Parts, part) // This assumes Part is copyable
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] provided non-text data.", event.Author)}) //
		}
	}

	newEvent := *event // Shallow copy for metadata
	newEvent.Author = "user" //
	newEvent.Content = &newContent
	return &newEvent
}

func mergeFunctionResponseEvents(functionResponseEvents []*Event) *Event {
	if len(functionResponseEvents) == 0 {
		return nil
	}
	if len(functionResponseEvents) == 1 {
		return functionResponseEvents[0]
	}

	baseEvent := functionResponseEvents[0]
	// mergedParts := make([]Part, 0) // Not used directly in Go like Python

	// Map to store the latest response part for each function call ID
	latestResponsesForID := make(map[string]Part)
	var nonResponseParts []Part // To collect text parts or other non-FR parts in order

	for _, event := range functionResponseEvents {
		if event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil {
				latestResponsesForID[part.FunctionResponse.ID] = part
			} else {
				nonResponseParts = append(nonResponseParts, part)
			}
		}
	}

	// The Python code implicitly relies on the order of parts in the *first* event
	// if it contained multiple function responses, and then updates them.
	// For Go, if the first event had multiple FCs, its FRs would be the base.
	// If subsequent events provide FRs for *different* FC IDs not in the first event, they are appended.
	// This simplified version will collect all unique FRs based on their latest appearance.
	tempParts := make([]Part, 0, len(latestResponsesForID)+len(nonResponseParts)) //
	tempParts = append(tempParts, nonResponseParts...)                            //
	// To maintain order similar to Python (where original order of FCs in the *first* event matters,
	// and then other unique FRs are appended/merged), we need a more sophisticated merge.
	// For now, this collects unique FRs; their order relative to nonResponseParts and each other might differ from Python's.
	// A better approach would be to iterate through the function calls of the *triggering* event (if available)
	// and append responses in that order, or sort collected FRs by some original call order.

	// Example: Iterate through first event's FRs if they exist as a base order
	if baseEvent.Content != nil {
		for _, part := range baseEvent.Content.Parts {
			if part.FunctionResponse != nil {
				if respPart, ok := latestResponsesForID[part.FunctionResponse.ID]; ok {
					tempParts = append(tempParts, respPart)
					delete(latestResponsesForID, part.FunctionResponse.ID) // Mark as added
				}
			}
		}
	}
	// Append any remaining unique FRs (those not in the first event's call order)
	for _, part := range latestResponsesForID { // Order from map is not guaranteed.
		tempParts = append(tempParts, part)
	}

	mergedEvent := &Event{ //
		ID:           newEventID(), // Assuming newEventID() is defined
		InvocationID: baseEvent.InvocationID,
		Author:       baseEvent.Author,
		Branch:       baseEvent.Branch,
		Content:      &Content{Role: "user", Parts: tempParts}, // Gemini expects FRs to be role "user"
		Actions:      &EventActions{},                          // Initialize to avoid nil pointer if baseEvent.Actions is nil
		Timestamp:    baseEvent.Timestamp,                      // Or use latest timestamp
	}

	// Merge actions from all events (simplified)
	for _, event := range functionResponseEvents {
		if event.Actions != nil && event.Actions.StateDelta != nil { //
			if mergedEvent.Actions.StateDelta == nil {
				mergedEvent.Actions.StateDelta = make(map[string]interface{})
			}
			for k, v := range event.Actions.StateDelta {
				mergedEvent.Actions.StateDelta[k] = v
			}
		}
		// ... merge other actions like RequestedAuthConfigs, ArtifactDelta etc.
	}

	return mergedEvent
}

// newEventID generates a new unique ID for an event.
// This is a placeholder; a robust UUID generation should be used.
func newEventID() string {
	// In a real implementation, use something like github.com/google/uuid
	return fmt.Sprintf("evt_%d", time.Now().UnixNano())
}


func isAuthEvent(event *Event) bool {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return false
	}
	for _, part := range event.Content.Parts {
		if part.FunctionCall != nil && part.FunctionCall.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
		if part.FunctionResponse != nil && part.FunctionResponse.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
	}
	return false
}

// removeClientFunctionCallID placeholder
func removeClientFunctionCallID(content *Content) {
	// In Python, this removes IDs starting with AF_FUNCTION_CALL_ID_PREFIX
	// For Go, this would iterate through content.Parts and nil out IDs if they match a pattern.
	// For example:
	// const afFunctionCallIDPrefix = "adk-"
	// if content == nil || content.Parts == nil {
	// 	return
	// }
	// for i := range content.Parts {
	// 	if content.Parts[i].FunctionCall != nil && strings.HasPrefix(content.Parts[i].FunctionCall.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionCall.ID = "" // Or based on how Gemini handles empty vs nil
	// 	}
	// 	if content.Parts[i].FunctionResponse != nil && strings.HasPrefix(content.Parts[i].FunctionResponse.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionResponse.ID = ""
	// 	}
	// }
}

// deepCopyContent performs a deep copy of the Content struct.
func deepCopyContent(original Content) Content { //
	// A real deep copy is needed here. This is a placeholder.
	// For Go structs, manual copying or a library might be needed for true deep copies.
	copied := original //
	if original.Parts != nil {
		copied.Parts = make([]Part, len(original.Parts))
		for i, p := range original.Parts {
			// Deep copy each part. This is simplified.
			// If Part contains pointers or slices, those need deep copying too.
			copiedPart := p
			if p.FunctionCall != nil {
				fcCopy := *p.FunctionCall
				if p.FunctionCall.Args != nil {
					fcCopy.Args = deepCopyMap(p.FunctionCall.Args)
				}
				copiedPart.FunctionCall = &fcCopy
			}
			if p.FunctionResponse != nil {
				frCopy := *p.FunctionResponse
				if p.FunctionResponse.Response != nil {
					frCopy.Response = deepCopyMap(p.FunctionResponse.Response)
				}
				copiedPart.FunctionResponse = &frCopy
			}
			// Add deep copy for other Part fields if necessary
			copied.Parts[i] = copiedPart
		}
	}
	return copied
}

// deepCopyMap creates a deep copy of a map[string]interface{}.
// This is a simplified version; a robust deep copy library might be better for complex nested structures.
func deepCopyMap(originalMap map[string]interface{}) map[string]interface{} {
	if originalMap == nil {
		return nil
	}
	copyMap := make(map[string]interface{})
	for key, value := range originalMap {
		switch v := value.(type) {
		case map[string]interface{}:
			copyMap[key] = deepCopyMap(v)
		case []interface{}:
			copyMap[key] = deepCopySlice(v)
		default:
			copyMap[key] = v // Assumes primitive types are copyable by value
		}
	}
	return copyMap
}

// deepCopySlice creates a deep copy of a slice of interface{}.
func deepCopySlice(originalSlice []interface{}) []interface{} {
	if originalSlice == nil {
		return nil
	}
	copySlice := make([]interface{}, len(originalSlice))
	for i, value := range originalSlice {
		switch v := value.(type) {
		case map[string]interface{}:
			copySlice[i] = deepCopyMap(v)
		case []interface{}:
			copySlice[i] = deepCopySlice(v)
		default:
			copySlice[i] = v
		}
	}
	return copySlice
}


func getKeys(m map[string]bool) []string { //
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}

// Placeholder for LlmAgent.GetIncludeContents() method.
// This should be part of the LlmAgent interface/struct definition.
// func (a *LlmAgentStruct) GetIncludeContents() string { return a.IncludeContents }

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/instructions.go
package processors

import (
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/utils" // For instructions_utils
)

// ReadonlyContext provides a read-only view of the invocation context.
type ReadonlyContext struct { //
	InvocationContext *InvocationContext
	// Potentially other fields if ReadonlyContext offers more than just InvocationContext access
}


// InstructionsLlmRequestProcessor handles instructions and global instructions for LLM flow.
type InstructionsLlmRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *InstructionsLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in InstructionsLlmRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx} //

		// Append global instructions if set.
		// In Python ADK, global_instruction is on the root agent.
		// Assuming LlmAgent interface has a RootAgent() method that returns the root agent (LlmAgent type).
		var rootLlmAgent LlmAgent
		if ra := llmAgent.RootAgent(); ra != nil {
			var rOk bool
			rootLlmAgent, rOk = ra.(LlmAgent)
			if !rOk {
				log.Println("Warning: Root agent is not an LlmAgent, cannot process global instructions.")
			}
		}


		if rootLlmAgent != nil { //
			// Assuming LlmAgent interface has CanonicalGlobalInstruction
			globalInstructionStr, bypassGlobal, err := rootLlmAgent.CanonicalGlobalInstruction(readonlyCtx) //
			if err != nil {
				log.Printf("Error getting canonical global instruction: %v", err) //
			}
			if globalInstructionStr != "" {
				var processedGlobalInstr string
				if !bypassGlobal {
					processedGlobalInstr, err = injectSessionState(globalInstructionStr, readonlyCtx) //
					if err != nil {
						log.Printf("Error injecting state into global instruction: %v", err) //
						processedGlobalInstr = globalInstructionStr                               // Use raw on error
					}
				} else {
					processedGlobalInstr = globalInstructionStr
				}
				// Assuming LlmRequest has AppendInstructions
				if llmReq.Config == nil {
					llmReq.Config = &GenerateContentConfig{}
				}
				if llmReq.Config.SystemInstruction == nil {
					llmReq.Config.SystemInstruction = &Content{}
				}
				// This needs a proper AppendInstructions method on LlmRequest or its Config
				currentSysInstruction := ""
				if len(llmReq.Config.SystemInstruction.Parts) > 0 {
					currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
				}
				if currentSysInstruction != "" {
					currentSysInstruction += "\n\n"
				}
				llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedGlobalInstr}}

			}
		}

		// Append agent instructions if set.
		// Assuming LlmAgent interface has CanonicalInstruction
		agentInstructionStr, bypassAgent, err := llmAgent.CanonicalInstruction(readonlyCtx) //
		if err != nil {
			log.Printf("Error getting canonical agent instruction: %v", err) //
		}
		if agentInstructionStr != "" {
			var processedAgentInstr string
			if !bypassAgent {
				processedAgentInstr, err = injectSessionState(agentInstructionStr, readonlyCtx) //
				if err != nil {
					log.Printf("Error injecting state into agent instruction: %v", err) //
					processedAgentInstr = agentInstructionStr                             // Use raw on error
				}
			} else {
				processedAgentInstr = agentInstructionStr
			}
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedAgentInstr}}
		}
		// No events are yielded by this specific processor.
	}()
	return outCh, nil
}

// injectSessionState populates values in the instruction template.
// This is a Go equivalent of instructions_utils.inject_session_state.
func injectSessionState(template string, readonlyCtx *ReadonlyContext) (string, error) {
	if readonlyCtx == nil || readonlyCtx.InvocationContext == nil || readonlyCtx.InvocationContext.Session == nil {
		return template, fmt.Errorf("readonlyCtx or its internal fields are nil")
	}
	invocationCtx := readonlyCtx.InvocationContext //

	// Go's regexp package doesn't support direct async replacement functions.
	// This will be synchronous for now.
	// Pattern to find {var_name} or {var_name?} or {artifact.file_name}
	// For simplicity, a basic version is used here. A more robust parser might be needed.
	re := regexp.MustCompile(`{([^}]+)}`) //

	var replacementErr error

	result := re.ReplaceAllStringFunc(template, func(match string) string { //
		if replacementErr != nil {
			return match
		}

		varNameWithMarker := strings.Trim(match, "{} ")
		isOptional := strings.HasSuffix(varNameWithMarker, "?")
		varName := strings.TrimSuffix(varNameWithMarker, "?")

		if strings.HasPrefix(varName, "artifact.") { //
			artifactName := strings.TrimPrefix(varName, "artifact.")
			if invocationCtx.ArtifactService == nil { //
				replacementErr = fmt.Errorf("artifact service is not initialized")
				return match
			}
			// Conceptual: ArtifactService would need a synchronous LoadArtifact or this needs to be async.
			// For this snippet, assuming a conceptual synchronous load or pre-loaded artifact map.
			// This part is commented out as ArtifactService and its methods are not fully defined here.
			/*
				artifactPart, err := invocationCtx.ArtifactService.LoadArtifactSync( //
					invocationCtx.Session.AppName, // Assuming AppName field
					invocationCtx.Session.UserID,  // Assuming UserID field
					invocationCtx.Session.ID,      // Assuming ID field
					artifactName,
				)
				if err != nil {
					if isOptional { return "" }
					replacementErr = fmt.Errorf("artifact '%s' not found: %w", artifactName, err)
					return match
				}
				if artifactPart != nil {
					return artifactPart.Text // Assuming text artifact for simplicity
				}
			*/
			return fmt.Sprintf("[ARTIFACT:%s]", artifactName) // Placeholder
		}

		if !isValidStateName(varName) { //
			return match
		}

		if val, ok := invocationCtx.Session.State[varName]; ok { //
			return fmt.Sprintf("%v", val)
		}
		if isOptional { //
			return ""
		}
		replacementErr = fmt.Errorf("context variable not found: `%s`", varName) //
		return match
	})

	if replacementErr != nil {
		return "", replacementErr
	}
	return result, nil
}

// isValidStateName (simplified) checks if a variable name could be a state key.
func isValidStateName(varName string) bool {
	parts := strings.Split(varName, ":")
	if len(parts) == 1 {
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(varName) //
	}
	if len(parts) == 2 {
		// In Python ADK, prefixes are "app:", "user:", "temp:"
		// Simplified check for now.
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[0]) &&
			regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[1]) //
	}
	return false
}

// LlmAgent interface needs to be fully defined, including these methods for the above to compile:
// RootAgent() BaseAgent // Assuming BaseAgent is a common interface for all agents
// CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error)
// CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error)

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/base_llm_processors.go
package agents

import (
	"context"
	"fmt"
	"log"

	llmprovideriface "github.com/KennethanCeyer/adk-go/llmproviders"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools"
)

// BaseLlmAgent provides a standard implementation for LlmAgent.
type BaseLlmAgent struct {
	AgentName         string
	AgentDescription  string
	ModelName         string
	SystemInstruction *modelstypes.Content // Use Content type for system instruction
	AgentTools        []toolstypes.Tool
	Provider          llmprovideriface.LLMProvider
}

// NewBaseLlmAgent creates a new BaseLlmAgent.
func NewBaseLlmAgent(
	name, description, modelName string,
	systemInstruction *modelstypes.Content, // Changed to *modelstypes.Content
	provider llmprovideriface.LLMProvider,
	agentTools []toolstypes.Tool,
) *BaseLlmAgent {
	if agentTools == nil {
		agentTools = []toolstypes.Tool{}
	}
	return &BaseLlmAgent{
		AgentName:         name,
		AgentDescription:  description,
		ModelName:         modelName,
		SystemInstruction: systemInstruction, // Store as Content
		AgentTools:        agentTools,
		Provider:          provider,
	}
}

// GetName returns the agent's name.
func (a *BaseLlmAgent) GetName() string { return a.AgentName }
// GetDescription returns the agent's description.
func (a *BaseLlmAgent) GetDescription() string { return a.AgentDescription }
// GetModelIdentifier returns the LLM model name.
func (a *BaseLlmAgent) GetModelIdentifier() string { return a.ModelName }
// GetSystemInstruction returns the agent's system instruction.
func (a *BaseLlmAgent) GetSystemInstruction() *modelstypes.Content { return a.SystemInstruction }
// GetTools returns the tools available to the agent.
func (a *BaseLlmAgent) GetTools() []toolstypes.Tool { return a.AgentTools }
// GetLLMProvider returns the LLM provider for this agent.
func (a *BaseLlmAgent) GetLLMProvider() llmprovideriface.LLMProvider { return a.Provider }

// Process handles a turn of conversation.
func (a *BaseLlmAgent) Process(
	ctx context.Context,
	history []modelstypes.Content,
	latestContent modelstypes.Content,
) (*modelstypes.Content, error) {
	if a.Provider == nil {
		return nil, fmt.Errorf("BaseLlmAgent Process: agent %s has no LLMProvider configured", a.AgentName)
	}

	currentHistory := history
	currentInputContent := latestContent
	maxToolCalls := 5 // Limit recursive tool calls

	for i := 0; i < maxToolCalls; i++ {
		llmResponseContent, err := a.Provider.GenerateContent(
			ctx,
			a.ModelName,
			a.SystemInstruction,
			a.AgentTools,
			currentHistory,
			currentInputContent,
		)
		if err != nil {
			return nil, fmt.Errorf("BaseLlmAgent Process: LLMProvider.GenerateContent failed for agent %s: %w", a.AgentName, err)
		}

		if llmResponseContent == nil {
			log.Printf("BaseLlmAgent Process: Agent %s received nil content from LLM.", a.AgentName)
			return &modelstypes.Content{Role: "model", Parts: []modelstypes.Part{{Text: new(string)}}}, nil // Empty text part
		}

		// Append the input that led to this LLM response, and the LLM response itself to history for the next turn.
		currentHistory = append(currentHistory, currentInputContent)
		currentHistory = append(currentHistory, *llmResponseContent)

		var pendingFunctionCall *modelstypes.FunctionCall
		for _, part := range llmResponseContent.Parts {
			if part.FunctionCall != nil {
				pendingFunctionCall = part.FunctionCall
				break
			}
		}

		if pendingFunctionCall == nil {
			return llmResponseContent, nil // No function call, this is the final response for this turn.
		}

		log.Printf("BaseLlmAgent Process: Agent %s attempting to call tool '%s'", a.AgentName, pendingFunctionCall.Name)
		var selectedTool toolstypes.Tool
		for _, t := range a.AgentTools {
			if t.Name() == pendingFunctionCall.Name {
				selectedTool = t
				break
			}
		}

		var toolExecutionResult modelstypes.Part
		if selectedTool == nil {
			errMsg := fmt.Sprintf("tool '%s' not found for agent %s", pendingFunctionCall.Name, a.AgentName)
			log.Println(errMsg)
			toolExecutionResult.FunctionResponse = &modelstypes.FunctionResponse{
				Name:     pendingFunctionCall.Name,
				Response: map[string]any{"error": errMsg},
			}
		} else {
			toolResultData, toolErr := selectedTool.Execute(ctx, pendingFunctionCall.Args)
			fr := &modelstypes.FunctionResponse{Name: selectedTool.Name()}
			if toolErr != nil {
				log.Printf("BaseLlmAgent Process: Error executing tool %s for agent %s: %v", selectedTool.Name(), a.AgentName, toolErr)
				fr.Response = map[string]any{"error": toolErr.Error()}
			} else {
				fr.Response = toolResultData
			}
			toolExecutionResult.FunctionResponse = fr
		}
		
		// Next input for the LLM is the function response.
		// Role for function response content should be "function" or "tool".
		currentInputContent = modelstypes.Content{
			Role:  "function", // Use "function" as per typical GenAI API function calling flows
			Parts: []modelstypes.Part{toolExecutionResult},
		}
		// History for the next LLM call already contains the previous user/model turns, including the FC.
	}

	return nil, fmt.Errorf("BaseLlmAgent Process: agent %s exceeded maximum tool call iterations (%d)", a.AgentName, maxToolCalls)
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/basic.go
package processors

import (
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, etc.
)

// BaseLlm defines the interface for a Large Language Model.
type BaseLlm interface {
	ModelName() string
	// Potentially other methods like GenerateContent, Connect etc.
}

// MockLLMForFlow is a placeholder for testing, representing a specific LLM implementation.
type MockLLMForFlow struct {
	ModelName string
}

func (m *MockLLMForFlow) ModelName() string {
	return m.ModelName
}
// Ensure MockLLMForFlow implements BaseLlm
var _ BaseLlm = &MockLLMForFlow{}


// RunConfig defines configuration for an agent run.
type RunConfig struct {
	// Define fields based on google.adk.agents.run_config.RunConfig
	// Example fields (conceptual):
	// ResponseModalities []string
	// SpeechConfig interface{}
	// OutputAudioTranscription bool
	// InputAudioTranscription bool
	// MaxLlmCalls int
	// StreamingMode string
}


// LlmAgent interface part related to BasicRequestProcessor
type LlmAgentForBasicProcessor interface {
	LlmAgent // Embed the general LlmAgent interface
	CanonicalModel() (BaseLlm, error)
	GetGenerateContentConfig() *GenerateContentConfig
	GetOutputSchema() interface{}
}


// BasicRequestProcessor handles basic information to build the LLM request.
type BasicRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *BasicRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) { //
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in BasicRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgentForBasicProcessor) // Use the more specific interface
		if !ok {
			// If agent is not an LlmAgent, this processor might not apply,
			// or it might indicate an issue with the agent setup.
			// For now, we simply return without error.
			log.Println("Error: Agent in InvocationContext is not of type LlmAgentForBasicProcessor or compatible interface")
			return
		}

		canonicalModel, err := llmAgent.CanonicalModel() //
		if err != nil {
			log.Printf("Error getting canonical model for agent %s: %v", llmAgent.GetName(), err) //
			return
		}

		if canonicalModel != nil { //
			llmReq.Model = canonicalModel.ModelName() //
		} else {
			log.Printf("Warning: CanonicalModel is nil for agent %s, model name might not be set correctly in LlmRequest.", llmAgent.GetName()) //
		}


		if cfg := llmAgent.GetGenerateContentConfig(); cfg != nil { //
			cfgCopy := *cfg // Shallow copy
			llmReq.Config = &cfgCopy //
		} else {
			llmReq.Config = &GenerateContentConfig{} // Ensure config is not nil
		}

		if outputSchema := llmAgent.GetOutputSchema(); outputSchema != nil { //
			// Assuming LlmRequest has a SetOutputSchema method
			// llmReq.SetOutputSchema(outputSchema) // This depends on LlmRequest's definition
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			llmReq.Config.ResponseSchema = outputSchema
			llmReq.Config.ResponseMimeType = "application/json" // Typically for structured output

		}

		if invocationCtx.RunConfig != nil { //
			// Conceptual assignments - actual fields depend on RunConfig and LlmRequest.LiveConnectConfig definitions
			// if llmReq.LiveConnectConfig == nil { llmReq.LiveConnectConfig = &LiveConnectConfig{} }
			// llmReq.LiveConnectConfig.ResponseModalities = invocationCtx.RunConfig.ResponseModalities
			// llmReq.LiveConnectConfig.SpeechConfig = invocationCtx.RunConfig.SpeechConfig
			// llmReq.LiveConnectConfig.OutputAudioTranscription = invocationCtx.RunConfig.OutputAudioTranscription
			// llmReq.LiveConnectConfig.InputAudioTranscription = invocationCtx.RunConfig.InputAudioTranscription
			// These would be direct assignments if LiveConnectConfig fields match RunConfig fields.
		}

		// No events are yielded by this specific processor, it only modifies llmReq.
	}()
	return outCh, nil
}

// These GetGenerateContentConfig and GetOutputSchema methods should be part of the LlmAgent implementation.
// Example implementation for a hypothetical LlmAgentStruct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentGenerateContentConfig *GenerateContentConfig
    OutputSchema interface{}
}

func (a *LlmAgentStruct) GetGenerateContentConfig() *GenerateContentConfig { //
    return a.AgentGenerateContentConfig
}

func (a *LlmAgentStruct) GetOutputSchema() interface{} { //
    return a.OutputSchema
}
*/

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/identity.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
)

// IdentityLlmRequestProcessor gives the agent identity from the framework.
type IdentityLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *IdentityLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in IdentityLlmRequestProcessor")
			return
		}
		agent := invocationCtx.Agent // Assuming Agent interface has GetName() and GetDescription()

		var instructions []string
		instructions = append(instructions, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName())) //
		if desc := agent.GetDescription(); desc != "" { //
			instructions = append(instructions, fmt.Sprintf(" The description about you is \"%s\"", desc)) //
		}

		// Assuming LlmRequest has AppendInstructions method or similar mechanism
		if llmReq.Config == nil {
			llmReq.Config = &GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &Content{}
		}
		currentSysInstruction := ""
		if len(llmReq.Config.SystemInstruction.Parts) > 0 {
			currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
		}

		addedInstruction := strings.Join(instructions, "")
		if currentSysInstruction != "" {
			currentSysInstruction += "\n\n" // Ensure separation if there's existing instruction
		}
		llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + addedInstruction}}
		// This processor does not yield events, it modifies llmReq.
	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/nlplanning.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/planners" // For PlanReActPlanner, BuiltInPlanner
	// "github.com/google/generative-ai-go/genai" // For genai.Part, genai.ThinkingConfig
)

// Planner defines the interface for agent planners.
type Planner interface { // [cite: 2133]
	BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) // [cite: 2133]
	ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) // [cite: 2133]
	// ApplyThinkingConfig is specific to BuiltInPlanner in Python, might be part of a more specific interface in Go
	ApplyThinkingConfig(llmReq *LlmRequest)
}

// BuiltInPlanner is a placeholder for planners.BuiltInPlanner [cite: 2133]
type BuiltInPlanner struct {
	ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig [cite: 2133]
}

// ApplyThinkingConfig applies the thinking configuration to the LLM request.
func (bip *BuiltInPlanner) ApplyThinkingConfig(llmReq *LlmRequest) { // [cite: 2133]
	if bip.ThinkingConfig != nil && llmReq.Config != nil {
		// llmReq.Config.ThinkingConfig = bip.ThinkingConfig // This depends on GenerateContentConfig definition
		// For example, if GenerateContentConfig has a field `ThinkingConfig interface{}`
		// then the above line would work.
		log.Println("BuiltInPlanner.ApplyThinkingConfig: ThinkingConfig application is conceptual.")
	}
}
// BuildPlanningInstruction for BuiltInPlanner.
func (bip *BuiltInPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) { // [cite: 2133]
	return "", nil // Python version returns None, so empty string and no error for Go
}
// ProcessPlanningResponse for BuiltInPlanner.
func (bip *BuiltInPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) { // [cite: 2133, 2134]
	return responseParts, nil
}

// PlanReActPlanner is a placeholder for planners.PlanReActPlanner [cite: 2134]
type PlanReActPlanner struct{}

// Constants for PlanReActPlanner tags [cite: 2134]
const (
	PlanningTag    = "/*PLANNING*/"    // [cite: 2134]
	RePlanningTag  = "/*REPLANNING*/"  // [cite: 2134]
	ReasoningTag   = "/*REASONING*/"   // [cite: 2134]
	ActionTag      = "/*ACTION*/"      // [cite: 2134]
	FinalAnswerTag = "/*FINAL_ANSWER*/" // [cite: 2134]
)

// BuildPlanningInstruction for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) {
	// This directly translates the Python _build_nl_planner_instruction method [cite: 2134]
	highLevelPreamble := fmt.Sprintf(` When answering the question, try to leverage the available tools to gather the information instead of your memorized knowledge. [cite: 2134, 2135]
Follow this process when answering the question: (1) first come up with a plan in natural language text format; [cite: 2135, 2136]
(2) Then use tools to execute the plan and provide reasoning between tool code snippets to make a summary of current state and next step. [cite: 2136, 2137]
Tool code snippets and reasoning should be interleaved with each other. (3) In the end, return one final answer. [cite: 2137]
Follow this format when answering the question: (1) The planning part should be under %s. [cite: 2138]
(2) The tool code snippets should be under %s, and the reasoning parts should be under %s. [cite: 2139]
(3) The final answer part should be under %s. `, PlanningTag, ActionTag, ReasoningTag, FinalAnswerTag) // [cite: 2139, 2140]

	planningPreamble := fmt.Sprintf(` Below are the requirements for the planning: The plan is made to answer the user query if following the plan. The plan is coherent and covers all aspects of information from user query, and only involves the tools that are accessible by the agent. The plan contains the decomposed steps as a numbered list where each step should use one or multiple available tools. By reading the plan, you can intuitively know which tools to trigger or what actions to take. [cite: 2140, 2141]
If the initial plan cannot be successfully executed, you should learn from previous execution results and revise your plan. The revised plan should be be under %s. Then use tools to follow the new plan. `, RePlanningTag) // [cite: 2141]

	// The Python original has reasoning_preamble, final_answer_preamble, tool_code_without_python_libraries_preamble, user_input_preamble
	// These would be similarly defined and concatenated. For brevity, I'm showing the pattern.
	// Assuming those other preamble parts are defined similarly:
	reasoningPreamble := ` Below are the requirements for the reasoning: The reasoning makes a summary of the current trajectory based on the user query and tool outputs. Based on the tool outputs and plan, the reasoning also comes up with instructions to the next steps, making the trajectory closer to the final answer. `
	finalAnswerPreamble := ` Below are the requirements for the final answer: The final answer should be precise and follow query formatting requirements. Some queries may not be answerable with the available tools and information. In those cases, inform the user why you cannot process their query and ask for more information. `
	toolCodeWithoutPythonLibrariesPreamble := ` Below are the requirements for the tool code: **Custom Tools:** The available tools are described in the context and can be directly used. - Code must be valid self-contained Python snippets with no imports and no references to tools or Python libraries that are not in the context. - You cannot use any parameters or fields that are not explicitly defined in the APIs in the context. - The code snippets should be readable, efficient, and directly relevant to the user query and reasoning steps. - When using the tools, you should use the library name together with the function name, e.g., vertex_search.search(). - If Python libraries are not provided in the context, NEVER write your own code other than the function calls using the provided tools. `
	userInputPreamble := ` VERY IMPORTANT instruction that you MUST follow in addition to the above instructions: You should ask for clarification if you need more information to answer the question. You should prefer using the information available in the context instead of repeated tool use. `

	return strings.Join([]string{
		highLevelPreamble,
		planningPreamble,
		reasoningPreamble,
		finalAnswerPreamble,
		toolCodeWithoutPythonLibrariesPreamble,
		userInputPreamble,
	}, "\n\n"), nil
}

// markAsThought marks a response part as a thought.
// In Go, `Part` doesn't have a direct `Thought` field like in Python's genai types.
// This might be handled by a wrapper struct or by convention (e.g., specific role or metadata).
// For this conceptual translation, we'll assume a way to mark it, or it's implicit in the planner's logic.
func (prp *PlanReActPlanner) markAsThought(part *Part) {
	// Conceptual: If Part struct had a Thought field: part.Thought = true
	// Or, this could modify the Part's role or add metadata if the Go ADK defines such conventions.
	// For now, this is a no-op unless Part struct is extended.
	log.Printf("Marking part as thought (conceptual): %v", part.Text)
}


// ProcessPlanningResponse for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) {
	if responseParts == nil { // [cite: 2134]
		return nil, nil
	}

	var preservedParts []Part
	firstFCPartIndex := -1

	for i, part := range responseParts {
		if part.FunctionCall != nil { // [cite: 2134]
			if part.FunctionCall.Name == "" {
				continue
			}
			preservedParts = append(preservedParts, part)
			if firstFCPartIndex == -1 {
				firstFCPartIndex = i
			}
		} else {
			// Handle non-function call parts based on tags
			if part.Text != "" {
				// Split by FINAL_ANSWER_TAG first
				if strings.Contains(part.Text, FINAL_ANSWER_TAG) {
					splits := strings.SplitN(part.Text, FINAL_ANSWER_TAG, 2)
					reasoningText := strings.TrimSpace(splits[0])
					finalAnswerText := ""
					if len(splits) > 1 {
						finalAnswerText = strings.TrimSpace(splits[1])
					}

					if reasoningText != "" {
						reasoningPart := Part{Text: reasoningText}
						prp.markAsThought(&reasoningPart) // Conceptual
						preservedParts = append(preservedParts, reasoningPart)
					}
					if finalAnswerText != "" {
						preservedParts = append(preservedParts, Part{Text: finalAnswerText})
					}
				} else {
					// If no FINAL_ANSWER_TAG, check for other thought tags
					isThought := false
					for _, tag := range []string{PlanningTag, ReasoningTag, ActionTag, RePlanningTag} {
						if strings.HasPrefix(part.Text, tag) {
							isThought = true
							break
						}
					}
					currentPart := part
					if isThought {
						prp.markAsThought(&currentPart) // Conceptual
					}
					preservedParts = append(preservedParts, currentPart)
				}
			}
		}
	}
	return preservedParts, nil
}

// ApplyThinkingConfig for PlanReActPlanner (not directly applicable as it doesn't use genai.ThinkingConfig)
func (prp *PlanReActPlanner) ApplyThinkingConfig(llmReq *LlmRequest) {
	// PlanReActPlanner builds instructions directly, doesn't use genai.ThinkingConfig
}


// LlmAgent interface needs to be fully defined for the below to be valid outside this file.
// For example, within an LlmAgent struct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentPlanner Planner
}

func (a *LlmAgentStruct) GetPlanner() Planner {
    return a.AgentPlanner
}
*/

// NlPlanningRequestProcessor handles NL planning for LLM flow.
type NlPlanningRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *NlPlanningRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in NlPlanningRequestProcessor")
			return
		}

		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}
		// Python has: if isinstance(planner, BuiltInPlanner): planner.apply_thinking_config(llm_req)
		// In Go, this would typically be handled by type assertion or checking an interface method.
		if bp, ok := planner.(*BuiltInPlanner); ok { // [cite: 2133]
			bp.ApplyThinkingConfig(llmReq) // [cite: 2133]
		}


		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx}
		instruction, err := planner.BuildPlanningInstruction(readonlyCtx, llmReq) // [cite: 2133]
		if err != nil {
			log.Printf("Error building planning instruction: %v", err)
			// Optionally send an error event or handle error
			return
		}
		if instruction != "" {
			// Assuming LlmRequest has AppendInstructions
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + instruction}}
		}
	}()
	return outCh, nil
}

// NlPlanningResponseProcessor processes LLM responses for NL planning.
type NlPlanningResponseProcessor struct{}

// RunAsync implements the BaseLlmResponseProcessor interface (conceptual).
func (p *NlPlanningResponseProcessor) RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || llmResp == nil || llmResp.Content == nil {
			log.Println("Error: InvocationContext, Agent, or LlmResponse content is nil in NlPlanningResponseProcessor")
			return
		}
		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}

		callbackCtx := &CallbackContext{InvocationContext: invocationCtx, EventActions: modelResponseEvent.Actions} // [cite: 2133]
		if callbackCtx.EventActions == nil { // Ensure EventActions is not nil
		    callbackCtx.EventActions = &EventActions{}
		}


		processedParts, err := planner.ProcessPlanningResponse(callbackCtx, llmResp.Content.Parts) // [cite: 2134]
		if err != nil {
			log.Printf("Error processing planning response: %v", err)
			// Optionally send an error event or handle error
			return
		}

		if processedParts != nil {
			llmResp.Content.Parts = processedParts
		}

		// If callbackCtx.State has changed (meaning planner updated state via EventActions)
		// and an event needs to be yielded. In Python, this is handled by the framework.
		// Here, we might need to explicitly yield an event if the state delta implies one.
		// However, the processor's role is usually to modify the llmResp or yield new events based on llmResp processing.
		// If state changes in callbackCtx are meant to produce a new event, that logic would go here.
		// For now, this processor primarily modifies the llmResp. If an event with updated actions
		// is needed, it would be constructed and sent to outCh.
		// This part of the Python code doesn't explicitly yield an Event from this processor.

	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/auto_flow.go
package llmflows

import (
	"fmt"
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
)

// AutoFlow dynamically selects and runs the appropriate flow for an agent.
// In Go, the dynamic creation of the LlmAgent itself as done in Python's AutoFlow
// (creating an LlmAgent on the fly with specific tools/planners based on config)
// is less straightforward due to static typing.
// Typically, the LlmAgent would already be constructed with its configuration.
// This AutoFlow equivalent will focus on selecting the flow and running it.
type AutoFlow struct {
	// Configuration for AutoFlow, if any, can go here.
	// For example, a map of agent types to flow constructors.
}

// NewAutoFlow creates a new AutoFlow.
func NewAutoFlow() *AutoFlow {
	return &AutoFlow{}
}

// RunAsync executes the appropriate flow for the given agent.
func (af *AutoFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	if invocationCtx == nil || invocationCtx.Agent == nil {
		err := fmt.Errorf("AutoFlow: InvocationContext or Agent is nil")
		log.Println(err)
		// Return a channel that immediately closes or sends an error event
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	agent := invocationCtx.Agent // Agent from InvocationContext

	// The Python AutoFlow can dynamically create an LlmAgent if the provided agent
	// is just a configuration. In Go, we'd expect `agent` to already be a runnable LlmAgent.
	// If `agent` is a configuration struct, a factory would be needed here to build the LlmAgent.

	llmAgent, ok := agent.(LlmAgent) // Assuming the agent in context is already an LlmAgent
	if !ok {
		err := fmt.Errorf("AutoFlow: agent in InvocationContext is not an LlmAgent, but %T", agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	// --- Flow Selection Logic ---
	// In Python, this checks agent.flow or agent.tools, agent.planner etc.
	// For Go, we'll assume a simple logic: if the agent is an LlmAgent, use SingleFlow.
	// More complex logic could be based on LlmAgent's properties or specific type.

	var selectedFlow interface { // Using interface{} to represent a generic Flow type
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	}

	// Example: If LlmAgent has a GetFlowType() method or similar configuration.
	// flowType := llmAgent.GetFlowType() // Hypothetical method
	// switch flowType {
	// case "single":
	//  selectedFlow = NewSingleFlow()
	// case "sequential":
	//  selectedFlow = NewSequentialFlow() // If SequentialFlow exists
	// default:
	//  selectedFlow = NewSingleFlow() // Default
	// }

	// For now, directly use SingleFlow if it's an LlmAgent
	if _, isLlmAgent := agent.(LlmAgent); isLlmAgent {
		log.Printf("AutoFlow: Using SingleFlow for LlmAgent: %s", llmAgent.GetName())
		selectedFlow = NewSingleFlow()
	} else {
		// Handle other agent types if necessary, or default
		err := fmt.Errorf("AutoFlow: Unhandled agent type %T for flow selection. Defaulting to SingleFlow if possible or erroring.", agent)
		log.Println(err)
        // Attempt to use SingleFlow if the base agent can be cast.
        // This part is tricky without knowing all agent types.
        // For robustnes, it's better to ensure llmAgent is correctly typed above.
		log.Println("AutoFlow: Defaulting to SingleFlow (or error if not LlmAgent).")
		selectedFlow = NewSingleFlow() // This will run if the initial cast to LlmAgent succeeded.
	}


	if selectedFlow == nil {
		err := fmt.Errorf("AutoFlow: Could not determine a flow for agent: %s (%T)", llmAgent.GetName(), agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}

	// Type assert selectedFlow to the expected Flow interface/type to call RunAsync
	// This assumes all flows (SingleFlow, etc.) implement this method signature.
	concreteFlow, flowOk := selectedFlow.(interface {
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	})
	if !flowOk {
		err := fmt.Errorf("AutoFlow: Selected flow does not implement the required RunAsync method signature.")
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}


	log.Printf("AutoFlow: Running flow for agent: %s", llmAgent.GetName())
	return concreteFlow.RunAsync(invocationCtx, llmAgent)
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/single_flow.go
package llmflows

import (
	"github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
)

// SingleFlow represents a simple LLM flow with a standard set of processors.
type SingleFlow struct {
	*BaseLlmFlow
}

// NewSingleFlow creates a new SingleFlow.
func NewSingleFlow() *SingleFlow {
	requestProcessors := []processors.BaseLlmRequestProcessor{
		&processors.BasicRequestProcessor{},
		&processors.IdentityLlmRequestProcessor{},
		&processors.ToolLlmRequestProcessor{}, // Assuming ToolLlmRequestProcessor is defined
		&processors.NlPlanningRequestProcessor{},
		&processors.InstructionsLlmRequestProcessor{},
		&processors.ContentLlmRequestProcessor{},
		&processors.CodeExecutionRequestProcessor{}, // Assuming CodeExecutionRequestProcessor is defined
	}

	responseProcessors := []processors.BaseLlmResponseProcessor{
		&processors.NlPlanningResponseProcessor{},
		&processors.FunctionCallResponseProcessor{}, // Assuming FunctionCallResponseProcessor is defined
		&processors.CodeExecutionResponseProcessor{},// Assuming CodeExecutionResponseProcessor is defined
		&processors.ToolUsageResponseProcessor{},    // Assuming ToolUsageResponseProcessor is defined
	}

	baseFlow := NewBaseLlmFlow(requestProcessors, responseProcessors)
	return &SingleFlow{BaseLlmFlow: baseFlow}
}

// Ensure SingleFlow implements an expected Flow interface if one exists
// type Flow interface {
//    RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error)
// }
// var _ Flow = &SingleFlow{}

# /Users/gopher/Desktop/workspace/adk-go/golang_code.txt
# /Users/gopher/Desktop/workspace/adk-go/cmd/helloworld_runner/main.go
// File: github.com/KennethanCeyer/adk-go/cmd/helloworld_runner/main.go
package main

import (
	"bufio"
	"context"
	"fmt"
	"log"
	"os"
	"os/signal"
	"strings"
	"syscall"

	"github.com/KennethanCeyer/adk-go/examples/helloworld"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
)

func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	go func() {
		<-sigChan
		log.Println("Shutdown signal received, cancelling context...")
		cancel()
	}()

	if helloworld.ConcreteLlmAgent == nil {
		log.Fatal("HelloWorld agent (ConcreteLlmAgent) is not initialized. Check init() in examples/helloworld/agent.go and ensure GEMINI_API_KEY is set.")
	}
	agentToRun := helloworld.ConcreteLlmAgent

	fmt.Printf("Starting agent: %s (%s)\n", agentToRun.GetName(), agentToRun.GetDescription())
	fmt.Println("Model:", agentToRun.GetModelIdentifier())
	fmt.Println("Available tools:", agentToRun.GetTools())
	fmt.Println("Type 'exit' or 'quit' to stop.")

	var history []modelstypes.Content // History stores Content directly
	scanner := bufio.NewScanner(os.Stdin)

	for {
		fmt.Print("[user]: ")
		if !scanner.Scan() {
			if err := scanner.Err(); err != nil {
				log.Printf("Error reading input: %v", err)
			}
			break // EOF or error
		}
		userInputText := strings.TrimSpace(scanner.Text())

		if strings.ToLower(userInputText) == "exit" || strings.ToLower(userInputText) == "quit" {
			fmt.Println("Exiting agent.")
			break
		}
		if userInputText == "" {
			continue
		}

		currentUserContent := modelstypes.Content{
			Role:  "user",
			Parts: []modelstypes.Part{{Text: &userInputText}},
		}

		// Pass history and current user content to the agent's Process method
		agentResponseContent, err := agentToRun.Process(ctx, history, currentUserContent)
		if err != nil {
			log.Printf("Error processing message with agent %s: %v", agentToRun.GetName(), err)
			// Add user message to history even on error for context
			history = append(history, currentUserContent)
			// Display error to user
			errorMsg := fmt.Sprintf("Sorry, I encountered an error: %v", err)
			fmt.Printf("[%s-error]: %s\n", agentToRun.GetName(), errorMsg)
			// Optionally add an error "model" response to history
			// history = append(history, modelstypes.Content{Role: "model", Parts: []modelstypes.Part{{Text: &errorMsg}}})
			continue
		}

		// Add current user input and agent's response to history
		history = append(history, currentUserContent)
		if agentResponseContent != nil { // Ensure response is not nil before appending
			history = append(history, *agentResponseContent)
		}


		// Display agent's response
		if agentResponseContent != nil && len(agentResponseContent.Parts) > 0 {
			var responseTexts []string
			for _, part := range agentResponseContent.Parts {
				if part.Text != nil {
					responseTexts = append(responseTexts, *part.Text)
				}
				if part.FunctionCall != nil { // The agent's Process method should handle FCs internally.
					responseTexts = append(responseTexts, fmt.Sprintf("[Agent wants to call tool: %s with args: %+v. This is unexpected here.]", part.FunctionCall.Name, part.FunctionCall.Args))
				}
			}
			fmt.Printf("[%s]: %s\n", agentToRun.GetName(), strings.Join(responseTexts, "\n"))
		} else {
			fmt.Printf("[%s]: (Received no parsable content from agent)\n", agentToRun.GetName())
		}


		const maxHistoryItems = 10 // Keep last 5 turns (user + model)
		if len(history) > maxHistoryItems {
			history = history[len(history)-maxHistoryItems:]
		}
	}
	log.Println("HelloWorld Agent Runner finished.")
}

# /Users/gopher/Desktop/workspace/adk-go/tools/types/types.go
package types

import (
	"context"
)

type Tool interface {
	Name() string
	Description() string
	Parameters() any // e.g., map[string]any for JSON schema
	Execute(ctx context.Context, args any) (result any, err error)
}

# /Users/gopher/Desktop/workspace/adk-go/llmproviders/gemini.go
package llmproviders

import (
	"context"
	"fmt"
	"log"
	"os"

	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools/types"

	"github.com/google/generative-ai-go/genai"
	"google.golang.org/api/option"
)

type GeminiLLMProvider struct {
	apiKey string
}

func NewGeminiLLMProvider() (*GeminiLLMProvider, error) {
	apiKey := os.Getenv("GEMINI_API_KEY")
	if apiKey == "" {
		return nil, fmt.Errorf("GEMINI_API_KEY environment variable not set")
	}
	return &GeminiLLMProvider{apiKey: apiKey}, nil
}

func convertADKToolsToGenaiTools(adkTools []toolstypes.Tool) []*genai.Tool {
	if len(adkTools) == 0 {
		return nil
	}
	genaiTools := make([]*genai.Tool, len(adkTools))
	for i, t := range adkTools {
		var paramSchema *genai.Schema
		paramSchemaData := t.Parameters() // Expects JSON schema as map[string]any or *genai.Schema
		if paramSchemaData != nil {
			if schemaMap, ok := paramSchemaData.(map[string]any); ok {
				paramSchema = &genai.Schema{Type: genai.TypeObject, Properties: make(map[string]*genai.Schema)}
				if props, ok := schemaMap["properties"].(map[string]any); ok {
					for k, v := range props {
						propSchemaMap, _ := v.(map[string]any)
						propTypeStr, _ := propSchemaMap["type"].(string)
						propDesc, _ := propSchemaMap["description"].(string)
						var propType genai.SchemaType
						switch propTypeStr {
						case "string":
							propType = genai.TypeString
						case "integer", "number":
							propType = genai.TypeNumber // Use TypeNumber for broader compatibility
						case "boolean":
							propType = genai.TypeBoolean
						default:
							propType = genai.TypeString
						}
						paramSchema.Properties[k] = &genai.Schema{Type: propType, Description: propDesc}
					}
				}
				if required, ok := schemaMap["required"].([]any); ok {
					for _, req := range required {
						if rStr, rOk := req.(string); rOk {
							paramSchema.Required = append(paramSchema.Required, rStr)
						}
					}
				}
			} else if schemaGenai, ok := paramSchemaData.(*genai.Schema); ok {
				paramSchema = schemaGenai
			}
		}

		genaiTools[i] = &genai.Tool{
			FunctionDeclarations: []*genai.FunctionDeclaration{{
				Name:        t.Name(),
				Description: t.Description(),
				Parameters:  paramSchema,
			}},
		}
	}
	return genaiTools
}

func convertADKContentToGenaiContent(adkContents []modelstypes.Content) []*genai.Content {
	var genaiContents []*genai.Content
	for _, adkContent := range adkContents {
		genaiC := &genai.Content{Role: adkContent.Role}
		for _, adkPart := range adkContent.Parts {
			var genaiP genai.Part
			if adkPart.Text != nil {
				genaiP = genai.Text(*adkPart.Text)
			} else if adkPart.FunctionCall != nil {
				if adkContent.Role == "model" { // FunctionCall parts in history are from the model
					genaiP = genai.FunctionCall{Name: adkPart.FunctionCall.Name, Args: adkPart.FunctionCall.Args}
				} else {
					continue // Skip user-originated FC parts in history for genai format
				}
			} else if adkPart.FunctionResponse != nil {
				// FunctionResponse parts for genai are typically role "function" or "tool"
				// but genai.Content uses "user" role for FunctionResponse inputs.
				if adkContent.Role != "user" && adkContent.Role != "function" && adkContent.Role != "tool" {
					// Forcing role to "user" if it's a function response part, as per genai SDK's current chat expectations
					// This might need adjustment if genai SDK changes or for different models.
					// Or the input adkContent should already have the correct role.
					log.Printf("Warning: FunctionResponse part with role %s, genai expects 'user' or 'tool' for FR inputs. Adjusting role for this part context.", adkContent.Role)
				}
				genaiP = genai.FunctionResponse{Name: adkPart.FunctionResponse.Name, Response: adkPart.FunctionResponse.Response}
			} else {
				continue
			}
			genaiC.Parts = append(genaiC.Parts, genaiP)
		}
		if len(genaiC.Parts) > 0 {
			genaiContents = append(genaiContents, genaiC)
		}
	}
	return genaiContents
}

func convertGenaiCandidateToADKContent(candidate *genai.Candidate) *modelstypes.Content {
	adkContent := &modelstypes.Content{Role: "model"} // LLM responses are from "model"
	if candidate == nil || candidate.Content == nil {
		text := "Error: LLM returned no content."
		adkContent.Parts = []modelstypes.Part{{Text: &text}}
		return adkContent
	}

	for _, genaiP := range candidate.Content.Parts {
		var adkP modelstypes.Part
		switch v := genaiP.(type) {
		case genai.Text:
			text := string(v)
			adkP.Text = &text
		case genai.FunctionCall:
			adkP.FunctionCall = &modelstypes.FunctionCall{Name: v.Name, Args: v.Args}
		default:
			unsupportedText := fmt.Sprintf("Unsupported genai.Part type received: %T", v)
			adkP.Text = &unsupportedText
		}
		adkContent.Parts = append(adkContent.Parts, adkP)
	}
	return adkContent
}

func (g *GeminiLLMProvider) GenerateContent(
	ctx context.Context,
	modelName string,
	systemInstruction *modelstypes.Content,
	tools []toolstypes.Tool,
	history []modelstypes.Content,
	latestContent modelstypes.Content,
) (*modelstypes.Content, error) {
	client, err := genai.NewClient(ctx, option.WithAPIKey(g.apiKey))
	if err != nil {
		return nil, fmt.Errorf("failed to create genai client: %w", err)
	}
	defer client.Close()

	model := client.GenerativeModel(modelName)

	if systemInstruction != nil && len(systemInstruction.Parts) > 0 {
		// Assuming system instruction is primarily text
		if systemInstruction.Parts[0].Text != nil {
			model.SystemInstruction = genai.Text(*systemInstruction.Parts[0].Text)
		}
	}

	if len(tools) > 0 {
		model.Tools = convertADKToolsToGenaiTools(tools)
	}

	chatHistory := convertADKContentToGenaiContent(history)
	convertedLatestContent := convertADKContentToGenaiContent([]modelstypes.Content{latestContent})

	if len(convertedLatestContent) == 0 || len(convertedLatestContent[0].Parts) == 0 {
		return nil, fmt.Errorf("latest message resulted in no usable content for LLM")
	}

	session := model.StartChat()
	if len(chatHistory) > 0 {
		session.History = chatHistory
	}
	
	// Use SendMessage for non-streaming to get full response at once
	resp, err := session.SendMessage(ctx, convertedLatestContent[0].Parts...)
	if err != nil {
		// Check for specific block reasons
		var blockedText string
		if resp != nil && resp.PromptFeedback != nil && resp.PromptFeedback.BlockReason != genai.BlockReasonUnspecified {
			blockedText = fmt.Sprintf("LLM response blocked. Reason: %s. Message: %s",
				resp.PromptFeedback.BlockReason.String(),
				resp.PromptFeedback.BlockReasonMessage)
			return &modelstypes.Content{Role: "model", Parts: []modelstypes.Part{{Text: &blockedText}}}, nil
		}
		return nil, fmt.Errorf("genai SendMessage failed: %w", err)
	}
	

	if resp == nil || len(resp.Candidates) == 0 {
		noCandidateText := "LLM returned no candidates."
		// Check for finish reason if available
		if resp != nil && len(resp.Candidates) > 0 && resp.Candidates[0].FinishReason != genai.FinishReasonUnspecified && resp.Candidates[0].FinishReason != genai.FinishReasonStop {
			noCandidateText = fmt.Sprintf("LLM generation finished with reason: %s", resp.Candidates[0].FinishReason.String())
		} else if resp != nil && resp.PromptFeedback != nil && resp.PromptFeedback.BlockReason != genai.BlockReasonUnspecified {
            noCandidateText = fmt.Sprintf("LLM response blocked. Reason: %s. Message: %s",
                resp.PromptFeedback.BlockReason.String(),
                resp.PromptFeedback.BlockReasonMessage)
        }
		return &modelstypes.Content{Role: "model", Parts: []modelstypes.Part{{Text: &noCandidateText}}}, nil
	}

	return convertGenaiCandidateToADKContent(resp.Candidates[0]), nil
}

# /Users/gopher/Desktop/workspace/adk-go/llmproviders/interfaces/interfaces.go
package llmproviders

import (
	"context"

	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools/types"
)

type LLMProvider interface {
	GenerateContent(
		ctx context.Context,
		modelName string,
		systemInstruction *modelstypes.Content,
		tools []toolstypes.Tool,
		history []modelstypes.Content, // Changed from Message to Content for direct use with genai
		latestContent modelstypes.Content, // Changed from Message to Content
	) (*modelstypes.Content, error) // Returns the LLM's response content
}

# /Users/gopher/Desktop/workspace/adk-go/auth/types/types.go
package types

type AuthConfigName string

type AuthConfig struct {
	Name         AuthConfigName
	AuthScheme   string
	Scope        string
	ClientID     string
	ClientSecret string
	// Other scheme-specific fields (e.g., auth_url, token_url)
}

type AuthCredential struct {
	Name         AuthConfigName
	AccessToken  string
	RefreshToken string
	ExpiresIn    int64 // Seconds
	ObtainedAt   int64 // Unix timestamp
	Scope        string
	IDToken      string
}

func (ac *AuthCredential) IsExpired() bool {
	if ac.AccessToken == "" || ac.ExpiresIn <= 0 || ac.ObtainedAt <= 0 {
		return true
	}
	// Needs actual time checking logic, e.g. using time.Now().Unix()
	// return time.Now().Unix() >= (ac.ObtainedAt + ac.ExpiresIn - 60) // 60s buffer
	return false // Placeholder
}

// AuthHandler interface (can also be in auth/interfaces/interfaces.go)
// To avoid too many "types" packages, interfaces can live with their functional code or in a dedicated interfaces sub-package.
// For now, keeping it here for simplicity of this correction step.
// If AuthHandler needs to be used by other packages that auth might import,
// it would need to be in a more common or dedicated interface package.
type AuthHandler interface {
	GetAllAuthConfigs() []AuthConfig
	GetAuthConfig(name AuthConfigName) (*AuthConfig, error)
	// Session type will be from github.com/KennethanCeyer/adk-go/sessions/types
	GetCredential(name AuthConfigName, session any) (AuthCredential, error)
	// IsToolAuthorized(toolName string, authConfigsUsedByTool []AuthConfigName, session any) (bool, error)
	// GetAuthTool() any // Would return a toolstypes.Tool
}

# /Users/gopher/Desktop/workspace/adk-go/auth/auth_preprocessor.go
package auth

import (
	"fmt"
	"log"
	"strings"
	"time"

	agentiface "github.com/KennethanCeyer/adk-go/agents"
	"github.com/KennethanCeyer/adk-go/agents/invocation"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"

	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions"
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

const defaultAuthInstructionTemplate = `
You have access to a set of tools that may require user authorization.
If you need to use a tool that requires authorization, you must first request authorization by calling the "%s" function.
The function call will return one of the following:
- A success message: "Successfully obtained credential."
- An error message: "Failed to obtain credential. <reason>"
- A message indicating that user interaction is required: "User interaction is required to obtain credential. <auth_url>"
Only if the function call returns a success message, you can then proceed to use the tool that requires authorization.
Otherwise, you must inform the user about the error or the required user interaction.

Current user:
<User>
%s
</User>
`

type AuthLlmRequestProcessor struct{}

// BaseLlmRequestProcessor interface (should be defined elsewhere, e.g., flows/llmflows/processors/interfaces.go)
type BaseLlmRequestProcessor interface {
	RunAsync(invocationCtx *invocation.InvocationContext, llmReq *modelstypes.LlmRequest) (<-chan *eventstypes.Event, error)
}

var _ BaseLlmRequestProcessor = (*AuthLlmRequestProcessor)(nil)

func (p *AuthLlmRequestProcessor) RunAsync(
	invocationCtx *invocation.InvocationContext,
	llmReq *modelstypes.LlmRequest,
) (<-chan *eventstypes.Event, error) {
	outCh := make(chan *eventstypes.Event)

	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || invocationCtx.Session == nil {
			log.Println("AuthLlmRequestProcessor: InvocationContext, Agent, or Session is nil.")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(agentiface.LlmAgent)
		if !ok {
			// invocationCtx.Agent is agentiface.Agent. We need to check if it's also an LlmAgent.
			// This might require a type assertion from the specific AgentInterface used in InvocationContext.
			// For now, assuming InvocationContext.Agent can be directly asserted if it's set with an LlmAgent.
			log.Printf("AuthLlmRequestProcessor: Agent in InvocationContext is not an LlmAgent.")
			return
		}

		authHandler := llmAgent.GetAuthHandler()
		if authHandler == nil {
			return
		}

		authConfigs := authHandler.GetAllAuthConfigs()
		if len(authConfigs) == 0 {
			return
		}

		// Cast session from any to *sessionstypes.Session for GetCredential
		// This relies on AuthHandler's GetCredential taking `any` and handling the cast,
		// or defining GetCredential more specifically. The authtypes.AuthHandler definition
		// uses `any` for session, so it's consistent for now.
		userInfo := p.buildUserInfo(invocationCtx.Session, authConfigs, authHandler)
		authInstruction := p.buildAuthInstruction(userInfo)

		if llmReq.Config == nil {
			llmReq.Config = &modelstypes.GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &modelstypes.Content{Parts: []modelstypes.Part{}}
		}

		var currentSysInstructionText string
		if len(llmReq.Config.SystemInstruction.Parts) > 0 && llmReq.Config.SystemInstruction.Parts[0].Text != nil {
			currentSysInstructionText = *llmReq.Config.SystemInstruction.Parts[0].Text
		}

		if currentSysInstructionText != "" {
			currentSysInstructionText += "\n\n"
		}
		newInstructionText := currentSysInstructionText + authInstruction
		
		if len(llmReq.Config.SystemInstruction.Parts) == 0 {
		    llmReq.Config.SystemInstruction.Parts = append(llmReq.Config.SystemInstruction.Parts, modelstypes.Part{Text: &newInstructionText})
		} else {
		    llmReq.Config.SystemInstruction.Parts[0].Text = &newInstructionText
		}

		if p.isAuthFlowCompletedInPriorTurn(invocationCtx.Session.Events) {
			log.Println("AuthLlmRequestProcessor: Auth flow was recently completed or attempted in a prior turn.")
		}
	}()

	return outCh, nil
}

func (p *AuthLlmRequestProcessor) buildUserInfo(
	session *sessionstypes.Session,
	authConfigs []authtypes.AuthConfig,
	authHandler authtypes.AuthHandler,
) string {
	if session == nil {
		return "User information not available (session is nil)."
	}
	var userInfoParts []string
	userInfoParts = append(userInfoParts, fmt.Sprintf("  User ID: %s", session.UserID))
	userInfoParts = append(userInfoParts, fmt.Sprintf("  App ID: %s", session.AppName))

	for _, authConfig := range authConfigs {
		credential, err := authHandler.GetCredential(authConfig.Name, session) // Pass session directly
		status := "Not authorized"

		if err != nil {
			log.Printf("AuthLlmRequestProcessor: Error getting credential for %s: %v", authConfig.Name, err)
			status = fmt.Sprintf("Error checking authorization for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
		} else {
			if credential.AccessToken != "" {
				if !credential.IsExpired() { // IsExpired defined on authtypes.AuthCredential
					status = fmt.Sprintf("Authorized for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
				} else {
					status = fmt.Sprintf("Authorization expired for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
				}
			} else {
				status = fmt.Sprintf("Not authorized for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
			}
		}
		userInfoParts = append(userInfoParts, "  "+status)
	}
	return strings.Join(userInfoParts, "\n")
}


func (p *AuthLlmRequestProcessor) buildAuthInstruction(userInfo string) string {
	return fmt.Sprintf(defaultAuthInstructionTemplate, functions.REQUEST_EUC_FUNCTION_CALL_NAME, userInfo)
}

func (p *AuthLlmRequestProcessor) isAuthFlowCompletedInPriorTurn(historyEvents []*eventstypes.Event) bool {
	if len(historyEvents) == 0 {
		return false
	}
	const turnsToLookBack = 2
	maxEventsToCheck := turnsToLookBack * 2
	if maxEventsToCheck > len(historyEvents) {
		maxEventsToCheck = len(historyEvents)
	}

	for i := len(historyEvents) - 1; i >= len(historyEvents)-maxEventsToCheck; i-- {
		event := historyEvents[i]
		if event == nil || event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil && part.FunctionResponse.Name == functions.REQUEST_EUC_FUNCTION_CALL_NAME {
				return true
			}
		}
	}
	return false
}

func newEventID() commontypes.EventID {
	return commontypes.EventID(fmt.Sprintf("evt-%d", time.Now().UnixNano()))
}

func getTimestamp() commontypes.Timestamp {
	return commontypes.Timestamp(float64(time.Now().UnixNano()) / 1e9)
}

# /Users/gopher/Desktop/workspace/adk-go/agents/invocation/inovocation.go
package invocation

import (
	"context"

	agentiface "github.com/KennethanCeyer/adk-go/agents"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

type RunConfig struct {
	// Fields for run-specific configurations if needed.
}

type InvocationContext struct {
	Ctx          context.Context
	InvocationID commontypes.InvocationID
	Agent        agentiface.Agent // The agent being invoked
	Session      *sessionstypes.Session
	UserContent  *modelstypes.Content // Initial user input for this specific invocation
	RunConfig    *RunConfig
}

type ReadonlyContext struct {
	invocationCtx *InvocationContext
}

func NewReadonlyContext(invCtx *InvocationContext) *ReadonlyContext {
	return &ReadonlyContext{invocationCtx: invCtx}
}

// Add getter methods for ReadonlyContext if specific fields need to be exposed.

# /Users/gopher/Desktop/workspace/adk-go/agents/base_agent.go
package agents

import (
	"fmt"
	"regexp"
	// Hypothetical Go ADK packages
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/genai/types" // For genai.Content
	// "github.com/KennethanCeyer/adk-go/telemetry"
)

// Forward declarations for types that would be defined in other packages.
// In a real Go ADK, these would be imported.
type Content struct { // Placeholder for genai.types.Content
	Role  string `json:"role,omitempty"`
	Parts []Part `json:"parts,omitempty"`
}

type Part struct { // Placeholder for genai.types.Part
	Text string `json:"text,omitempty"`
	// Other fields like FunctionCall, FunctionResponse, InlineData would be here
}

type Event struct { // Placeholder for events.Event
	InvocationID string       `json:"invocationId,omitempty"`
	Author       string       `json:"author,omitempty"`
	Branch       string       `json:"branch,omitempty"`
	Content      *Content     `json:"content,omitempty"`
	Actions      EventActions `json:"actions,omitempty"` // Placeholder
	ID           string       `json:"id,omitempty"`
	Timestamp    float64      `json:"timestamp,omitempty"`
	// ... other Event fields
}

type EventActions struct { // Placeholder for events.EventActions
	StateDelta map[string]interface{} `json:"stateDelta,omitempty"`
	// ... other EventActions fields
}

type InvocationContext struct { // Placeholder
	Agent            Agent
	Branch           string
	InvocationID     string
	Session          *Session // Placeholder
	EndInvocation    bool
	UserContent      *Content
	ArtifactService  interface{} // Placeholder
	SessionService   interface{} // Placeholder
	MemoryService    interface{} // Placeholder
	LiveRequestQueue interface{} // Placeholder for LiveRequestQueue
	RunConfig        *RunConfig  // Placeholder
}

type Session struct { // Placeholder for sessions.Session
	State map[string]interface{}
	// ... other Session fields
}

type RunConfig struct { // Placeholder for RunConfig
	// ... RunConfig fields
}

type CallbackContext struct { // Placeholder
	InvocationContext *InvocationContext
	EventActions      EventActions
	State             MutableState // Placeholder for a state map that tracks deltas
}

type MutableState map[string]interface{} // Simplified placeholder

func (ms MutableState) HasDelta() bool {
	// In a real implementation, this would check if _delta has items
	return len(ms) > 0 // Simplified
}

// Agent is the base interface for all agents in the Agent Development Kit.
type Agent interface {
	GetName() string
	GetDescription() string
	ParentAgent() Agent
	SetParentAgent(parent Agent) error
	SubAgents() []Agent
	AddSubAgent(subAgent Agent)
	RunAsync(parentCtx *InvocationContext) (<-chan *Event, error)
	RunLive(parentCtx *InvocationContext) (<-chan *Event, error)
	RootAgent() Agent
	FindAgent(name string) Agent
	FindSubAgent(name string) Agent
	GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback
	GetCanonicalAfterAgentCallbacks() []SingleAgentCallback
}

// SingleAgentCallback defines the signature for agent lifecycle callbacks.
type SingleAgentCallback func(callbackCtx *CallbackContext) (*Content, error)

// BaseAgent provides a common structure for agents.
type BaseAgent struct {
	Name                   string                `json:"name"`
	Description            string                `json:"description,omitempty"`
	Parent                 Agent                 `json:"-"` // Exclude from JSON, handle manually
	Subs                   []Agent               `json:"subAgents,omitempty"`
	BeforeAgentCallback    []SingleAgentCallback `json:"-"` // Callbacks are not typically part of serialized state
	AfterAgentCallback     []SingleAgentCallback `json:"-"`
	agentInvocationContext *InvocationContext    // Internal context for the current run
}

// NewBaseAgent creates a new BaseAgent.
// Name must be a valid Go identifier and not "user".
func NewBaseAgent(name string, description string) (*BaseAgent, error) {
	if !isValidIdentifier(name) {
		return nil, fmt.Errorf("invalid agent name: `%s`. Agent name must be a valid identifier", name)
	}
	if name == "user" {
		return nil, fmt.Errorf("agent name cannot be `user`. `user` is reserved for end-user's input")
	}
	return &BaseAgent{
		Name:        name,
		Description: description,
		Subs:        make([]Agent, 0),
	}, nil
}

func (ba *BaseAgent) GetName() string {
	return ba.Name
}

func (ba *BaseAgent) GetDescription() string {
	return ba.Description
}

func (ba *BaseAgent) ParentAgent() Agent {
	return ba.Parent
}

func (ba *BaseAgent) SetParentAgent(parent Agent) error {
	if ba.Parent != nil && ba.Parent != parent {
		return fmt.Errorf("agent `%s` already has a parent agent: `%s`, cannot add to: `%s`", ba.Name, ba.Parent.GetName(), parent.GetName())
	}
	ba.Parent = parent
	return nil
}

func (ba *BaseAgent) SubAgents() []Agent {
	return ba.Subs
}

func (ba *BaseAgent) AddSubAgent(subAgent Agent) {
	if err := subAgent.SetParentAgent(ba); err != nil {
		// Or handle error as appropriate, e.g. log and skip
		fmt.Printf("Error setting parent for sub-agent %s: %v\n", subAgent.GetName(), err)
		return
	}
	ba.Subs = append(ba.Subs, subAgent)
}

// RunAsync is the entry method to run an agent via text-based conversation.
func (ba *BaseAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	// In Go, we'd typically use channels for async generators.
	// The actual implementation of _runAsyncImpl would push events to this channel.
	// The telemetry.tracer logic would wrap the core processing.
	// For this snippet, we'll simulate the channel.

	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run [%s]", ba.Name)):
		
		event, err := ba.handleBeforeAgentCallback(ctx)
		if err != nil {
			// ì–´ë–»ê²Œ ì—ëŸ¬ë¥¼ outChìœ¼ë¡œ ë³´ë‚¼ ì§€ ê³ ë¯¼ í•„ìš”. Eventì— Error í•„ë“œ ì¶”ê°€?
			// For now, log and potentially send an error event.
			fmt.Printf("Error in beforeAgentCallback for %s: %v\n", ba.Name, err)
			// outCh <- &events.Event{Error: err.Error()} // Conceptual
			return
		}
		if event != nil {
			outCh <- event
		}
		if ctx.EndInvocation {
			return
		}

		err = ba.runAsyncImpl(ctx, outCh) // Pass channel to implementation
		if err != nil {
			fmt.Printf("Error in runAsyncImpl for %s: %v\n", ba.Name, err)
			return
		}

		if ctx.EndInvocation {
			return
		}

		event, err = ba.handleAfterAgentCallback(ctx)
		if err != nil {
			fmt.Printf("Error in afterAgentCallback for %s: %v\n", ba.Name, err)
			return
		}
		if event != nil {
			outCh <- event
		}
	}()

	return outCh, nil
}

// runAsyncImpl is the core logic to run this agent via text-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runAsyncImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	// This is where the Python _run_async_impl's `yield Event(...)` would happen.
	// In Go, we send to the channel.
	// Example: outCh <- &events.Event{Author: ba.Name, Content: ...}
	return fmt.Errorf("_runAsyncImpl for %T is not implemented", ba)
}

// RunLive is the entry method to run an agent via video/audio-based conversation.
func (ba *BaseAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run_live [%s]", ba.Name)):
		err := ba.runLiveImpl(ctx, outCh) // Pass channel
		if err != nil {
			fmt.Printf("Error in runLiveImpl for %s: %v\n", ba.Name, err)
			// Potentially send an error event
		}
	}()
	return outCh, nil
}

// runLiveImpl is the core logic to run this agent via video/audio-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runLiveImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	return fmt.Errorf("_runLiveImpl for %T is not implemented", ba)
}

func (ba *BaseAgent) RootAgent() Agent {
	current := ba
	for current.ParentAgent() != nil {
		current = current.ParentAgent().(*BaseAgent) // Assuming BaseAgent for simplicity
	}
	return current
}

func (ba *BaseAgent) FindAgent(name string) Agent {
	if ba.Name == name {
		return ba
	}
	return ba.FindSubAgent(name)
}

func (ba *BaseAgent) FindSubAgent(name string) Agent {
	for _, subAgent := range ba.Subs {
		if found := subAgent.FindAgent(name); found != nil {
			return found
		}
	}
	return nil
}

func (ba *BaseAgent) createInvocationContext(parentCtx *InvocationContext) *InvocationContext {
	// Create a copy and update agent and branch
	// This is a simplified deep copy. Real implementation needs care.
	ctxCopy := *parentCtx
	ctxCopy.Agent = ba
	if parentCtx.Branch != "" {
		ctxCopy.Branch = fmt.Sprintf("%s.%s", parentCtx.Branch, ba.Name)
	} else {
		ctxCopy.Branch = ba.Name
	}
	return &ctxCopy
}

func (ba *BaseAgent) GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback {
	return ba.BeforeAgentCallback
}

func (ba *BaseAgent) GetCanonicalAfterAgentCallbacks() []SingleAgentCallback {
	return ba.AfterAgentCallback
}

func (ba *BaseAgent) handleBeforeAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalBeforeAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalBeforeAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("beforeAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions, // Assuming EventActions is part of CallbackContext
			}
			ctx.EndInvocation = true
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			Actions:      callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

func (ba *BaseAgent) handleAfterAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalAfterAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalAfterAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("afterAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions,
			}
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			// Content might be nil if callback only changed state
			Actions: callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

// isValidIdentifier checks if a string is a valid Go identifier (simplified).
// Go identifiers are more restrictive than Python's.
func isValidIdentifier(name string) bool {
	if name == "" {
		return false
	}
	// Regex for a simplified Go identifier: starts with a letter, followed by letters or digits.
	// Go allows Unicode letters/digits. This regex is simplified.
	// Actual Go spec: letter { letter | unicode_digit } .
	// For ADK agent names, we might enforce stricter ASCII for interoperability or simplicity.
	matched, _ := regexp.MatchString(`^[a-zA-Z_][a-zA-Z0-9_]*$`, name)
	return matched
}

# /Users/gopher/Desktop/workspace/adk-go/agents/llm_agent.go
package agents

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/tools" // Placeholder for tools package
	// "github.com/KennethanCeyer/adk-go/models" // Placeholder for models package
	// "github.com/KennethanCeyer/adk-go/examples" // Placeholder for examples package
	// "github.com/KennethanCeyer/adk-go/planners" // Placeholder for planners package
	// "github.com/KennethanCeyer/adk-go/codeexecutors" // Placeholder for code executors
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, genai.Content etc.
)

// --- Forward declarations / Placeholders ---
// These would be actual types in a full Go ADK.

type BaseLlm struct { // Placeholder for models.BaseLlm
	ModelName string
}

type GenerateContentConfig struct { // Placeholder for genai.GenerateContentConfig
	SystemInstruction   string        `json:"systemInstruction,omitempty"`
	Tools               []Tool        `json:"tools,omitempty"`         // Placeholder for genai.Tool
	ResponseSchema      interface{}   `json:"responseSchema,omitempty"` // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string        `json:"responseMimeType,omitempty"`
	ThinkingConfig      interface{}   `json:"thinkingConfig,omitempty"`   // Placeholder for genai.ThinkingConfig
	Temperature         *float32      `json:"temperature,omitempty"`
	SafetySettings      []interface{} `json:"safetySettings,omitempty"` // Placeholder for genai.SafetySetting
}

type LlmRequest struct { // Placeholder for models.LlmRequest
	Model               string
	Config              *GenerateContentConfig
	Contents            []Content
	ToolsDict           map[string]Tool // Placeholder
	LiveConnectConfig   interface{}     // Placeholder for genai.LiveConnectConfig
	SystemInstruction   string          // Added for simplicity, original ADK appends to config
	OutputSchema        interface{}     // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string
}

func (lr *LlmRequest) AppendInstructions(instructions []string) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	if lr.Config.SystemInstruction != "" {
		lr.Config.SystemInstruction += "\n\n"
	}
	lr.Config.SystemInstruction += strings.Join(instructions, "\n\n")
}

func (lr *LlmRequest) SetOutputSchema(schema interface{}) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	lr.Config.ResponseSchema = schema
	lr.Config.ResponseMIMEType = "application/json"
}

type LlmResponse struct { // Placeholder for models.LlmResponse
	Content         *Content    `json:"content,omitempty"`
	ErrorCode       string      `json:"errorCode,omitempty"`
	ErrorMessage    string      `json:"errorMessage,omitempty"`
	Partial         bool        `json:"partial,omitempty"`
	CustomMetadata  map[string]interface{} `json:"customMetadata,omitempty"`
}


type Tool interface { // Placeholder for tools.BaseTool
	GetName() string
	GetDescription() string
	GetDeclaration() (*ToolDeclaration, error) // Placeholder for genai.FunctionDeclaration
	ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error
}

type ToolDeclaration struct { // Placeholder for genai.FunctionDeclaration
	Name        string      `json:"name"`
	Description string      `json:"description"`
	Parameters  interface{} `json:"parameters"`
}

type FunctionTool struct { // Placeholder for tools.FunctionTool
	BaseToolImpl
	fn interface{}
}

func NewFunctionTool(fn interface{}) *FunctionTool {
	// Simplified name/description extraction
	name := "unknown_function"
	// In Go, reflection for function name/doc is more involved
	// For now, users might need to provide this explicitly if FunctionTool wraps a raw func
	return &FunctionTool{BaseToolImpl: BaseToolImpl{ToolName: name}, fn: fn}
}

func (ft *FunctionTool) GetName() string { return ft.ToolName }
func (ft *FunctionTool) GetDescription() string { return ft.ToolDescription }
func (ft *FunctionTool) GetDeclaration() (*ToolDeclaration, error) { /* TODO */ return nil, nil }
func (ft *FunctionTool) ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error { /* TODO */ return nil }


type BaseToolset interface { // Placeholder for tools.BaseToolset
	GetTools(readonlyCtx *ReadonlyContext) ([]Tool, error)
}

type Example struct { // Placeholder for examples.Example
	Input  Content   `json:"input"`
	Output []Content `json:"output"`
}

type BaseExampleProvider interface { // Placeholder for examples.BaseExampleProvider
	GetExamples(query string) ([]Example, error)
}

type Planner interface { // Placeholder for planners.BasePlanner
	// Methods would be defined here
}

type CodeExecutor interface { // Placeholder for codeexecutors.BaseCodeExecutor
	// Methods would be defined here
}

type ToolContext struct { // Placeholder for tools.ToolContext
	InvocationContext *InvocationContext
	State             MutableState
	// ... other fields like FunctionCallID, EventActions
}

// Represents ReadonlyContext
type ReadonlyContext struct {
	InvocationContext *InvocationContext
}

func (rc *ReadonlyContext) GetState() map[string]interface{} {
	// Return a read-only copy or wrapper
	// For simplicity, returning the direct map for now, assuming no modification.
	if rc.InvocationContext != nil && rc.InvocationContext.Session != nil {
		return rc.InvocationContext.Session.State
	}
	return make(map[string]interface{})
}


// LLMProvider interface (simplified from previous example)
type LLMProvider interface {
    GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error)
    GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error)
    // Potentially a Connect method for bidirectional streaming if needed for live mode
}

// MockLLMProvider for LlmAgent
type MockLLM struct {
	BaseLlm
}

func (m *MockLLM) GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error) {
	// Simulate LLM call
	log.Printf("MockLLM GenerateContentStream called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	ch := make(chan *LlmResponse, 1)
	go func() {
		defer close(ch)
		// Simplified: just echo back part of the first user message if available
		responseText := "Default mock response"
		if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
			responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
		}
		ch <- &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}
	}()
	return ch, nil
}

func (m *MockLLM) GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error) {
	log.Printf("MockLLM GenerateContent called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	responseText := "Default mock response"
	if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
		responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
	}
	return &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}, nil
}


// SingleBeforeModelCallback defines the signature for a single before-model callback.
type SingleBeforeModelCallback func(callbackCtx *CallbackContext, llmReq *LlmRequest) (*LlmResponse, error)

// SingleAfterModelCallback defines the signature for a single after-model callback.
type SingleAfterModelCallback func(callbackCtx *CallbackContext, llmResp *LlmResponse) (*LlmResponse, error)

// SingleBeforeToolCallback defines the signature for a single before-tool callback.
type SingleBeforeToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext) (map[string]interface{}, error)

// SingleAfterToolCallback defines the signature for a single after-tool callback.
type SingleAfterToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext, toolResponse map[string]interface{}) (map[string]interface{}, error)

// InstructionProvider defines a function that provides an instruction string.
type InstructionProvider func(readonlyCtx *ReadonlyContext) (string, error)

// ToolUnion represents a type that can be a callable, a BaseTool, or a BaseToolset.
// In Go, this would typically be handled by interfaces or by checking types at runtime.
// For this conceptual translation, we'll use `interface{}` and runtime type assertions.
type ToolUnion interface{}

// ExamplesUnion represents types that can provide examples.
type ExamplesUnion interface{} // list[Example] or BaseExampleProvider

// LlmAgent is an LLM-based Agent.
type LlmAgent struct {
	BaseAgent
	AgentModel                  interface{} // string or *BaseLlm (actual model instance)
	AgentInstruction            interface{} // string or InstructionProvider
	AgentGlobalInstruction      interface{} // string or InstructionProvider
	AgentTools                  []ToolUnion
	AgentGenerateContentConfig  *GenerateContentConfig
	DisallowTransferToParent    bool
	DisallowTransferToPeers     bool
	IncludeContents             string // "default" or "none"
	InputSchema                 interface{} // reflect.Type of a struct, or nil
	OutputSchema                interface{} // reflect.Type of a struct, or nil
	OutputKey                   string
	AgentPlanner                Planner
	AgentCodeExecutor           CodeExecutor
	AgentExamples               ExamplesUnion
	AgentBeforeModelCallbacks   []SingleBeforeModelCallback
	AgentAfterModelCallbacks    []SingleAfterModelCallback
	AgentBeforeToolCallbacks    []SingleBeforeToolCallback
	AgentAfterToolCallbacks     []SingleAfterToolCallback

	// Internal flow strategy
	llmFlow LlmFlow
}

// LlmFlow defines the behavior of the LLM agent.
type LlmFlow interface {
	RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error)
	RunLive(invocationCtx *InvocationContext) (<-chan *Event, error)
}

// NewLlmAgent creates a new LlmAgent.
// This constructor attempts to mirror Pydantic's initialization logic.
func NewLlmAgent(name, description string, model interface{}, instruction interface{}) (*LlmAgent, error) {
	base, err := NewBaseAgent(name, description)
	if err != nil {
		return nil, err
	}

	agent := &LlmAgent{
		BaseAgent:        *base,
		AgentModel:       model,
		AgentInstruction: instruction,
		IncludeContents:  "default",
		// Default to AutoFlow which includes SingleFlow + agent transfer
		llmFlow: newAutoFlow(), // Placeholder for flow initialization
	}

	// Simulating Pydantic's model_post_init and field_validator logic
	if err := agent.validateAndProcessConfig(); err != nil {
		return nil, err
	}

	return agent, nil
}

func (a *LlmAgent) validateAndProcessConfig() error {
	if err := a.checkOutputSchema(); err != nil {
		return err
	}
	if err := a.validateGenerateContentConfig(); err != nil {
		return err
	}
	// In Python, Pydantic's model_post_init calls __set_parent_agent_for_sub_agents
	// This is handled by AddSubAgent in Go's BaseAgent.

	// Determine flow based on transfer disallowed flags
	if a.DisallowTransferToParent && a.DisallowTransferToPeers && len(a.Subs) == 0 {
		a.llmFlow = newSingleFlow() // Placeholder
	} else {
		a.llmFlow = newAutoFlow() // Placeholder
	}

	return nil
}


// RunAsync implements the Agent interface.
// It delegates to the internal LlmFlow.
func (a *LlmAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx) // from BaseAgent
	a.agentInvocationContext = ctx // Store context for this agent's run
	return a.llmFlow.RunAsync(ctx)
}

// RunLive implements the Agent interface.
func (a *LlmAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx)
	a.agentInvocationContext = ctx
	return a.llmFlow.RunLive(ctx)
}


func (a *LlmAgent) CanonicalModel() (BaseLlm, error) {
	if modelInst, ok := a.AgentModel.(BaseLlm); ok {
		return modelInst, nil
	}
	if modelStr, ok := a.AgentModel.(string); ok && modelStr != "" {
		// Placeholder for LLMRegistry.NewLlm(modelStr)
		// In a real Go ADK, this would look up and instantiate the model.
		return MockLLM{BaseLlm{ModelName: modelStr}}, nil
	}

	current := a.ParentAgent()
	for current != nil {
		if llmAgent, ok := current.(*LlmAgent); ok {
			return llmAgent.CanonicalModel()
		}
		current = current.ParentAgent()
	}
	return MockLLM{}, fmt.Errorf("no model found for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentInstruction.(func(ctx *ReadonlyContext) (string, error)); ok { // Allow func type as well
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, fmt.Errorf("invalid instruction type for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentGlobalInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentGlobalInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentGlobalInstruction.(func(ctx *ReadonlyContext) (string, error)); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, nil // Global instruction can be empty
}

func (a *LlmAgent) CanonicalTools(ctx *ReadonlyContext) ([]Tool, error) {
	var resolvedTools []Tool
	for _, toolUnion := range a.AgentTools {
		switch t := toolUnion.(type) {
		case Tool:
			resolvedTools = append(resolvedTools, t)
		case func(interface{}) interface{}: // Simplified check for a callable
			// This is a very basic check. Robustly handling arbitrary callables
			// and converting them to tools.Tool would require more reflection
			// or specific registration mechanisms.
			// For now, wrapping it in a conceptual FunctionTool.
			resolvedTools = append(resolvedTools, NewFunctionTool(t))
		case BaseToolset:
			toolsFromSet, err := t.GetTools(ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to get tools from toolset: %w", err)
			}
			resolvedTools = append(resolvedTools, toolsFromSet...)
		default:
			return nil, fmt.Errorf("unsupported tool type: %T", t)
		}
	}
	return resolvedTools, nil
}


func (a *LlmAgent) GetCanonicalBeforeModelCallbacks() []SingleBeforeModelCallback {
	return a.AgentBeforeModelCallbacks
}

func (a *LlmAgent) GetCanonicalAfterModelCallbacks() []SingleAfterModelCallback {
	return a.AgentAfterModelCallbacks
}

func (a *LlmAgent) GetCanonicalBeforeToolCallbacks() []SingleBeforeToolCallback {
	return a.AgentBeforeToolCallbacks
}

func (a *LlmAgent) GetCanonicalAfterToolCallbacks() []SingleAfterToolCallback {
	return a.AgentAfterToolCallbacks
}

func (a *LlmAgent) maybeSaveOutputToState(event *Event) {
	if a.OutputKey == "" || !eventIsFinalResponse(event) || event.Content == nil || len(event.Content.Parts) == 0 {
		return
	}

	var resultData interface{}
	var combinedText []string
	for _, part := range event.Content.Parts {
		if part.Text != "" {
			combinedText = append(combinedText, part.Text)
		}
		// In a real scenario, you'd handle other part types like function calls/responses if they constituted the "output"
	}
	fullText := strings.Join(combinedText, "\n")


	if a.OutputSchema != nil {
		// In Go, Pydantic's model_validate_json would be equivalent to json.Unmarshal into a struct.
		// This requires a.OutputSchema to be a reflect.Type of the target struct.
		// For simplicity, this placeholder assumes fullText is valid JSON for the schema.
		// A real implementation would use json.Unmarshal(byte(fullText), &targetStructInstance)
		// and then potentially convert targetStructInstance to map[string]interface{} if needed.
		var tempMap map[string]interface{}
		// This is a simplified placeholder. Real unmarshalling and validation needed.
		if err := json.Unmarshal([]byte(fullText), &tempMap); err == nil {
			resultData = tempMap
		} else {
			log.Printf("Warning: Failed to parse output for key '%s' against schema: %v. Storing raw text.", a.OutputKey, err)
			resultData = fullText // Fallback to raw text
		}
	} else {
		resultData = fullText
	}

	if event.Actions.StateDelta == nil {
		event.Actions.StateDelta = make(map[string]interface{})
	}
	event.Actions.StateDelta[a.OutputKey] = resultData
}

func (a *LlmAgent) checkOutputSchema() error {
	if a.OutputSchema == nil {
		return nil
	}

	if !a.DisallowTransferToParent || !a.DisallowTransferToPeers {
		log.Println(
			fmt.Sprintf("Warning: Invalid config for agent %s: outputSchema cannot co-exist with agent transfer configurations. Setting disallowTransferToParent=true, disallowTransferToPeers=true", a.Name),
		)
		a.DisallowTransferToParent = true
		a.DisallowTransferToPeers = true
	}

	if len(a.SubAgents()) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, subAgents must be empty to disable agent transfer", a.Name)
	}

	if len(a.AgentTools) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, tools must be empty", a.Name)
	}
	return nil
}

func (a *LlmAgent) validateGenerateContentConfig() error {
	if a.AgentGenerateContentConfig == nil {
		a.AgentGenerateContentConfig = &GenerateContentConfig{} // Ensure it's not nil
		return nil
	}
	if a.AgentGenerateContentConfig.ThinkingConfig != nil {
		return fmt.Errorf("thinkingConfig should be set via LlmAgent.Planner")
	}
	if len(a.AgentGenerateContentConfig.Tools) > 0 {
		return fmt.Errorf("all tools must be set via LlmAgent.Tools")
	}
	if a.AgentGenerateContentConfig.SystemInstruction != "" {
		return fmt.Errorf("system instruction must be set via LlmAgent.Instruction or LlmAgent.GlobalInstruction")
	}
	if a.AgentGenerateContentConfig.ResponseSchema != nil {
		return fmt.Errorf("response schema must be set via LlmAgent.OutputSchema")
	}
	return nil
}

// Helper, would be part of event logic
func eventIsFinalResponse(e *Event) bool {
	if e.Actions.StateDelta["skipSummarization"] == true { // Assuming skipSummarization is stored in StateDelta
		return true
	}
	// Simplified: In Python ADK, this checks for function calls/responses and partial flags.
	// Here, we'll assume an event is final if it's not marked partial and has content.
	return e.Content != nil && !e.Partial // Placeholder for e.Partial
}

// --- Placeholder Flow Implementations ---
// In a real Go ADK, these would be more fleshed out and likely in their own package.

type SingleFlow struct {
	// RequestProcessors  []BaseLlmRequestProcessor  // Placeholder
	// ResponseProcessors []BaseLlmResponseProcessor // Placeholder
}

func newSingleFlow() *SingleFlow {
	// Initialize processors
	return &SingleFlow{}
}

func (sf *SingleFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)
		llmReq := &LlmRequest{} // Initialize LlmRequest

		// Simulate preprocessing (would iterate through sf.RequestProcessors)
		if err := sf.preprocessAsync(invocationCtx, llmReq); err != nil {
			log.Printf("Error during preprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()} // Conceptual error event
			return
		}

		llmAgent, ok := invocationCtx.Agent.(*LlmAgent)
		if !ok {
			log.Printf("Error: agent is not an LlmAgent")
			return
		}
		llm, err := llmAgent.CanonicalModel()
		if err != nil {
			log.Printf("Error getting canonical model: %v", err)
			return
		}
		
		// Simplified LLM call for SingleFlow
		// A real implementation would handle streaming, function calling loops, etc.
		var llmResponse *LlmResponse

		// This is a simplification. The Python ADK has a loop for multiple LLM calls if tools are used.
		// For this Go conceptual translation, we'll assume a single LLM call or a very simplified tool use.
		if provider, ok := llm.(LLMProvider); ok {
			// Use GenerateContent for non-streaming for simplicity in this conceptual flow
			llmResponse, err = provider.GenerateContent(context.Background(), llmReq)
			if err != nil {
				log.Printf("Error calling LLM: %v", err)
				// outCh <- &Event{Error: err.Error()}
				return
			}
		} else {
			log.Printf("Error: LLM model does not implement LLMProvider interface")
			return
		}
		

		modelResponseEvent := &Event{
			ID:            newID(), // Generate new event ID
			InvocationID:  invocationCtx.InvocationID,
			Author:        invocationCtx.Agent.GetName(),
			Branch:        invocationCtx.Branch,
			Content:       llmResponse.Content, // Simplified
			// Actions, ErrorCode, ErrorMessage, etc. would be populated
		}

		// Simulate postprocessing (would iterate through sf.ResponseProcessors)
		if err := sf.postprocessAsync(invocationCtx, llmReq, llmResponse, modelResponseEvent, outCh); err != nil {
			log.Printf("Error during postprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()}
			return
		}
		
		// If not handled by post-processors (e.g. tool calls), yield the direct LLM response event
		if modelResponseEvent.Content != nil || modelResponseEvent.Actions.StateDelta != nil { // Yield if there's something to yield
			outCh <- modelResponseEvent
		}

	}()
	return outCh, nil
}


func (sf *SingleFlow) RunLive(invocationCtx *InvocationContext) (<-chan *Event, error) {
	// Simplified RunLive, a real one would be more complex with bidirectional streaming
	return sf.RunAsync(invocationCtx) // Fallback for now
}

func (sf *SingleFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	// Conceptual: Iterate through request_processors from Python ADK
	// Example: basic.request_processor, instructions.request_processor, etc.
	// For this Go version, we'll call helper methods directly for some core logic.
	agent := invocationCtx.Agent.(*LlmAgent) // Assuming LlmAgent

	// Basic processor logic
	modelInstance, err := agent.CanonicalModel()
	if err != nil {
		return err
	}
	llmReq.Model = modelInstance.ModelName // Assuming BaseLlm has ModelName
	if agent.AgentGenerateContentConfig != nil {
		cfgCopy := *agent.AgentGenerateContentConfig
		llmReq.Config = &cfgCopy
	} else {
		llmReq.Config = &GenerateContentConfig{}
	}
	if agent.OutputSchema != nil {
		llmReq.SetOutputSchema(agent.OutputSchema)
	}
	// ... (add live_connect_config population from RunConfig)

	// Instructions processor logic
	// Global Instruction
	rootAgent := agent.RootAgent().(*LlmAgent) // Assuming LlmAgent
	if rootAgent.AgentGlobalInstruction != nil {
		instr, _, err := rootAgent.CanonicalGlobalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		// Simplified: In Python, instructions_utils.inject_session_state is used.
		// For Go, this would involve a similar template processing.
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	// Agent Instruction
	if agent.AgentInstruction != nil {
		instr, _, err := agent.CanonicalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	
	// Identity processor
	var si []string
	si = append(si, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName()))
	if agent.GetDescription() != "" {
		si = append(si, fmt.Sprintf(" The description about you is \"%s\"", agent.GetDescription()))
	}
	llmReq.AppendInstructions(si)

	// Contents processor logic (simplified)
	if agent.IncludeContents != "none" {
		llmReq.Contents = getContentsForLlm(invocationCtx.Branch, invocationCtx.Session.Events, agent.GetName())
	}

	// Tool processing (adding declarations to llmReq.Config.Tools)
	tools, err := agent.CanonicalTools(&ReadonlyContext{InvocationContext: invocationCtx})
	if err != nil { return err}
	if len(tools) > 0 {
		if llmReq.Config.Tools == nil {
			llmReq.Config.Tools = make([]Tool, 0)
		}
	}
	for _, tool := range tools {
		// Simplified: directly appending, Python ADK has more complex logic for FunctionDeclaration
		llmReq.Config.Tools = append(llmReq.Config.Tools, tool) // This assumes tool itself is the declaration or can provide it
		
		// Store tool in llmReq.ToolsDict for later execution lookup by name
		if llmReq.ToolsDict == nil {
			llmReq.ToolsDict = make(map[string]Tool)
		}
		llmReq.ToolsDict[tool.GetName()] = tool
	}

	return nil
}

func (sf *SingleFlow) postprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest, llmResp *LlmResponse, modelResponseEvent *Event, outCh chan<- *Event) error {
	// Conceptual: Iterate through response_processors
	// Example: _nl_planning.response_processor, _code_execution.response_processor
	
	// Handle function calls (simplified from functions.py)
	if llmResp.Content != nil {
		var functionCalls []struct{ Name string; Args map[string]interface{}; ID string } // Simplified representation
		for _, part := range llmResp.Content.Parts {
			// Placeholder: In Python this uses part.function_call
			// For Go, we'd check a similar field if LlmResponse.Content.Parts had FunctionCall type
			// For this example, assume FunctionCall is identified by a specific structure in Part
			// For now, this logic is highly simplified and conceptual
			if strings.HasPrefix(part.Text, "CALL_TOOL:") { // Very basic trigger
				parts := strings.SplitN(strings.TrimPrefix(part.Text, "CALL_TOOL:"), "(", 2)
				name := parts[0]
				argsStr := ""
				if len(parts) > 1 {
					argsStr = strings.TrimSuffix(parts[1], ")")
				}
				var args map[string]interface{}
				json.Unmarshal([]byte(argsStr), &args) // Simplified arg parsing
				functionCalls = append(functionCalls, struct{ Name string; Args map[string]interface{}; ID string }{Name: name, Args: args, ID: newID()})

				// Populate client function call ID if LLM didn't provide one
				modelResponseEvent.Content.Parts[0].Text = "" // Clear text part conceptually
				// modelResponseEvent.Content.Parts[0].FunctionCall = &genai.FunctionCall{Name: name, Args: args, ID: ...}
			}
		}

		if len(functionCalls) > 0 {
			// In Python ADK, this calls functions.handle_function_calls_async
			// which then calls tool.run_async and creates a response event.
			// This is a very complex part involving callbacks, state updates, etc.
			// Here's a highly simplified conceptual flow:
			var toolResponseParts []Part
			for _, fc := range functionCalls {
				tool, ok := llmReq.ToolsDict[fc.Name]
				if !ok {
					log.Printf("Tool %s not found", fc.Name)
					toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Error: Tool %s not found", fc.Name)})
					continue
				}
				// Conceptual tool execution
				// toolResult, err := tool.Execute(context.Background(), fc.Args) // This is complex
				// Simplified:
				toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Tool %s called with %v (mocked)", fc.Name, fc.Args)})
			}
			
			// Create a new event for tool responses and send it
			toolResponseEvent := &Event{
				ID:            newID(),
				InvocationID:  invocationCtx.InvocationID,
				Author:        invocationCtx.Agent.GetName(),
				Branch:        invocationCtx.Branch,
				Content:       &Content{Role: "model", Parts: toolResponseParts}, // Simplified: role should be 'user' for function response in Gemini
			}
			outCh <- toolResponseEvent
			
			// The original LLM response event (modelResponseEvent) that *contained* the function call
			// is also yielded. Then, the flow would typically make *another* LLM call with the tool responses.
			// For simplicity, we will stop here in this conceptual snippet.
			// The python ADK has a loop in _run_one_step_async for this.
			modelResponseEvent.Content = nil // After tool call, the original content is usually superseded or followed by tool response processing
		}
	}
	
	return nil
}

type AutoFlow struct {
	SingleFlow
	// agentTransferRequestProcessor // Placeholder for agent_transfer.request_processor
}

func newAutoFlow() *AutoFlow {
	return &AutoFlow{SingleFlow: *newSingleFlow()}
}

func (af *AutoFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	if err := af.SingleFlow.preprocessAsync(invocationCtx, llmReq); err != nil {
		return err
	}
	// Agent transfer logic
	agent := invocationCtx.Agent.(*LlmAgent)
	transferTargets := getTransferTargets(agent)
	if len(transferTargets) > 0 {
		llmReq.AppendInstructions([]string{buildTargetAgentsInstructions(agent, transferTargets)})
		// Conceptually add transfer_to_agent tool
		// transferTool := NewFunctionTool(transferToAgent) // transferToAgent needs to be defined
		// ... add transferTool to llmReq.Config.Tools and llmReq.ToolsDict
	}
	return nil
}

// Helper to simulate Python's list comprehension and filtering for contents
func getContentsForLlm(currentBranch string, allEvents []*Event, agentName string) []Content {
	var contents []Content
	// This is a simplified version of Python ADK's _get_contents, which has complex logic
	// for rearranging events, handling async function responses, and filtering by branch.
	for _, event := range allEvents {
		if event.Content != nil && len(event.Content.Parts) > 0 {
			// Simplified branch check and foreign event conversion
			if isEventOnBranch(currentBranch, event) && !isAuthEvent(event) {
				if isOtherAgentReply(agentName, event) {
					contents = append(contents, *convertForeignEvent(event))
				} else {
					// Deep copy content before modifying (e.g. removing client func call ID)
					contentCopy := *event.Content 
					// removeClientFunctionCallID(&contentCopy) // Placeholder for actual logic
					contents = append(contents, contentCopy)
				}
			}
		}
	}
	return contents
}

// Simplified placeholders for helper functions from Python's contents.py
func isEventOnBranch(invocationBranch string, event *Event) bool { 
	if invocationBranch == "" || event.Branch == "" { return true }
	return strings.HasPrefix(invocationBranch, event.Branch)
}
func isAuthEvent(event *Event) bool { return false /* TODO */ }
func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return event.Author != currentAgentName && event.Author != "user"
}
func convertForeignEvent(event *Event) *Content { 
	// Simplified conversion
	return &Content{Role: "user", Parts: []Part{{Text: fmt.Sprintf("Context from %s: %s", event.Author, event.Content.Parts[0].Text)}}}
}
func newID() string { return "temp-id" } // Placeholder for ID generation

func getTransferTargets(agent *LlmAgent) []Agent {
    var targets []Agent
    targets = append(targets, agent.SubAgents()...) // Assuming SubAgents returns []Agent

    if agent.ParentAgent() != nil {
        if llmParent, ok := agent.ParentAgent().(*LlmAgent); ok {
            if !agent.DisallowTransferToParent {
                targets = append(targets, llmParent)
            }
            if !agent.DisallowTransferToPeers {
                for _, peerAgent := range llmParent.SubAgents() {
                    if peerAgent.GetName() != agent.GetName() {
                        targets = append(targets, peerAgent)
                    }
                }
            }
        }
    }
    return targets
}

func buildTargetAgentsInfo(targetAgent Agent) string {
    return fmt.Sprintf("\nAgent name: %s\nAgent description: %s\n", targetAgent.GetName(), targetAgent.GetDescription())
}

func buildTargetAgentsInstructions(agent *LlmAgent, targetAgents []Agent) string {
    var sb strings.Builder
    sb.WriteString("\nYou have a list of other agents to transfer to:\n")
    for _, target := range targetAgents {
        sb.WriteString(buildTargetAgentsInfo(target))
    }
    sb.WriteString("\nIf you are the best to answer the question according to your description, you can answer it.")
    sb.WriteString("\nIf another agent is better for answering the question according to its description, call `transfer_to_agent` function to transfer the question to that agent. When transferring, do not generate any text other than the function call.\n")

    if agent.ParentAgent() != nil {
        sb.WriteString(fmt.Sprintf("\nYour parent agent is %s. If neither the other agents nor you are best for answering the question according to the descriptions, transfer to your parent agent. If you don't have parent agent, try answer by yourself.\n", agent.ParentAgent().GetName()))
    }
    return sb.String()
}

// Placeholder for instruction template processing
func processInstructionTemplate(template string, ctx *ReadonlyContext) (string, error) {
	// In Python, this uses string.Formatter with a custom class to access state.
	// In Go, this would require a more explicit template engine or string replacement.
	// This is a very simplified version.
	processed := template
	// Regex to find {key} or {key?}
	re := regexp.MustCompile(`{([^}]+)}`)
	state := ctx.GetState() // Assuming GetState returns map[string]interface{}

	processed = re.ReplaceAllStringFunc(processed, func(match string) string {
		keyWithOptionalMarker := strings.Trim(match, "{} ")
		key := strings.TrimSuffix(keyWithOptionalMarker, "?")
		isOptional := strings.HasSuffix(keyWithOptionalMarker, "?")

		// Handle artifact.filename format
		if strings.HasPrefix(key, "artifact.") {
			// artifactName := strings.TrimPrefix(key, "artifact.")
			// artifactContent, err := loadArtifact(ctx.InvocationContext, artifactName) // Placeholder
			// if err != nil {
			// 	if isOptional { return "" }
			// 	log.Printf("Error loading artifact %s: %v", artifactName, err)
			// 	return match // return original on error
			// }
			// return artifactContent
			return "[ARTIFACT_CONTENT_PLACEHOLDER]" // Simplified
		}

		val, ok := state[key]
		if ok {
			return fmt.Sprintf("%v", val)
		}
		if isOptional {
			return ""
		}
		log.Printf("Warning: Context variable not found and not optional: `%s`", key)
		return match // Return original placeholder if not found and not optional
	})
	return processed, nil
}

# /Users/gopher/Desktop/workspace/adk-go/agents/interfaces/interfaces.go
package agents

import (
	"context"

	// Corrected import paths, assuming types are in their respective /types/ subdirectories
	// commontypes "github.com/KennethanCeyer/adk-go/common/types"
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	plannertypes "github.com/KennethanCeyer/adk-go/planners/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools/types"
	// InvocationContext and ReadonlyContext would be defined in adk-go/agents/invocation
	// invocation "github.com/KennethanCeyer/adk-go/agents/invocation"
)

// Placeholders for types from sub-packages or sibling packages to avoid initial import cycles.
// These should be replaced with fully qualified types once all packages are correctly defined.
type InvocationContextPlaceholder any // To be *invocation.InvocationContext
type ReadonlyContextPlaceholder any // To be *invocation.ReadonlyContext


type Agent interface {
	GetName() string
	GetDescription() string
}

type LlmAgent interface {
	Agent

	GetModelIdentifier() string
	GetSystemInstruction() string
	GetTools() []toolstypes.Tool
	GetAuthHandler() authtypes.AuthHandler
	GetPlanner() plannertypes.Planner
	GetGenerateContentConfig() *modelstypes.GenerateContentConfig
	GetOutputSchema() any
	GetIncludeContents() string
	IsRootAgent() bool
	GetParentAgent() LlmAgent
	RootAgent() LlmAgent

	PredictAsync(ctx context.Context, invocationCtx InvocationContextPlaceholder, request *modelstypes.LlmRequest) (<-chan *eventstypes.Event, error)

	CanonicalInstruction(ctx ReadonlyContextPlaceholder) (instruction string, bypassStateInjection bool, err error)
	CanonicalGlobalInstruction(ctx ReadonlyContextPlaceholder) (instruction string, bypassStateInjection bool, err error)
}

# /Users/gopher/Desktop/workspace/adk-go/models/types/types.go
package types

type Part struct {
	Text             *string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
}

type Content struct {
	Parts []Part
	Role  string // e.g., "user", "model"
}

type FunctionCall struct {
	Name string
	Args any // Typically map[string]any
}

type FunctionResponse struct {
	Name     string
	Response any // Typically map[string]any
}

type GenerateContentConfig struct {
	Temperature     *float32
	TopP            *float32
	TopK            *int32
	MaxOutputTokens *int32
	StopSequences   []string
	// SystemInstruction can be part of LlmRequest or set on the LLM client directly
}

type LlmRequest struct {
	ModelIdentifier string
	SystemInstruction *Content // Optional system instruction
	Contents        []Content // Conversation history + current message
	Config          *GenerateContentConfig
	// Tools are typically configured on the LLM client or passed to the GenerateContent call
}

type LlmResponse struct {
	Content *Content
	// Other fields like FinishReason, TokenCount can be added
}

# /Users/gopher/Desktop/workspace/adk-go/README.md
# ADK-Go: Agent Development Kit in Go (Runnable HelloWorld Example)

This repository provides a foundational, runnable "Hello World" agent example, demonstrating core Agent Development Kit (ADK) concepts ported from Python to Go. It adheres to the project structure `github.com/KennethanCeyer/adk-go`.

The primary goal of this example is to provide a working, minimal agent that interacts with a Large Language Model (LLM). More complex features and modules from the Python ADK can be incrementally built upon this base.

## Project Structure

```plaintext
adk-go
â”œâ”€â”€ common/
â”‚ â””â”€â”€ types/
â”‚ â””â”€â”€ types.go # Common, basic data types
â”œâ”€â”€ models/
â”‚ â””â”€â”€ types/
â”‚ â””â”€â”€ types.go # Types for LLM request/response (Content, Part, etc.)
â”œâ”€â”€ llmproviders/ # LLM interaction implementations
â”‚ â”œâ”€â”€ interfaces.go # LLMProvider interface
â”‚ â””â”€â”€ gemini.go # Google Gemini LLM provider
â”œâ”€â”€ tools/
â”‚ â”œâ”€â”€ types/
â”‚ â”‚ â””â”€â”€ types.go # Tool interface
â”‚ â””â”€â”€ example/ # Example tool implementations
â”‚ â””â”€â”€ echo_tool.go
â”œâ”€â”€ agents/
â”‚ â”œâ”€â”€ interfaces.go # Agent, LlmAgent interfaces
â”‚ â”œâ”€â”€ base_llm_agent.go # Base implementation for LlmAgent
â”‚ â””â”€â”€ invocation/
â”‚ â””â”€â”€ invocation.go # InvocationContext, RunConfig, ReadonlyContext
â”œâ”€â”€ examples/
â”‚ â””â”€â”€ helloworld/
â”‚ â””â”€â”€ agent.go # HelloWorld agent definition and initialization
â”œâ”€â”€ cmd/
â”‚ â””â”€â”€ helloworld_runner/
â”‚ â””â”€â”€ main.go # Executable for the HelloWorld agent
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ README.md
```

## Prerequisites

1.  **Go**: Version 1.20 or later. ([Installation Guide](https://go.dev/doc/install))
2.  **Google Cloud Project & API Key**:
    - A Google Cloud Project with the Generative Language API (for Gemini) enabled.
    - An API key for Google Gemini. Obtain this from [Google AI Studio](https://aistudio.google.com/app/apikey) or your Google Cloud console.
    - **Security**: Treat your API key as a secret. Do not commit it to your repository.
3.  **Environment Variable**: Set the `GEMINI_API_KEY` environment variable:
    ```bash
    export GEMINI_API_KEY="YOUR_API_KEY_HERE"
    ```

## Setup & Running the HelloWorld Agent

1.  **Create Project Directory:**

    ```bash
    mkdir -p adk-go
    cd adk-go
    ```

2.  **Initialize Go Module:**

    ```bash
    go mod init [github.com/KennethanCeyer/adk-go](https://github.com/KennethanCeyer/adk-go)
    ```

3.  **Install Dependencies:**
    This example uses Google's generative AI Go SDK for Gemini.

    ```bash
    go get [github.com/google/generative-ai-go/genai](https://github.com/google/generative-ai-go/genai)
    go get google.golang.org/api/option // Indirect dependency often needed
    ```

4.  **Create the Code Files:**
    Manually create the directory structure and files as listed in the "Project Structure" section above. Populate them with the Go code provided in the subsequent sections of this response. Ensure all file paths and package declarations are correct.

5.  **Set `GEMINI_API_KEY`**:
    Make sure the `GEMINI_API_KEY` environment variable is set in your terminal session before running.

6.  **Run the HelloWorld Agent:**
    From the root of your `adk-go` project directory:

    ```bash
    go run ./cmd/helloworld_runner/main.go
    ```

7.  **Interact with the Agent:**
    The runner will prompt you for input (`[user]: `). Try typing:
    - `Hello, world!`
    - `Use the echo tool to say: testing 123`
    - Type `exit` or `quit` to terminate.

# /Users/gopher/Desktop/workspace/adk-go/common/types/types.go
package types

type Timestamp float64

type InvocationID string

type SessionID string

type AgentID string

type ToolID string

type EventID string

type ErrorCode string

# /Users/gopher/Desktop/workspace/adk-go/sessions/types/types.go
package types

import (
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"
)

type State map[string]any

type Session struct {
	ID             commontypes.SessionID
	AppName        string
	UserID         string
	CreationTime   commontypes.Timestamp
	LastAccessTime commontypes.Timestamp
	State          State
	Events         []*eventstypes.Event `json:"-"`
	Metadata       map[string]string    `json:",omitempty"`
}

# /Users/gopher/Desktop/workspace/adk-go/examples/helloworld/agent.go
// File: github.com/KennethanCeyer/adk-go/examples/helloworld/agent.go
package helloworld

import (
	// Added for panic
	"log"

	"github.com/KennethanCeyer/adk-go/agents"
	llmproviders "github.com/KennethanCeyer/adk-go/llmproviders"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	exampletools "github.com/KennethanCeyer/adk-go/tools/example"
	toolstypes "github.com/KennethanCeyer/adk-go/tools/types"
)

var ConcreteLlmAgent agents.LlmAgent

func init() {
	geminiProvider, err := llmproviders.NewGeminiLLMProvider()
	if err != nil {
		log.Fatalf("Failed to create GeminiLLMProvider: %v. Ensure GEMINI_API_KEY is set.", err)
	}

	echoTool := exampletools.NewEchoTool()
	agentTools := []toolstypes.Tool{echoTool}

	systemInstructionText := "You are a helpful assistant named HelloWorldAgent. You can use tools provided to you. If asked to echo, use the echo_tool."
	systemInstruction := &modelstypes.Content{
		Parts: []modelstypes.Part{{Text: &systemInstructionText}},
	}

	// Ensure this model supports function calling effectively.
	// "gemini-1.5-flash-latest" is a good candidate. "gemini-pro" also works.
	modelName := "gemini-1.5-flash-latest"

	ConcreteLlmAgent = agents.NewBaseLlmAgent(
		"HelloWorldAgent",
		"A simple agent that can echo messages using a tool.",
		modelName,
		systemInstruction,
		geminiProvider,
		agentTools,
	)
	if ConcreteLlmAgent == nil { // Should not happen if NewBaseLlmAgent doesn't return nil on error
		panic("Failed to initialize HelloWorldAgent")
	}
	log.Println("HelloWorldAgent initialized successfully.")
}

# /Users/gopher/Desktop/workspace/adk-go/planners/types/types.go
package types

// Forward declarations for types. Replace with actual imports when types are fully defined.
type AnyLlmRequest any
type AnyReadonlyContext any
type AnyCallbackContext any
type AnyPart any

type Planner interface {
	Name() string
	BuildPlanningInstruction(readonlyCtx AnyReadonlyContext, llmReq AnyLlmRequest) (instruction string, err error)
	ProcessPlanningResponse(callbackCtx AnyCallbackContext, responseParts []AnyPart) (processedParts []AnyPart, err error)
	ApplyThinkingConfig(llmReq AnyLlmRequest)
}

# /Users/gopher/Desktop/workspace/adk-go/events/types/types.go
package types

import (
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
)

type EventActions struct {
	StateDelta             map[string]any
	RequestedAuthConfigs []authtypes.AuthConfigName
	EscalateTo             commontypes.AgentID
	TransferToAgent        commontypes.AgentID
	SkipSummarization      bool
	ProducedArtifacts      []string
	ConsumedArtifacts      []string
	CustomMetadata         map[string]any
	TurnComplete           bool
}

type Event struct {
	ID                commontypes.EventID
	InvocationID      commontypes.InvocationID
	SessionID         commontypes.SessionID
	ParentEventID     commontypes.EventID `json:",omitempty"`
	Timestamp         commontypes.Timestamp
	Author            string
	Role              string `json:",omitempty"`
	Content           *modelstypes.Content
	Actions           *EventActions `json:",omitempty"`
	Branch            commontypes.BranchID `json:",omitempty"`
	IsPartial         bool `json:",omitempty"`
	ErrorCode         commontypes.ErrorCode `json:",omitempty"`
	ErrorMessage      string `json:",omitempty"`
	LlmResponse       *modelstypes.LlmResponse `json:",omitempty"`
	ObservedLatencyMs int64 `json:",omitempty"`
}

func (e *Event) GetFunctionCalls() []modelstypes.FunctionCall {
	if e == nil || e.Content == nil {
		return nil
	}
	var calls []modelstypes.FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

func (e *Event) GetFunctionResponses() []modelstypes.FunctionResponse {
	if e == nil || e.Content == nil {
		return nil
	}
	var responses []modelstypes.FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/base_llm_flow.go
package llmflows

import (
	"fmt"
	"log"
	"reflect"
	"runtime"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/sessions"
	// "github.com/KennethanCeyer/adk-go/utils"
)

// Placeholder types from assumed packages (if not already fully defined in processors)
type InvocationContext = processors.InvocationContext
type LlmRequest = models.LlmRequest
type LlmResponse = models.LlmResponse
type Event = events.Event
type EventActions = events.EventActions
type LlmAgent = agents.LlmAgent // This should be the primary agent interface
type Content = models.Content   // Assuming models package defines Content, Part etc.
type Part = models.Part
type FunctionCall = models.FunctionCall
type FunctionResponse = models.FunctionResponse
type State = sessions.State
type RunConfig = agents.RunConfig // Assuming RunConfig is in agents

// BaseLlmFlow is the base for LLM-based flows.
type BaseLlmFlow struct {
	RequestProcessors  []processors.BaseLlmRequestProcessor
	ResponseProcessors []processors.BaseLlmResponseProcessor
}

// NewBaseLlmFlow creates a new BaseLlmFlow.
func NewBaseLlmFlow(
	requestProcessors []processors.BaseLlmRequestProcessor,
	responseProcessors []processors.BaseLlmResponseProcessor) *BaseLlmFlow {
	return &BaseLlmFlow{
		RequestProcessors:  requestProcessors,
		ResponseProcessors: responseProcessors,
	}
}

// RunAsync executes the LLM flow.
func (f *BaseLlmFlow) RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error) {
	outputEventCh := make(chan *Event)

	go func() {
		defer close(outputEventCh)

		llmReq := &LlmRequest{} // Initialize with appropriate defaults if any
		if llmAgent.GetGenerateContentConfig() != nil { // Assuming LlmAgent has GetGenerateContentConfig
			llmReq.Config = llmAgent.GetGenerateContentConfig()
		} else {
			llmReq.Config = &models.GenerateContentConfig{} // Ensure config is not nil
		}


		// 1. Pre-LLM processing
		for _, processor := range f.RequestProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name() // Get processor name for logging
			log.Printf("Running request processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmReq)
			if err != nil {
				log.Printf("Error running request processor %s: %v", processorName, err)
				outputEventCh <- &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in request processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				}
				return
			}
			// Drain events from processor if any (though these processors typically modify llmReq)
			for event := range eventCh {
				log.Printf("Event from request processor %s: %v", processorName, event.ID)
				outputEventCh <- event
			}
		}

		// 2. Call LLM
		// In Python ADK, this is `llm_agent.predict_async(llm_req, invocation_context=invocation_ctx)`
		// which internally calls the model connection.
		// Here, we assume LlmAgent has a method to interact with the LLM.
		log.Printf("Sending request to LLM for agent: %s, model: %s", llmAgent.GetName(), llmReq.Model)
		// For debugging: log.Printf("LLM Request Contents: %+v", llmReq.Contents)
		// For debugging: log.Printf("LLM Request System Instruction: %+v", llmReq.Config.SystemInstruction)


		llmRespCh, err := llmAgent.PredictAsync(invocationCtx, llmReq) // Assuming LlmAgent has PredictAsync
		if err != nil {
			log.Printf("Error initiating LLM prediction for agent %s: %v", llmAgent.GetName(), err)
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error calling LLM: %v", err)}}},
				ErrorCode:    "llm_call_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// Wait for LLM response (event containing LlmResponse)
		modelResponseEvent, ok := <-llmRespCh
		if !ok {
			log.Printf("LLM response channel closed unexpectedly for agent %s", llmAgent.GetName())
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: "LLM response channel closed unexpectedly"}}},
				ErrorCode:    "llm_response_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// The event from PredictAsync should contain the LlmResponse.
		// We need a way to extract it. Let's assume Event has an LlmResponse field or a method.
		var llmResponse *LlmResponse
		if modelResponseEvent.LlmResponse != nil { // Assuming Event has a direct LlmResponse field
			llmResponse = modelResponseEvent.LlmResponse
		} else {
			// Fallback: if LlmResponse is embedded in Content.Parts as a special Part type,
			// or if the event itself *is* the LlmResponse structure (less likely for an Event).
			// This part needs clarification based on how PredictAsync structures its output event.
			log.Printf("Warning: LlmResponse not directly found in modelResponseEvent for agent %s. Event: %+v", llmAgent.GetName(), modelResponseEvent)
			// For now, let's assume if LlmResponse is nil, the event content itself is the raw model output.
            // This means the response processors might need to handle raw Content directly.
            // To make it work with current ResponseProcessor interface, we'll construct a dummy LlmResponse if Content exists.
            if modelResponseEvent.Content != nil && llmResponse == nil {
                llmResponse = &LlmResponse{Content: modelResponseEvent.Content}
                 // Also ensure the event itself is passed, as it contains actions, etc.
            } else if llmResponse == nil {
                 log.Printf("Error: No LlmResponse or Content in modelResponseEvent for agent %s.", llmAgent.GetName())
                 outputEventCh <- &Event{
                    ID:           utils.NewEventID(),
                    InvocationID: invocationCtx.InvocationID,
                    Author:       llmAgent.GetName(),
                    Branch:       invocationCtx.Branch,
                    Content:      &Content{Parts: []Part{{Text: "Invalid or empty LLM response event"}}},
                    ErrorCode:    "llm_response_invalid",
                    Timestamp:    utils.GetTimestamp(),
                }
                return
            }
		}


		// 3. Post-LLM processing
		// The Python ADK iterates response_processors and then sends the modelResponseEvent.
		// If processors yield further events, those are sent too.
		// The original modelResponseEvent is sent *after* all processors complete.

		var accumulatedProcessorEvents []*Event

		for _, processor := range f.ResponseProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name()
			log.Printf("Running response processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmResponse, modelResponseEvent) // Pass the original event for context (e.g., actions)
			if err != nil {
				log.Printf("Error running response processor %s: %v", processorName, err)
				// Decide if this error is fatal or if we should continue with other processors
				// For now, accumulate and proceed, then send error event.
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in response processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				})
				continue // Or return if fatal
			}
			for procEvent := range eventCh {
				log.Printf("Event from response processor %s: %v", processorName, procEvent.ID)
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, procEvent)
			}
		}

		// Send the (potentially modified by processors) modelResponseEvent first
		outputEventCh <- modelResponseEvent

		// Then send any events generated by the response processors
		for _, procEvent := range accumulatedProcessorEvents {
			outputEventCh <- procEvent
		}

	}()

	return outputEventCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/contents.go
package processors

import (
	"fmt"
	"log"
	"sort"
	"strings"
	"time"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions" // For REQUEST_EUC_FUNCTION_CALL_NAME & removeClientFunctionCallID
)

// InvocationContext defines the context for a single invocation of an agent.
type InvocationContext struct {
	InvocationID    string
	Agent           LlmAgent // Assuming LlmAgent or a more general Agent interface
	Session         *Session // Assuming this contains Events and State
	Branch          string
	UserContent     *Content
	ArtifactService interface{} // Placeholder for actual ArtifactService type
	// ... other fields like SessionService, MemoryService, RunConfig, etc.
}

// LlmAgent represents an LLM-based agent.
// This is a simplified interface based on usage in the Python code.
type LlmAgent interface {
	GetName() string
	GetIncludeContents() string // "default", "none"
	// ... other methods like GetPlanner, RootAgent, CanonicalInstruction etc.
}

// Session represents a user session.
type Session struct {
	ID     string
	Events []*Event
	State  map[string]interface{}
	AppName string
	UserID  string
	// ... other session fields
}

// Event represents an event in the system.
type Event struct {
	ID            string
	InvocationID  string
	Author        string
	Content       *Content
	Actions       *EventActions
	Timestamp     float64
	Branch        string
	// ... other event fields like TurnComplete, Partial, ErrorCode, ErrorMessage, etc.
}

// GetFunctionCalls extracts function calls from the event's content parts.
func (e *Event) GetFunctionCalls() []FunctionCall {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var calls []FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

// GetFunctionResponses extracts function responses from the event's content parts.
func (e *Event) GetFunctionResponses() []FunctionResponse {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var responses []FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}


// Content represents the content of a message.
type Content struct {
	Role  string
	Parts []Part
}

// Part represents a part of a message's content.
type Part struct {
	Text             string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
	// ... other part types like InlineData, FileData, etc.
}

// FunctionCall represents a function call requested by the LLM.
type FunctionCall struct {
	ID   string
	Name string
	Args map[string]interface{}
}

// FunctionResponse represents the result of a function call.
type FunctionResponse struct {
	ID       string
	Name     string
	Response map[string]interface{}
}

// EventActions represents actions associated with an event.
type EventActions struct {
	StateDelta             map[string]interface{}
	RequestedAuthConfigs map[string]interface{} // Placeholder for actual AuthConfig type
	// ... other actions like SkipSummarization, TransferToAgent, Escalate
}


// LlmRequest represents a request to the LLM.
type LlmRequest struct {
	Model    string
	Contents []Content
	Config   *GenerateContentConfig // Assuming GenerateContentConfig is defined elsewhere
	// ... other fields like LiveConnectConfig, ToolsDict
}

// GenerateContentConfig holds configuration for LLM content generation.
type GenerateContentConfig struct {
	// Define fields based on google.genai.types.GenerateContentConfig
	// For example:
	// Temperature *float32
	// TopP *float32
	// TopK *int32
	// CandidateCount *int32
	// MaxOutputTokens *int32
	// StopSequences []string
	// SafetySettings []*SafetySetting
	// Tools []*Tool
	// SystemInstruction *Content
	// ResponseSchema interface{}
	// ResponseMimeType string
	// ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig
}


// Placeholder, assuming it's defined in the functions package or similar
const REQUEST_EUC_FUNCTION_CALL_NAME = "adk_request_credential"

// ContentLlmRequestProcessor builds the contents for the LLM request.
type ContentLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *ContentLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		if llmAgent.GetIncludeContents() != "none" { // Assuming LlmAgent has GetIncludeContents()
			if invocationCtx.Session == nil {
				log.Println("Error: InvocationContext.Session is nil")
				return
			}
			llmReq.Contents = getContents(
				invocationCtx.Branch,
				invocationCtx.Session.Events, // Assuming Session has Events []*Event
				llmAgent.GetName(),
			)
		}
		// This processor does not yield events.
	}()
	return outCh, nil
}

func rearrangeEventsForAsyncFunctionResponsesInHistory(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	functionCallIDToResponseEventsIndex := make(map[string]int)
	for i, event := range events {
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			for _, fr := range responses {
				functionCallIDToResponseEventsIndex[fr.ID] = i
			}
		}
	}

	var resultEvents []*Event
	processedResponseIndices := make(map[int]bool) // To avoid adding same response event multiple times

	for i, event := range events {
		if _, ok := processedResponseIndices[i]; ok && len(event.GetFunctionResponses()) > 0 { //
			// This response event was already merged with its call, skip it.
			continue //
		}

		if calls := event.GetFunctionCalls(); len(calls) > 0 { //
			resultEvents = append(resultEvents, event) // Add the function call event itself

			var responseEventsToMerge []*Event
			var indicesToMarkProcessed []int

			for _, fc := range calls {
				if responseIdx, ok := functionCallIDToResponseEventsIndex[fc.ID]; ok { //
					isAlreadyProcessedByAnotherCall := false
					for _, resEvent := range resultEvents {
						if resEvent == events[responseIdx] {
							isAlreadyProcessedByAnotherCall = true
							break
						}
						// Also check if the response event's content (parts) were merged into another event.
						// For simplicity, we rely on direct event object comparison or index tracking.
					}
					if !isAlreadyProcessedByAnotherCall && !processedResponseIndices[responseIdx] { //
						responseEventsToMerge = append(responseEventsToMerge, events[responseIdx])
						indicesToMarkProcessed = append(indicesToMarkProcessed, responseIdx)
					}
				}
			}

			if len(responseEventsToMerge) > 0 {
				sort.SliceStable(responseEventsToMerge, func(i, j int) bool { //
					var idxI, idxJ int = -1, -1
					for k, e := range events {
						if e == responseEventsToMerge[i] {
							idxI = k
						}
						if e == responseEventsToMerge[j] {
							idxJ = k
						}
						if idxI != -1 && idxJ != -1 {
							break
						}
					}
					return idxI < idxJ
				})
				mergedRespEvent := mergeFunctionResponseEvents(responseEventsToMerge)
				if mergedRespEvent != nil {
					resultEvents = append(resultEvents, mergedRespEvent)
					for _, idx := range indicesToMarkProcessed { //
						processedResponseIndices[idx] = true
					}
				}
			}
		} else {
			if !(len(event.GetFunctionResponses()) > 0 && processedResponseIndices[i]) { //
				resultEvents = append(resultEvents, event)
			}
		}
	}
	return resultEvents
}

func rearrangeEventsForLatestFunctionResponse(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	lastEvent := events[len(events)-1]
	responsesInLastEvent := lastEvent.GetFunctionResponses()
	if len(responsesInLastEvent) == 0 { //
		return events
	}

	responseIDs := make(map[string]bool)
	for _, fr := range responsesInLastEvent {
		responseIDs[fr.ID] = true
	}

	if len(events) >= 2 { //
		eventBeforeLast := events[len(events)-2]
		callsInEventBeforeLast := eventBeforeLast.GetFunctionCalls()
		allCallsMatched := true
		if len(callsInEventBeforeLast) == len(responseIDs) && len(callsInEventBeforeLast) > 0 { //
			for _, fc := range callsInEventBeforeLast {
				if !responseIDs[fc.ID] {
					allCallsMatched = false
					break
				}
			}
			if allCallsMatched {
				return events
			}
		}
	}

	functionCallEventIdx := -1 //
	for i := len(events) - 2; i >= 0; i-- { //
		event := events[i]
		calls := event.GetFunctionCalls()
		if len(calls) > 0 {
			foundMatch := false
			for _, fc := range calls {
				if responseIDs[fc.ID] {
					foundMatch = true
					for _, otherFc := range calls {
						responseIDs[otherFc.ID] = true
					}
					break
				}
			}
			if foundMatch {
				functionCallEventIdx = i
				break
			}
		}
	}

	if functionCallEventIdx == -1 { //
		log.Printf("Warning: No matching function call event found for function response IDs: %v", getKeys(responseIDs))
		return events //
	}

	var functionResponseEventsToMerge []*Event
	for i := functionCallEventIdx + 1; i < len(events)-1; i++ { //
		event := events[i]
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			isRelevantResponse := false
			for _, fr := range responses {
				if responseIDs[fr.ID] {
					isRelevantResponse = true
					break
				}
			}
			if isRelevantResponse {
				functionResponseEventsToMerge = append(functionResponseEventsToMerge, event)
			}
		}
	}
	functionResponseEventsToMerge = append(functionResponseEventsToMerge, lastEvent)

	resultEvents := make([]*Event, 0, functionCallEventIdx+2) //
	resultEvents = append(resultEvents, events[:functionCallEventIdx+1]...)
	if merged := mergeFunctionResponseEvents(functionResponseEventsToMerge); merged != nil { //
		resultEvents = append(resultEvents, merged)
	}

	return resultEvents
}

func getContents(currentBranch string, historyEvents []*Event, agentName string) []Content {
	var filteredEvents []*Event
	for _, event := range historyEvents {
		if event.Content == nil || len(event.Content.Parts) == 0 { //
			continue
		}
		// Go: we assume an empty text part is still a valid part unless explicitly filtered.
		// For now, let's assume empty text part implies no meaningful content for LLM.
		hasMeaningfulText := false //
		for _, p := range event.Content.Parts {
			if p.Text != "" {
				hasMeaningfulText = true
				break
			}
		}
		if !hasMeaningfulText && len(event.GetFunctionCalls()) == 0 && len(event.GetFunctionResponses()) == 0 { //
			continue
		}

		if !isEventOnBranch(currentBranch, event) {
			continue
		}
		if isAuthEvent(event) {
			continue
		}

		if isOtherAgentReply(agentName, event) {
			filteredEvents = append(filteredEvents, convertForeignEventToGo(event))
		} else {
			filteredEvents = append(filteredEvents, event)
		}
	}

	// Python calls _rearrange_events_for_latest_function_response first, then _rearrange_events_for_async_function_responses_in_history.
	resultEventsStage1 := rearrangeEventsForLatestFunctionResponse(filteredEvents)
	resultEventsStage2 := rearrangeEventsForAsyncFunctionResponsesInHistory(resultEventsStage1)

	var contents []Content
	for _, event := range resultEventsStage2 {
		if event.Content != nil {
			contentCopy := deepCopyContent(*event.Content)
			removeClientFunctionCallID(&contentCopy) // Placeholder
			contents = append(contents, contentCopy)
		}
	}
	return contents
}

func isEventOnBranch(currentBranch string, event *Event) bool {
	// Placeholder for logic to check if event is on the current branch
	// This depends on how Branch is structured and compared.
	if event == nil {
		return false
	}
	return event.Branch == "" || currentBranch == "" || strings.HasPrefix(event.Branch, currentBranch) || strings.HasPrefix(currentBranch, event.Branch)
}

func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return currentAgentName != "" && event.Author != currentAgentName && event.Author != "user"
}

func convertForeignEventToGo(event *Event) *Event {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return event
	}

	newContent := Content{Role: "user"} //
	newContent.Parts = append(newContent.Parts, Part{Text: "For context:"})

	for _, part := range event.Content.Parts {
		if part.Text != "" {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] said: %s", event.Author, part.Text)})
		} else if part.FunctionCall != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] called tool `%s` with parameters: %v", event.Author, part.FunctionCall.Name, part.FunctionCall.Args)})
		} else if part.FunctionResponse != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] `%s` tool returned result: %v", event.Author, part.FunctionResponse.Name, part.FunctionResponse.Response)})
		} else {
			// This part needs careful consideration for how to represent non-text/non-FC/non-FR parts.
			// For simplicity, appending as-is if possible, or a placeholder.
			// newContent.Parts = append(newContent.Parts, part) // This assumes Part is copyable
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] provided non-text data.", event.Author)}) //
		}
	}

	newEvent := *event // Shallow copy for metadata
	newEvent.Author = "user" //
	newEvent.Content = &newContent
	return &newEvent
}

func mergeFunctionResponseEvents(functionResponseEvents []*Event) *Event {
	if len(functionResponseEvents) == 0 {
		return nil
	}
	if len(functionResponseEvents) == 1 {
		return functionResponseEvents[0]
	}

	baseEvent := functionResponseEvents[0]
	// mergedParts := make([]Part, 0) // Not used directly in Go like Python

	// Map to store the latest response part for each function call ID
	latestResponsesForID := make(map[string]Part)
	var nonResponseParts []Part // To collect text parts or other non-FR parts in order

	for _, event := range functionResponseEvents {
		if event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil {
				latestResponsesForID[part.FunctionResponse.ID] = part
			} else {
				nonResponseParts = append(nonResponseParts, part)
			}
		}
	}

	// The Python code implicitly relies on the order of parts in the *first* event
	// if it contained multiple function responses, and then updates them.
	// For Go, if the first event had multiple FCs, its FRs would be the base.
	// If subsequent events provide FRs for *different* FC IDs not in the first event, they are appended.
	// This simplified version will collect all unique FRs based on their latest appearance.
	tempParts := make([]Part, 0, len(latestResponsesForID)+len(nonResponseParts)) //
	tempParts = append(tempParts, nonResponseParts...)                            //
	// To maintain order similar to Python (where original order of FCs in the *first* event matters,
	// and then other unique FRs are appended/merged), we need a more sophisticated merge.
	// For now, this collects unique FRs; their order relative to nonResponseParts and each other might differ from Python's.
	// A better approach would be to iterate through the function calls of the *triggering* event (if available)
	// and append responses in that order, or sort collected FRs by some original call order.

	// Example: Iterate through first event's FRs if they exist as a base order
	if baseEvent.Content != nil {
		for _, part := range baseEvent.Content.Parts {
			if part.FunctionResponse != nil {
				if respPart, ok := latestResponsesForID[part.FunctionResponse.ID]; ok {
					tempParts = append(tempParts, respPart)
					delete(latestResponsesForID, part.FunctionResponse.ID) // Mark as added
				}
			}
		}
	}
	// Append any remaining unique FRs (those not in the first event's call order)
	for _, part := range latestResponsesForID { // Order from map is not guaranteed.
		tempParts = append(tempParts, part)
	}

	mergedEvent := &Event{ //
		ID:           newEventID(), // Assuming newEventID() is defined
		InvocationID: baseEvent.InvocationID,
		Author:       baseEvent.Author,
		Branch:       baseEvent.Branch,
		Content:      &Content{Role: "user", Parts: tempParts}, // Gemini expects FRs to be role "user"
		Actions:      &EventActions{},                          // Initialize to avoid nil pointer if baseEvent.Actions is nil
		Timestamp:    baseEvent.Timestamp,                      // Or use latest timestamp
	}

	// Merge actions from all events (simplified)
	for _, event := range functionResponseEvents {
		if event.Actions != nil && event.Actions.StateDelta != nil { //
			if mergedEvent.Actions.StateDelta == nil {
				mergedEvent.Actions.StateDelta = make(map[string]interface{})
			}
			for k, v := range event.Actions.StateDelta {
				mergedEvent.Actions.StateDelta[k] = v
			}
		}
		// ... merge other actions like RequestedAuthConfigs, ArtifactDelta etc.
	}

	return mergedEvent
}

// newEventID generates a new unique ID for an event.
// This is a placeholder; a robust UUID generation should be used.
func newEventID() string {
	// In a real implementation, use something like github.com/google/uuid
	return fmt.Sprintf("evt_%d", time.Now().UnixNano())
}


func isAuthEvent(event *Event) bool {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return false
	}
	for _, part := range event.Content.Parts {
		if part.FunctionCall != nil && part.FunctionCall.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
		if part.FunctionResponse != nil && part.FunctionResponse.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
	}
	return false
}

// removeClientFunctionCallID placeholder
func removeClientFunctionCallID(content *Content) {
	// In Python, this removes IDs starting with AF_FUNCTION_CALL_ID_PREFIX
	// For Go, this would iterate through content.Parts and nil out IDs if they match a pattern.
	// For example:
	// const afFunctionCallIDPrefix = "adk-"
	// if content == nil || content.Parts == nil {
	// 	return
	// }
	// for i := range content.Parts {
	// 	if content.Parts[i].FunctionCall != nil && strings.HasPrefix(content.Parts[i].FunctionCall.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionCall.ID = "" // Or based on how Gemini handles empty vs nil
	// 	}
	// 	if content.Parts[i].FunctionResponse != nil && strings.HasPrefix(content.Parts[i].FunctionResponse.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionResponse.ID = ""
	// 	}
	// }
}

// deepCopyContent performs a deep copy of the Content struct.
func deepCopyContent(original Content) Content { //
	// A real deep copy is needed here. This is a placeholder.
	// For Go structs, manual copying or a library might be needed for true deep copies.
	copied := original //
	if original.Parts != nil {
		copied.Parts = make([]Part, len(original.Parts))
		for i, p := range original.Parts {
			// Deep copy each part. This is simplified.
			// If Part contains pointers or slices, those need deep copying too.
			copiedPart := p
			if p.FunctionCall != nil {
				fcCopy := *p.FunctionCall
				if p.FunctionCall.Args != nil {
					fcCopy.Args = deepCopyMap(p.FunctionCall.Args)
				}
				copiedPart.FunctionCall = &fcCopy
			}
			if p.FunctionResponse != nil {
				frCopy := *p.FunctionResponse
				if p.FunctionResponse.Response != nil {
					frCopy.Response = deepCopyMap(p.FunctionResponse.Response)
				}
				copiedPart.FunctionResponse = &frCopy
			}
			// Add deep copy for other Part fields if necessary
			copied.Parts[i] = copiedPart
		}
	}
	return copied
}

// deepCopyMap creates a deep copy of a map[string]interface{}.
// This is a simplified version; a robust deep copy library might be better for complex nested structures.
func deepCopyMap(originalMap map[string]interface{}) map[string]interface{} {
	if originalMap == nil {
		return nil
	}
	copyMap := make(map[string]interface{})
	for key, value := range originalMap {
		switch v := value.(type) {
		case map[string]interface{}:
			copyMap[key] = deepCopyMap(v)
		case []interface{}:
			copyMap[key] = deepCopySlice(v)
		default:
			copyMap[key] = v // Assumes primitive types are copyable by value
		}
	}
	return copyMap
}

// deepCopySlice creates a deep copy of a slice of interface{}.
func deepCopySlice(originalSlice []interface{}) []interface{} {
	if originalSlice == nil {
		return nil
	}
	copySlice := make([]interface{}, len(originalSlice))
	for i, value := range originalSlice {
		switch v := value.(type) {
		case map[string]interface{}:
			copySlice[i] = deepCopyMap(v)
		case []interface{}:
			copySlice[i] = deepCopySlice(v)
		default:
			copySlice[i] = v
		}
	}
	return copySlice
}


func getKeys(m map[string]bool) []string { //
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}

// Placeholder for LlmAgent.GetIncludeContents() method.
// This should be part of the LlmAgent interface/struct definition.
// func (a *LlmAgentStruct) GetIncludeContents() string { return a.IncludeContents }

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/instructions.go
package processors

import (
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/utils" // For instructions_utils
)

// ReadonlyContext provides a read-only view of the invocation context.
type ReadonlyContext struct { //
	InvocationContext *InvocationContext
	// Potentially other fields if ReadonlyContext offers more than just InvocationContext access
}


// InstructionsLlmRequestProcessor handles instructions and global instructions for LLM flow.
type InstructionsLlmRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *InstructionsLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in InstructionsLlmRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx} //

		// Append global instructions if set.
		// In Python ADK, global_instruction is on the root agent.
		// Assuming LlmAgent interface has a RootAgent() method that returns the root agent (LlmAgent type).
		var rootLlmAgent LlmAgent
		if ra := llmAgent.RootAgent(); ra != nil {
			var rOk bool
			rootLlmAgent, rOk = ra.(LlmAgent)
			if !rOk {
				log.Println("Warning: Root agent is not an LlmAgent, cannot process global instructions.")
			}
		}


		if rootLlmAgent != nil { //
			// Assuming LlmAgent interface has CanonicalGlobalInstruction
			globalInstructionStr, bypassGlobal, err := rootLlmAgent.CanonicalGlobalInstruction(readonlyCtx) //
			if err != nil {
				log.Printf("Error getting canonical global instruction: %v", err) //
			}
			if globalInstructionStr != "" {
				var processedGlobalInstr string
				if !bypassGlobal {
					processedGlobalInstr, err = injectSessionState(globalInstructionStr, readonlyCtx) //
					if err != nil {
						log.Printf("Error injecting state into global instruction: %v", err) //
						processedGlobalInstr = globalInstructionStr                               // Use raw on error
					}
				} else {
					processedGlobalInstr = globalInstructionStr
				}
				// Assuming LlmRequest has AppendInstructions
				if llmReq.Config == nil {
					llmReq.Config = &GenerateContentConfig{}
				}
				if llmReq.Config.SystemInstruction == nil {
					llmReq.Config.SystemInstruction = &Content{}
				}
				// This needs a proper AppendInstructions method on LlmRequest or its Config
				currentSysInstruction := ""
				if len(llmReq.Config.SystemInstruction.Parts) > 0 {
					currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
				}
				if currentSysInstruction != "" {
					currentSysInstruction += "\n\n"
				}
				llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedGlobalInstr}}

			}
		}

		// Append agent instructions if set.
		// Assuming LlmAgent interface has CanonicalInstruction
		agentInstructionStr, bypassAgent, err := llmAgent.CanonicalInstruction(readonlyCtx) //
		if err != nil {
			log.Printf("Error getting canonical agent instruction: %v", err) //
		}
		if agentInstructionStr != "" {
			var processedAgentInstr string
			if !bypassAgent {
				processedAgentInstr, err = injectSessionState(agentInstructionStr, readonlyCtx) //
				if err != nil {
					log.Printf("Error injecting state into agent instruction: %v", err) //
					processedAgentInstr = agentInstructionStr                             // Use raw on error
				}
			} else {
				processedAgentInstr = agentInstructionStr
			}
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedAgentInstr}}
		}
		// No events are yielded by this specific processor.
	}()
	return outCh, nil
}

// injectSessionState populates values in the instruction template.
// This is a Go equivalent of instructions_utils.inject_session_state.
func injectSessionState(template string, readonlyCtx *ReadonlyContext) (string, error) {
	if readonlyCtx == nil || readonlyCtx.InvocationContext == nil || readonlyCtx.InvocationContext.Session == nil {
		return template, fmt.Errorf("readonlyCtx or its internal fields are nil")
	}
	invocationCtx := readonlyCtx.InvocationContext //

	// Go's regexp package doesn't support direct async replacement functions.
	// This will be synchronous for now.
	// Pattern to find {var_name} or {var_name?} or {artifact.file_name}
	// For simplicity, a basic version is used here. A more robust parser might be needed.
	re := regexp.MustCompile(`{([^}]+)}`) //

	var replacementErr error

	result := re.ReplaceAllStringFunc(template, func(match string) string { //
		if replacementErr != nil {
			return match
		}

		varNameWithMarker := strings.Trim(match, "{} ")
		isOptional := strings.HasSuffix(varNameWithMarker, "?")
		varName := strings.TrimSuffix(varNameWithMarker, "?")

		if strings.HasPrefix(varName, "artifact.") { //
			artifactName := strings.TrimPrefix(varName, "artifact.")
			if invocationCtx.ArtifactService == nil { //
				replacementErr = fmt.Errorf("artifact service is not initialized")
				return match
			}
			// Conceptual: ArtifactService would need a synchronous LoadArtifact or this needs to be async.
			// For this snippet, assuming a conceptual synchronous load or pre-loaded artifact map.
			// This part is commented out as ArtifactService and its methods are not fully defined here.
			/*
				artifactPart, err := invocationCtx.ArtifactService.LoadArtifactSync( //
					invocationCtx.Session.AppName, // Assuming AppName field
					invocationCtx.Session.UserID,  // Assuming UserID field
					invocationCtx.Session.ID,      // Assuming ID field
					artifactName,
				)
				if err != nil {
					if isOptional { return "" }
					replacementErr = fmt.Errorf("artifact '%s' not found: %w", artifactName, err)
					return match
				}
				if artifactPart != nil {
					return artifactPart.Text // Assuming text artifact for simplicity
				}
			*/
			return fmt.Sprintf("[ARTIFACT:%s]", artifactName) // Placeholder
		}

		if !isValidStateName(varName) { //
			return match
		}

		if val, ok := invocationCtx.Session.State[varName]; ok { //
			return fmt.Sprintf("%v", val)
		}
		if isOptional { //
			return ""
		}
		replacementErr = fmt.Errorf("context variable not found: `%s`", varName) //
		return match
	})

	if replacementErr != nil {
		return "", replacementErr
	}
	return result, nil
}

// isValidStateName (simplified) checks if a variable name could be a state key.
func isValidStateName(varName string) bool {
	parts := strings.Split(varName, ":")
	if len(parts) == 1 {
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(varName) //
	}
	if len(parts) == 2 {
		// In Python ADK, prefixes are "app:", "user:", "temp:"
		// Simplified check for now.
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[0]) &&
			regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[1]) //
	}
	return false
}

// LlmAgent interface needs to be fully defined, including these methods for the above to compile:
// RootAgent() BaseAgent // Assuming BaseAgent is a common interface for all agents
// CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error)
// CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error)

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/base_llm_processors.go
package processors

// BaseLlmRequestProcessor defines the interface for processing LLM requests.
type BaseLlmRequestProcessor interface {
	RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error)
}

// BaseLlmResponseProcessor defines the interface for processing LLM responses.
type BaseLlmResponseProcessor interface {
	RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error)
}

// BaseLlmProcessor is a conceptual base for processors if common utility methods were needed.
// In Go, composition or helper functions are often preferred over deep inheritance hierarchies.
// For now, interfaces suffice.

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/basic.go
package processors

import (
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, etc.
)

// BaseLlm defines the interface for a Large Language Model.
type BaseLlm interface {
	ModelName() string
	// Potentially other methods like GenerateContent, Connect etc.
}

// MockLLMForFlow is a placeholder for testing, representing a specific LLM implementation.
type MockLLMForFlow struct {
	ModelName string
}

func (m *MockLLMForFlow) ModelName() string {
	return m.ModelName
}
// Ensure MockLLMForFlow implements BaseLlm
var _ BaseLlm = &MockLLMForFlow{}


// RunConfig defines configuration for an agent run.
type RunConfig struct {
	// Define fields based on google.adk.agents.run_config.RunConfig
	// Example fields (conceptual):
	// ResponseModalities []string
	// SpeechConfig interface{}
	// OutputAudioTranscription bool
	// InputAudioTranscription bool
	// MaxLlmCalls int
	// StreamingMode string
}


// LlmAgent interface part related to BasicRequestProcessor
type LlmAgentForBasicProcessor interface {
	LlmAgent // Embed the general LlmAgent interface
	CanonicalModel() (BaseLlm, error)
	GetGenerateContentConfig() *GenerateContentConfig
	GetOutputSchema() interface{}
}


// BasicRequestProcessor handles basic information to build the LLM request.
type BasicRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *BasicRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) { //
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in BasicRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgentForBasicProcessor) // Use the more specific interface
		if !ok {
			// If agent is not an LlmAgent, this processor might not apply,
			// or it might indicate an issue with the agent setup.
			// For now, we simply return without error.
			log.Println("Error: Agent in InvocationContext is not of type LlmAgentForBasicProcessor or compatible interface")
			return
		}

		canonicalModel, err := llmAgent.CanonicalModel() //
		if err != nil {
			log.Printf("Error getting canonical model for agent %s: %v", llmAgent.GetName(), err) //
			return
		}

		if canonicalModel != nil { //
			llmReq.Model = canonicalModel.ModelName() //
		} else {
			log.Printf("Warning: CanonicalModel is nil for agent %s, model name might not be set correctly in LlmRequest.", llmAgent.GetName()) //
		}


		if cfg := llmAgent.GetGenerateContentConfig(); cfg != nil { //
			cfgCopy := *cfg // Shallow copy
			llmReq.Config = &cfgCopy //
		} else {
			llmReq.Config = &GenerateContentConfig{} // Ensure config is not nil
		}

		if outputSchema := llmAgent.GetOutputSchema(); outputSchema != nil { //
			// Assuming LlmRequest has a SetOutputSchema method
			// llmReq.SetOutputSchema(outputSchema) // This depends on LlmRequest's definition
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			llmReq.Config.ResponseSchema = outputSchema
			llmReq.Config.ResponseMimeType = "application/json" // Typically for structured output

		}

		if invocationCtx.RunConfig != nil { //
			// Conceptual assignments - actual fields depend on RunConfig and LlmRequest.LiveConnectConfig definitions
			// if llmReq.LiveConnectConfig == nil { llmReq.LiveConnectConfig = &LiveConnectConfig{} }
			// llmReq.LiveConnectConfig.ResponseModalities = invocationCtx.RunConfig.ResponseModalities
			// llmReq.LiveConnectConfig.SpeechConfig = invocationCtx.RunConfig.SpeechConfig
			// llmReq.LiveConnectConfig.OutputAudioTranscription = invocationCtx.RunConfig.OutputAudioTranscription
			// llmReq.LiveConnectConfig.InputAudioTranscription = invocationCtx.RunConfig.InputAudioTranscription
			// These would be direct assignments if LiveConnectConfig fields match RunConfig fields.
		}

		// No events are yielded by this specific processor, it only modifies llmReq.
	}()
	return outCh, nil
}

// These GetGenerateContentConfig and GetOutputSchema methods should be part of the LlmAgent implementation.
// Example implementation for a hypothetical LlmAgentStruct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentGenerateContentConfig *GenerateContentConfig
    OutputSchema interface{}
}

func (a *LlmAgentStruct) GetGenerateContentConfig() *GenerateContentConfig { //
    return a.AgentGenerateContentConfig
}

func (a *LlmAgentStruct) GetOutputSchema() interface{} { //
    return a.OutputSchema
}
*/

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/identity.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
)

// IdentityLlmRequestProcessor gives the agent identity from the framework.
type IdentityLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *IdentityLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in IdentityLlmRequestProcessor")
			return
		}
		agent := invocationCtx.Agent // Assuming Agent interface has GetName() and GetDescription()

		var instructions []string
		instructions = append(instructions, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName())) //
		if desc := agent.GetDescription(); desc != "" { //
			instructions = append(instructions, fmt.Sprintf(" The description about you is \"%s\"", desc)) //
		}

		// Assuming LlmRequest has AppendInstructions method or similar mechanism
		if llmReq.Config == nil {
			llmReq.Config = &GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &Content{}
		}
		currentSysInstruction := ""
		if len(llmReq.Config.SystemInstruction.Parts) > 0 {
			currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
		}

		addedInstruction := strings.Join(instructions, "")
		if currentSysInstruction != "" {
			currentSysInstruction += "\n\n" // Ensure separation if there's existing instruction
		}
		llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + addedInstruction}}
		// This processor does not yield events, it modifies llmReq.
	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/nlplanning.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/planners" // For PlanReActPlanner, BuiltInPlanner
	// "github.com/google/generative-ai-go/genai" // For genai.Part, genai.ThinkingConfig
)

// Planner defines the interface for agent planners.
type Planner interface { // [cite: 2133]
	BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) // [cite: 2133]
	ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) // [cite: 2133]
	// ApplyThinkingConfig is specific to BuiltInPlanner in Python, might be part of a more specific interface in Go
	ApplyThinkingConfig(llmReq *LlmRequest)
}

// BuiltInPlanner is a placeholder for planners.BuiltInPlanner [cite: 2133]
type BuiltInPlanner struct {
	ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig [cite: 2133]
}

// ApplyThinkingConfig applies the thinking configuration to the LLM request.
func (bip *BuiltInPlanner) ApplyThinkingConfig(llmReq *LlmRequest) { // [cite: 2133]
	if bip.ThinkingConfig != nil && llmReq.Config != nil {
		// llmReq.Config.ThinkingConfig = bip.ThinkingConfig // This depends on GenerateContentConfig definition
		// For example, if GenerateContentConfig has a field `ThinkingConfig interface{}`
		// then the above line would work.
		log.Println("BuiltInPlanner.ApplyThinkingConfig: ThinkingConfig application is conceptual.")
	}
}
// BuildPlanningInstruction for BuiltInPlanner.
func (bip *BuiltInPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) { // [cite: 2133]
	return "", nil // Python version returns None, so empty string and no error for Go
}
// ProcessPlanningResponse for BuiltInPlanner.
func (bip *BuiltInPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) { // [cite: 2133, 2134]
	return responseParts, nil
}

// PlanReActPlanner is a placeholder for planners.PlanReActPlanner [cite: 2134]
type PlanReActPlanner struct{}

// Constants for PlanReActPlanner tags [cite: 2134]
const (
	PlanningTag    = "/*PLANNING*/"    // [cite: 2134]
	RePlanningTag  = "/*REPLANNING*/"  // [cite: 2134]
	ReasoningTag   = "/*REASONING*/"   // [cite: 2134]
	ActionTag      = "/*ACTION*/"      // [cite: 2134]
	FinalAnswerTag = "/*FINAL_ANSWER*/" // [cite: 2134]
)

// BuildPlanningInstruction for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) {
	// This directly translates the Python _build_nl_planner_instruction method [cite: 2134]
	highLevelPreamble := fmt.Sprintf(` When answering the question, try to leverage the available tools to gather the information instead of your memorized knowledge. [cite: 2134, 2135]
Follow this process when answering the question: (1) first come up with a plan in natural language text format; [cite: 2135, 2136]
(2) Then use tools to execute the plan and provide reasoning between tool code snippets to make a summary of current state and next step. [cite: 2136, 2137]
Tool code snippets and reasoning should be interleaved with each other. (3) In the end, return one final answer. [cite: 2137]
Follow this format when answering the question: (1) The planning part should be under %s. [cite: 2138]
(2) The tool code snippets should be under %s, and the reasoning parts should be under %s. [cite: 2139]
(3) The final answer part should be under %s. `, PlanningTag, ActionTag, ReasoningTag, FinalAnswerTag) // [cite: 2139, 2140]

	planningPreamble := fmt.Sprintf(` Below are the requirements for the planning: The plan is made to answer the user query if following the plan. The plan is coherent and covers all aspects of information from user query, and only involves the tools that are accessible by the agent. The plan contains the decomposed steps as a numbered list where each step should use one or multiple available tools. By reading the plan, you can intuitively know which tools to trigger or what actions to take. [cite: 2140, 2141]
If the initial plan cannot be successfully executed, you should learn from previous execution results and revise your plan. The revised plan should be be under %s. Then use tools to follow the new plan. `, RePlanningTag) // [cite: 2141]

	// The Python original has reasoning_preamble, final_answer_preamble, tool_code_without_python_libraries_preamble, user_input_preamble
	// These would be similarly defined and concatenated. For brevity, I'm showing the pattern.
	// Assuming those other preamble parts are defined similarly:
	reasoningPreamble := ` Below are the requirements for the reasoning: The reasoning makes a summary of the current trajectory based on the user query and tool outputs. Based on the tool outputs and plan, the reasoning also comes up with instructions to the next steps, making the trajectory closer to the final answer. `
	finalAnswerPreamble := ` Below are the requirements for the final answer: The final answer should be precise and follow query formatting requirements. Some queries may not be answerable with the available tools and information. In those cases, inform the user why you cannot process their query and ask for more information. `
	toolCodeWithoutPythonLibrariesPreamble := ` Below are the requirements for the tool code: **Custom Tools:** The available tools are described in the context and can be directly used. - Code must be valid self-contained Python snippets with no imports and no references to tools or Python libraries that are not in the context. - You cannot use any parameters or fields that are not explicitly defined in the APIs in the context. - The code snippets should be readable, efficient, and directly relevant to the user query and reasoning steps. - When using the tools, you should use the library name together with the function name, e.g., vertex_search.search(). - If Python libraries are not provided in the context, NEVER write your own code other than the function calls using the provided tools. `
	userInputPreamble := ` VERY IMPORTANT instruction that you MUST follow in addition to the above instructions: You should ask for clarification if you need more information to answer the question. You should prefer using the information available in the context instead of repeated tool use. `

	return strings.Join([]string{
		highLevelPreamble,
		planningPreamble,
		reasoningPreamble,
		finalAnswerPreamble,
		toolCodeWithoutPythonLibrariesPreamble,
		userInputPreamble,
	}, "\n\n"), nil
}

// markAsThought marks a response part as a thought.
// In Go, `Part` doesn't have a direct `Thought` field like in Python's genai types.
// This might be handled by a wrapper struct or by convention (e.g., specific role or metadata).
// For this conceptual translation, we'll assume a way to mark it, or it's implicit in the planner's logic.
func (prp *PlanReActPlanner) markAsThought(part *Part) {
	// Conceptual: If Part struct had a Thought field: part.Thought = true
	// Or, this could modify the Part's role or add metadata if the Go ADK defines such conventions.
	// For now, this is a no-op unless Part struct is extended.
	log.Printf("Marking part as thought (conceptual): %v", part.Text)
}


// ProcessPlanningResponse for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) {
	if responseParts == nil { // [cite: 2134]
		return nil, nil
	}

	var preservedParts []Part
	firstFCPartIndex := -1

	for i, part := range responseParts {
		if part.FunctionCall != nil { // [cite: 2134]
			if part.FunctionCall.Name == "" {
				continue
			}
			preservedParts = append(preservedParts, part)
			if firstFCPartIndex == -1 {
				firstFCPartIndex = i
			}
		} else {
			// Handle non-function call parts based on tags
			if part.Text != "" {
				// Split by FINAL_ANSWER_TAG first
				if strings.Contains(part.Text, FINAL_ANSWER_TAG) {
					splits := strings.SplitN(part.Text, FINAL_ANSWER_TAG, 2)
					reasoningText := strings.TrimSpace(splits[0])
					finalAnswerText := ""
					if len(splits) > 1 {
						finalAnswerText = strings.TrimSpace(splits[1])
					}

					if reasoningText != "" {
						reasoningPart := Part{Text: reasoningText}
						prp.markAsThought(&reasoningPart) // Conceptual
						preservedParts = append(preservedParts, reasoningPart)
					}
					if finalAnswerText != "" {
						preservedParts = append(preservedParts, Part{Text: finalAnswerText})
					}
				} else {
					// If no FINAL_ANSWER_TAG, check for other thought tags
					isThought := false
					for _, tag := range []string{PlanningTag, ReasoningTag, ActionTag, RePlanningTag} {
						if strings.HasPrefix(part.Text, tag) {
							isThought = true
							break
						}
					}
					currentPart := part
					if isThought {
						prp.markAsThought(&currentPart) // Conceptual
					}
					preservedParts = append(preservedParts, currentPart)
				}
			}
		}
	}
	return preservedParts, nil
}

// ApplyThinkingConfig for PlanReActPlanner (not directly applicable as it doesn't use genai.ThinkingConfig)
func (prp *PlanReActPlanner) ApplyThinkingConfig(llmReq *LlmRequest) {
	// PlanReActPlanner builds instructions directly, doesn't use genai.ThinkingConfig
}


// LlmAgent interface needs to be fully defined for the below to be valid outside this file.
// For example, within an LlmAgent struct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentPlanner Planner
}

func (a *LlmAgentStruct) GetPlanner() Planner {
    return a.AgentPlanner
}
*/

// NlPlanningRequestProcessor handles NL planning for LLM flow.
type NlPlanningRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *NlPlanningRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in NlPlanningRequestProcessor")
			return
		}

		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}
		// Python has: if isinstance(planner, BuiltInPlanner): planner.apply_thinking_config(llm_req)
		// In Go, this would typically be handled by type assertion or checking an interface method.
		if bp, ok := planner.(*BuiltInPlanner); ok { // [cite: 2133]
			bp.ApplyThinkingConfig(llmReq) // [cite: 2133]
		}


		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx}
		instruction, err := planner.BuildPlanningInstruction(readonlyCtx, llmReq) // [cite: 2133]
		if err != nil {
			log.Printf("Error building planning instruction: %v", err)
			// Optionally send an error event or handle error
			return
		}
		if instruction != "" {
			// Assuming LlmRequest has AppendInstructions
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + instruction}}
		}
	}()
	return outCh, nil
}

// NlPlanningResponseProcessor processes LLM responses for NL planning.
type NlPlanningResponseProcessor struct{}

// RunAsync implements the BaseLlmResponseProcessor interface (conceptual).
func (p *NlPlanningResponseProcessor) RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || llmResp == nil || llmResp.Content == nil {
			log.Println("Error: InvocationContext, Agent, or LlmResponse content is nil in NlPlanningResponseProcessor")
			return
		}
		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}

		callbackCtx := &CallbackContext{InvocationContext: invocationCtx, EventActions: modelResponseEvent.Actions} // [cite: 2133]
		if callbackCtx.EventActions == nil { // Ensure EventActions is not nil
		    callbackCtx.EventActions = &EventActions{}
		}


		processedParts, err := planner.ProcessPlanningResponse(callbackCtx, llmResp.Content.Parts) // [cite: 2134]
		if err != nil {
			log.Printf("Error processing planning response: %v", err)
			// Optionally send an error event or handle error
			return
		}

		if processedParts != nil {
			llmResp.Content.Parts = processedParts
		}

		// If callbackCtx.State has changed (meaning planner updated state via EventActions)
		// and an event needs to be yielded. In Python, this is handled by the framework.
		// Here, we might need to explicitly yield an event if the state delta implies one.
		// However, the processor's role is usually to modify the llmResp or yield new events based on llmResp processing.
		// If state changes in callbackCtx are meant to produce a new event, that logic would go here.
		// For now, this processor primarily modifies the llmResp. If an event with updated actions
		// is needed, it would be constructed and sent to outCh.
		// This part of the Python code doesn't explicitly yield an Event from this processor.

	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/auto_flow.go
package llmflows

import (
	"fmt"
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
)

// AutoFlow dynamically selects and runs the appropriate flow for an agent.
// In Go, the dynamic creation of the LlmAgent itself as done in Python's AutoFlow
// (creating an LlmAgent on the fly with specific tools/planners based on config)
// is less straightforward due to static typing.
// Typically, the LlmAgent would already be constructed with its configuration.
// This AutoFlow equivalent will focus on selecting the flow and running it.
type AutoFlow struct {
	// Configuration for AutoFlow, if any, can go here.
	// For example, a map of agent types to flow constructors.
}

// NewAutoFlow creates a new AutoFlow.
func NewAutoFlow() *AutoFlow {
	return &AutoFlow{}
}

// RunAsync executes the appropriate flow for the given agent.
func (af *AutoFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	if invocationCtx == nil || invocationCtx.Agent == nil {
		err := fmt.Errorf("AutoFlow: InvocationContext or Agent is nil")
		log.Println(err)
		// Return a channel that immediately closes or sends an error event
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	agent := invocationCtx.Agent // Agent from InvocationContext

	// The Python AutoFlow can dynamically create an LlmAgent if the provided agent
	// is just a configuration. In Go, we'd expect `agent` to already be a runnable LlmAgent.
	// If `agent` is a configuration struct, a factory would be needed here to build the LlmAgent.

	llmAgent, ok := agent.(LlmAgent) // Assuming the agent in context is already an LlmAgent
	if !ok {
		err := fmt.Errorf("AutoFlow: agent in InvocationContext is not an LlmAgent, but %T", agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	// --- Flow Selection Logic ---
	// In Python, this checks agent.flow or agent.tools, agent.planner etc.
	// For Go, we'll assume a simple logic: if the agent is an LlmAgent, use SingleFlow.
	// More complex logic could be based on LlmAgent's properties or specific type.

	var selectedFlow interface { // Using interface{} to represent a generic Flow type
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	}

	// Example: If LlmAgent has a GetFlowType() method or similar configuration.
	// flowType := llmAgent.GetFlowType() // Hypothetical method
	// switch flowType {
	// case "single":
	//  selectedFlow = NewSingleFlow()
	// case "sequential":
	//  selectedFlow = NewSequentialFlow() // If SequentialFlow exists
	// default:
	//  selectedFlow = NewSingleFlow() // Default
	// }

	// For now, directly use SingleFlow if it's an LlmAgent
	if _, isLlmAgent := agent.(LlmAgent); isLlmAgent {
		log.Printf("AutoFlow: Using SingleFlow for LlmAgent: %s", llmAgent.GetName())
		selectedFlow = NewSingleFlow()
	} else {
		// Handle other agent types if necessary, or default
		err := fmt.Errorf("AutoFlow: Unhandled agent type %T for flow selection. Defaulting to SingleFlow if possible or erroring.", agent)
		log.Println(err)
        // Attempt to use SingleFlow if the base agent can be cast.
        // This part is tricky without knowing all agent types.
        // For robustnes, it's better to ensure llmAgent is correctly typed above.
		log.Println("AutoFlow: Defaulting to SingleFlow (or error if not LlmAgent).")
		selectedFlow = NewSingleFlow() // This will run if the initial cast to LlmAgent succeeded.
	}


	if selectedFlow == nil {
		err := fmt.Errorf("AutoFlow: Could not determine a flow for agent: %s (%T)", llmAgent.GetName(), agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}

	// Type assert selectedFlow to the expected Flow interface/type to call RunAsync
	// This assumes all flows (SingleFlow, etc.) implement this method signature.
	concreteFlow, flowOk := selectedFlow.(interface {
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	})
	if !flowOk {
		err := fmt.Errorf("AutoFlow: Selected flow does not implement the required RunAsync method signature.")
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}


	log.Printf("AutoFlow: Running flow for agent: %s", llmAgent.GetName())
	return concreteFlow.RunAsync(invocationCtx, llmAgent)
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/single_flow.go
package llmflows

import (
	"github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
)

// SingleFlow represents a simple LLM flow with a standard set of processors.
type SingleFlow struct {
	*BaseLlmFlow
}

// NewSingleFlow creates a new SingleFlow.
func NewSingleFlow() *SingleFlow {
	requestProcessors := []processors.BaseLlmRequestProcessor{
		&processors.BasicRequestProcessor{},
		&processors.IdentityLlmRequestProcessor{},
		&processors.ToolLlmRequestProcessor{}, // Assuming ToolLlmRequestProcessor is defined
		&processors.NlPlanningRequestProcessor{},
		&processors.InstructionsLlmRequestProcessor{},
		&processors.ContentLlmRequestProcessor{},
		&processors.CodeExecutionRequestProcessor{}, // Assuming CodeExecutionRequestProcessor is defined
	}

	responseProcessors := []processors.BaseLlmResponseProcessor{
		&processors.NlPlanningResponseProcessor{},
		&processors.FunctionCallResponseProcessor{}, // Assuming FunctionCallResponseProcessor is defined
		&processors.CodeExecutionResponseProcessor{},// Assuming CodeExecutionResponseProcessor is defined
		&processors.ToolUsageResponseProcessor{},    // Assuming ToolUsageResponseProcessor is defined
	}

	baseFlow := NewBaseLlmFlow(requestProcessors, responseProcessors)
	return &SingleFlow{BaseLlmFlow: baseFlow}
}

// Ensure SingleFlow implements an expected Flow interface if one exists
// type Flow interface {
//    RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error)
// }
// var _ Flow = &SingleFlow{}

# /Users/gopher/Desktop/workspace/adk-go/golang_code.txt
# /Users/gopher/Desktop/workspace/adk-go/tools/types/types.go
package types

import (
	"context"
)

type Tool interface {
	Name() string
	Description() string
	Parameters() any // e.g., map[string]any for JSON schema
	Execute(ctx context.Context, args any) (result any, err error)
}

# /Users/gopher/Desktop/workspace/adk-go/auth/types/types.go
package types

type AuthConfigName string

type AuthConfig struct {
	Name         AuthConfigName
	AuthScheme   string
	Scope        string
	ClientID     string
	ClientSecret string
	// Other scheme-specific fields (e.g., auth_url, token_url)
}

type AuthCredential struct {
	Name         AuthConfigName
	AccessToken  string
	RefreshToken string
	ExpiresIn    int64 // Seconds
	ObtainedAt   int64 // Unix timestamp
	Scope        string
	IDToken      string
}

func (ac *AuthCredential) IsExpired() bool {
	if ac.AccessToken == "" || ac.ExpiresIn <= 0 || ac.ObtainedAt <= 0 {
		return true
	}
	// Needs actual time checking logic, e.g. using time.Now().Unix()
	// return time.Now().Unix() >= (ac.ObtainedAt + ac.ExpiresIn - 60) // 60s buffer
	return false // Placeholder
}

// AuthHandler interface (can also be in auth/interfaces/interfaces.go)
// To avoid too many "types" packages, interfaces can live with their functional code or in a dedicated interfaces sub-package.
// For now, keeping it here for simplicity of this correction step.
// If AuthHandler needs to be used by other packages that auth might import,
// it would need to be in a more common or dedicated interface package.
type AuthHandler interface {
	GetAllAuthConfigs() []AuthConfig
	GetAuthConfig(name AuthConfigName) (*AuthConfig, error)
	// Session type will be from github.com/KennethanCeyer/adk-go/sessions/types
	GetCredential(name AuthConfigName, session any) (AuthCredential, error)
	// IsToolAuthorized(toolName string, authConfigsUsedByTool []AuthConfigName, session any) (bool, error)
	// GetAuthTool() any // Would return a toolstypes.Tool
}

# /Users/gopher/Desktop/workspace/adk-go/auth/auth_preprocessor.go
// File: github.com/KennethanCeyer/adk-go/auth/auth_preprocessor.go
package auth

import (
	"fmt"
	"log"
	"strings"
	"time"

	agentiface "github.com/KennethanCeyer/adk-go/agents"
	"github.com/KennethanCeyer/adk-go/agents/invocation"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"

	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions"
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

const defaultAuthInstructionTemplate = `
You have access to a set of tools that may require user authorization.
If you need to use a tool that requires authorization, you must first request authorization by calling the "%s" function.
The function call will return one of the following:
- A success message: "Successfully obtained credential."
- An error message: "Failed to obtain credential. <reason>"
- A message indicating that user interaction is required: "User interaction is required to obtain credential. <auth_url>"
Only if the function call returns a success message, you can then proceed to use the tool that requires authorization.
Otherwise, you must inform the user about the error or the required user interaction.

Current user:
<User>
%s
</User>
`

type AuthLlmRequestProcessor struct{}

// BaseLlmRequestProcessor interface (should be defined elsewhere, e.g., flows/llmflows/processors/interfaces.go)
type BaseLlmRequestProcessor interface {
	RunAsync(invocationCtx *invocation.InvocationContext, llmReq *modelstypes.LlmRequest) (<-chan *eventstypes.Event, error)
}

var _ BaseLlmRequestProcessor = (*AuthLlmRequestProcessor)(nil)

func (p *AuthLlmRequestProcessor) RunAsync(
	invocationCtx *invocation.InvocationContext,
	llmReq *modelstypes.LlmRequest,
) (<-chan *eventstypes.Event, error) {
	outCh := make(chan *eventstypes.Event)

	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || invocationCtx.Session == nil {
			log.Println("AuthLlmRequestProcessor: InvocationContext, Agent, or Session is nil.")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(agentiface.LlmAgent)
		if !ok {
			// invocationCtx.Agent is agentiface.Agent. We need to check if it's also an LlmAgent.
			// This might require a type assertion from the specific AgentInterface used in InvocationContext.
			// For now, assuming InvocationContext.Agent can be directly asserted if it's set with an LlmAgent.
			log.Printf("AuthLlmRequestProcessor: Agent in InvocationContext is not an LlmAgent.")
			return
		}

		authHandler := llmAgent.GetAuthHandler()
		if authHandler == nil {
			return
		}

		authConfigs := authHandler.GetAllAuthConfigs()
		if len(authConfigs) == 0 {
			return
		}

		// Cast session from any to *sessionstypes.Session for GetCredential
		// This relies on AuthHandler's GetCredential taking `any` and handling the cast,
		// or defining GetCredential more specifically. The authtypes.AuthHandler definition
		// uses `any` for session, so it's consistent for now.
		userInfo := p.buildUserInfo(invocationCtx.Session, authConfigs, authHandler)
		authInstruction := p.buildAuthInstruction(userInfo)

		if llmReq.Config == nil {
			llmReq.Config = &modelstypes.GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &modelstypes.Content{Parts: []modelstypes.Part{}}
		}

		var currentSysInstructionText string
		if len(llmReq.Config.SystemInstruction.Parts) > 0 && llmReq.Config.SystemInstruction.Parts[0].Text != nil {
			currentSysInstructionText = *llmReq.Config.SystemInstruction.Parts[0].Text
		}

		if currentSysInstructionText != "" {
			currentSysInstructionText += "\n\n"
		}
		newInstructionText := currentSysInstructionText + authInstruction
		
		if len(llmReq.Config.SystemInstruction.Parts) == 0 {
		    llmReq.Config.SystemInstruction.Parts = append(llmReq.Config.SystemInstruction.Parts, modelstypes.Part{Text: &newInstructionText})
		} else {
		    llmReq.Config.SystemInstruction.Parts[0].Text = &newInstructionText
		}

		if p.isAuthFlowCompletedInPriorTurn(invocationCtx.Session.Events) {
			log.Println("AuthLlmRequestProcessor: Auth flow was recently completed or attempted in a prior turn.")
		}
	}()

	return outCh, nil
}

func (p *AuthLlmRequestProcessor) buildUserInfo(
	session *sessionstypes.Session,
	authConfigs []authtypes.AuthConfig,
	authHandler authtypes.AuthHandler,
) string {
	if session == nil {
		return "User information not available (session is nil)."
	}
	var userInfoParts []string
	userInfoParts = append(userInfoParts, fmt.Sprintf("  User ID: %s", session.UserID))
	userInfoParts = append(userInfoParts, fmt.Sprintf("  App ID: %s", session.AppName))

	for _, authConfig := range authConfigs {
		credential, err := authHandler.GetCredential(authConfig.Name, session) // Pass session directly
		status := "Not authorized"

		if err != nil {
			log.Printf("AuthLlmRequestProcessor: Error getting credential for %s: %v", authConfig.Name, err)
			status = fmt.Sprintf("Error checking authorization for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
		} else {
			if credential.AccessToken != "" {
				if !credential.IsExpired() { // IsExpired defined on authtypes.AuthCredential
					status = fmt.Sprintf("Authorized for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
				} else {
					status = fmt.Sprintf("Authorization expired for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
				}
			} else {
				status = fmt.Sprintf("Not authorized for: %s (Scope: %s)", authConfig.Name, authConfig.Scope)
			}
		}
		userInfoParts = append(userInfoParts, "  "+status)
	}
	return strings.Join(userInfoParts, "\n")
}


func (p *AuthLlmRequestProcessor) buildAuthInstruction(userInfo string) string {
	return fmt.Sprintf(defaultAuthInstructionTemplate, functions.REQUEST_EUC_FUNCTION_CALL_NAME, userInfo)
}

func (p *AuthLlmRequestProcessor) isAuthFlowCompletedInPriorTurn(historyEvents []*eventstypes.Event) bool {
	if len(historyEvents) == 0 {
		return false
	}
	const turnsToLookBack = 2
	maxEventsToCheck := turnsToLookBack * 2
	if maxEventsToCheck > len(historyEvents) {
		maxEventsToCheck = len(historyEvents)
	}

	for i := len(historyEvents) - 1; i >= len(historyEvents)-maxEventsToCheck; i-- {
		event := historyEvents[i]
		if event == nil || event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil && part.FunctionResponse.Name == functions.REQUEST_EUC_FUNCTION_CALL_NAME {
				return true
			}
		}
	}
	return false
}

func newEventID() commontypes.EventID {
	return commontypes.EventID(fmt.Sprintf("evt-%d", time.Now().UnixNano()))
}

func getTimestamp() commontypes.Timestamp {
	return commontypes.Timestamp(float64(time.Now().UnixNano()) / 1e9)
}

# /Users/gopher/Desktop/workspace/adk-go/agents/invocation/inovocation.go
package invocation

import (
	"context"

	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	// Corrected: agentiface "github.com/KennethanCeyer/adk-go/agents/interfaces"
	// agentiface "github.com/KennethanCeyer/adk-go/agents" // If interfaces.go is in agents package
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
)

// AgentInterface is a forward declaration for agentiface.Agent to break import cycle if needed.
// Ideally, agents/interfaces.go defines this and this package imports it.
type AgentInterface interface {
	GetName() string
	GetDescription() string
}


type RunConfig struct {
	ResponseModalities       []string
	MaxLlmCalls              int
	StreamingMode            string
	OutputAudioTranscription bool
	InputAudioTranscription  bool
}

type InvocationContext struct {
	Ctx                context.Context
	InvocationID       commontypes.InvocationID
	Agent              AgentInterface // Use the local forward declaration or direct import if possible
	Session            *sessionstypes.Session
	Branch             commontypes.BranchID
	UserContent        *modelstypes.Content
	RunConfig          *RunConfig
	TranscriptionCache []any
}

type ReadonlyContext struct {
	invocationCtx *InvocationContext
}

func NewReadonlyContext(invCtx *InvocationContext) *ReadonlyContext {
	return &ReadonlyContext{invocationCtx: invCtx}
}

func (roc *ReadonlyContext) GetSession() *sessionstypes.Session {
	if roc.invocationCtx == nil {
		return nil
	}
	return roc.invocationCtx.Session
}

func (roc *ReadonlyContext) GetAgent() AgentInterface {
	if roc.invocationCtx == nil {
		return nil
	}
	return roc.invocationCtx.Agent
}

func (roc *ReadonlyContext) GetInvocationID() commontypes.InvocationID {
	if roc.invocationCtx == nil {
		return ""
	}
	return roc.invocationCtx.InvocationID
}

# /Users/gopher/Desktop/workspace/adk-go/agents/base_agent.go
package agents

import (
	"fmt"
	"regexp"
	// Hypothetical Go ADK packages
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/genai/types" // For genai.Content
	// "github.com/KennethanCeyer/adk-go/telemetry"
)

// Forward declarations for types that would be defined in other packages.
// In a real Go ADK, these would be imported.
type Content struct { // Placeholder for genai.types.Content
	Role  string `json:"role,omitempty"`
	Parts []Part `json:"parts,omitempty"`
}

type Part struct { // Placeholder for genai.types.Part
	Text string `json:"text,omitempty"`
	// Other fields like FunctionCall, FunctionResponse, InlineData would be here
}

type Event struct { // Placeholder for events.Event
	InvocationID string       `json:"invocationId,omitempty"`
	Author       string       `json:"author,omitempty"`
	Branch       string       `json:"branch,omitempty"`
	Content      *Content     `json:"content,omitempty"`
	Actions      EventActions `json:"actions,omitempty"` // Placeholder
	ID           string       `json:"id,omitempty"`
	Timestamp    float64      `json:"timestamp,omitempty"`
	// ... other Event fields
}

type EventActions struct { // Placeholder for events.EventActions
	StateDelta map[string]interface{} `json:"stateDelta,omitempty"`
	// ... other EventActions fields
}

type InvocationContext struct { // Placeholder
	Agent            Agent
	Branch           string
	InvocationID     string
	Session          *Session // Placeholder
	EndInvocation    bool
	UserContent      *Content
	ArtifactService  interface{} // Placeholder
	SessionService   interface{} // Placeholder
	MemoryService    interface{} // Placeholder
	LiveRequestQueue interface{} // Placeholder for LiveRequestQueue
	RunConfig        *RunConfig  // Placeholder
}

type Session struct { // Placeholder for sessions.Session
	State map[string]interface{}
	// ... other Session fields
}

type RunConfig struct { // Placeholder for RunConfig
	// ... RunConfig fields
}

type CallbackContext struct { // Placeholder
	InvocationContext *InvocationContext
	EventActions      EventActions
	State             MutableState // Placeholder for a state map that tracks deltas
}

type MutableState map[string]interface{} // Simplified placeholder

func (ms MutableState) HasDelta() bool {
	// In a real implementation, this would check if _delta has items
	return len(ms) > 0 // Simplified
}

// Agent is the base interface for all agents in the Agent Development Kit.
type Agent interface {
	GetName() string
	GetDescription() string
	ParentAgent() Agent
	SetParentAgent(parent Agent) error
	SubAgents() []Agent
	AddSubAgent(subAgent Agent)
	RunAsync(parentCtx *InvocationContext) (<-chan *Event, error)
	RunLive(parentCtx *InvocationContext) (<-chan *Event, error)
	RootAgent() Agent
	FindAgent(name string) Agent
	FindSubAgent(name string) Agent
	GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback
	GetCanonicalAfterAgentCallbacks() []SingleAgentCallback
}

// SingleAgentCallback defines the signature for agent lifecycle callbacks.
type SingleAgentCallback func(callbackCtx *CallbackContext) (*Content, error)

// BaseAgent provides a common structure for agents.
type BaseAgent struct {
	Name                   string                `json:"name"`
	Description            string                `json:"description,omitempty"`
	Parent                 Agent                 `json:"-"` // Exclude from JSON, handle manually
	Subs                   []Agent               `json:"subAgents,omitempty"`
	BeforeAgentCallback    []SingleAgentCallback `json:"-"` // Callbacks are not typically part of serialized state
	AfterAgentCallback     []SingleAgentCallback `json:"-"`
	agentInvocationContext *InvocationContext    // Internal context for the current run
}

// NewBaseAgent creates a new BaseAgent.
// Name must be a valid Go identifier and not "user".
func NewBaseAgent(name string, description string) (*BaseAgent, error) {
	if !isValidIdentifier(name) {
		return nil, fmt.Errorf("invalid agent name: `%s`. Agent name must be a valid identifier", name)
	}
	if name == "user" {
		return nil, fmt.Errorf("agent name cannot be `user`. `user` is reserved for end-user's input")
	}
	return &BaseAgent{
		Name:        name,
		Description: description,
		Subs:        make([]Agent, 0),
	}, nil
}

func (ba *BaseAgent) GetName() string {
	return ba.Name
}

func (ba *BaseAgent) GetDescription() string {
	return ba.Description
}

func (ba *BaseAgent) ParentAgent() Agent {
	return ba.Parent
}

func (ba *BaseAgent) SetParentAgent(parent Agent) error {
	if ba.Parent != nil && ba.Parent != parent {
		return fmt.Errorf("agent `%s` already has a parent agent: `%s`, cannot add to: `%s`", ba.Name, ba.Parent.GetName(), parent.GetName())
	}
	ba.Parent = parent
	return nil
}

func (ba *BaseAgent) SubAgents() []Agent {
	return ba.Subs
}

func (ba *BaseAgent) AddSubAgent(subAgent Agent) {
	if err := subAgent.SetParentAgent(ba); err != nil {
		// Or handle error as appropriate, e.g. log and skip
		fmt.Printf("Error setting parent for sub-agent %s: %v\n", subAgent.GetName(), err)
		return
	}
	ba.Subs = append(ba.Subs, subAgent)
}

// RunAsync is the entry method to run an agent via text-based conversation.
func (ba *BaseAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	// In Go, we'd typically use channels for async generators.
	// The actual implementation of _runAsyncImpl would push events to this channel.
	// The telemetry.tracer logic would wrap the core processing.
	// For this snippet, we'll simulate the channel.

	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run [%s]", ba.Name)):
		
		event, err := ba.handleBeforeAgentCallback(ctx)
		if err != nil {
			// ì–´ë–»ê²Œ ì—ëŸ¬ë¥¼ outChìœ¼ë¡œ ë³´ë‚¼ ì§€ ê³ ë¯¼ í•„ìš”. Eventì— Error í•„ë“œ ì¶”ê°€?
			// For now, log and potentially send an error event.
			fmt.Printf("Error in beforeAgentCallback for %s: %v\n", ba.Name, err)
			// outCh <- &events.Event{Error: err.Error()} // Conceptual
			return
		}
		if event != nil {
			outCh <- event
		}
		if ctx.EndInvocation {
			return
		}

		err = ba.runAsyncImpl(ctx, outCh) // Pass channel to implementation
		if err != nil {
			fmt.Printf("Error in runAsyncImpl for %s: %v\n", ba.Name, err)
			return
		}

		if ctx.EndInvocation {
			return
		}

		event, err = ba.handleAfterAgentCallback(ctx)
		if err != nil {
			fmt.Printf("Error in afterAgentCallback for %s: %v\n", ba.Name, err)
			return
		}
		if event != nil {
			outCh <- event
		}
	}()

	return outCh, nil
}

// runAsyncImpl is the core logic to run this agent via text-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runAsyncImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	// This is where the Python _run_async_impl's `yield Event(...)` would happen.
	// In Go, we send to the channel.
	// Example: outCh <- &events.Event{Author: ba.Name, Content: ...}
	return fmt.Errorf("_runAsyncImpl for %T is not implemented", ba)
}

// RunLive is the entry method to run an agent via video/audio-based conversation.
func (ba *BaseAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run_live [%s]", ba.Name)):
		err := ba.runLiveImpl(ctx, outCh) // Pass channel
		if err != nil {
			fmt.Printf("Error in runLiveImpl for %s: %v\n", ba.Name, err)
			// Potentially send an error event
		}
	}()
	return outCh, nil
}

// runLiveImpl is the core logic to run this agent via video/audio-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runLiveImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	return fmt.Errorf("_runLiveImpl for %T is not implemented", ba)
}

func (ba *BaseAgent) RootAgent() Agent {
	current := ba
	for current.ParentAgent() != nil {
		current = current.ParentAgent().(*BaseAgent) // Assuming BaseAgent for simplicity
	}
	return current
}

func (ba *BaseAgent) FindAgent(name string) Agent {
	if ba.Name == name {
		return ba
	}
	return ba.FindSubAgent(name)
}

func (ba *BaseAgent) FindSubAgent(name string) Agent {
	for _, subAgent := range ba.Subs {
		if found := subAgent.FindAgent(name); found != nil {
			return found
		}
	}
	return nil
}

func (ba *BaseAgent) createInvocationContext(parentCtx *InvocationContext) *InvocationContext {
	// Create a copy and update agent and branch
	// This is a simplified deep copy. Real implementation needs care.
	ctxCopy := *parentCtx
	ctxCopy.Agent = ba
	if parentCtx.Branch != "" {
		ctxCopy.Branch = fmt.Sprintf("%s.%s", parentCtx.Branch, ba.Name)
	} else {
		ctxCopy.Branch = ba.Name
	}
	return &ctxCopy
}

func (ba *BaseAgent) GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback {
	return ba.BeforeAgentCallback
}

func (ba *BaseAgent) GetCanonicalAfterAgentCallbacks() []SingleAgentCallback {
	return ba.AfterAgentCallback
}

func (ba *BaseAgent) handleBeforeAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalBeforeAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalBeforeAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("beforeAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions, // Assuming EventActions is part of CallbackContext
			}
			ctx.EndInvocation = true
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			Actions:      callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

func (ba *BaseAgent) handleAfterAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalAfterAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalAfterAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("afterAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions,
			}
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			// Content might be nil if callback only changed state
			Actions: callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

// isValidIdentifier checks if a string is a valid Go identifier (simplified).
// Go identifiers are more restrictive than Python's.
func isValidIdentifier(name string) bool {
	if name == "" {
		return false
	}
	// Regex for a simplified Go identifier: starts with a letter, followed by letters or digits.
	// Go allows Unicode letters/digits. This regex is simplified.
	// Actual Go spec: letter { letter | unicode_digit } .
	// For ADK agent names, we might enforce stricter ASCII for interoperability or simplicity.
	matched, _ := regexp.MatchString(`^[a-zA-Z_][a-zA-Z0-9_]*$`, name)
	return matched
}

# /Users/gopher/Desktop/workspace/adk-go/agents/llm_agent.go
package agents

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/tools" // Placeholder for tools package
	// "github.com/KennethanCeyer/adk-go/models" // Placeholder for models package
	// "github.com/KennethanCeyer/adk-go/examples" // Placeholder for examples package
	// "github.com/KennethanCeyer/adk-go/planners" // Placeholder for planners package
	// "github.com/KennethanCeyer/adk-go/codeexecutors" // Placeholder for code executors
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, genai.Content etc.
)

// --- Forward declarations / Placeholders ---
// These would be actual types in a full Go ADK.

type BaseLlm struct { // Placeholder for models.BaseLlm
	ModelName string
}

type GenerateContentConfig struct { // Placeholder for genai.GenerateContentConfig
	SystemInstruction   string        `json:"systemInstruction,omitempty"`
	Tools               []Tool        `json:"tools,omitempty"`         // Placeholder for genai.Tool
	ResponseSchema      interface{}   `json:"responseSchema,omitempty"` // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string        `json:"responseMimeType,omitempty"`
	ThinkingConfig      interface{}   `json:"thinkingConfig,omitempty"`   // Placeholder for genai.ThinkingConfig
	Temperature         *float32      `json:"temperature,omitempty"`
	SafetySettings      []interface{} `json:"safetySettings,omitempty"` // Placeholder for genai.SafetySetting
}

type LlmRequest struct { // Placeholder for models.LlmRequest
	Model               string
	Config              *GenerateContentConfig
	Contents            []Content
	ToolsDict           map[string]Tool // Placeholder
	LiveConnectConfig   interface{}     // Placeholder for genai.LiveConnectConfig
	SystemInstruction   string          // Added for simplicity, original ADK appends to config
	OutputSchema        interface{}     // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string
}

func (lr *LlmRequest) AppendInstructions(instructions []string) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	if lr.Config.SystemInstruction != "" {
		lr.Config.SystemInstruction += "\n\n"
	}
	lr.Config.SystemInstruction += strings.Join(instructions, "\n\n")
}

func (lr *LlmRequest) SetOutputSchema(schema interface{}) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	lr.Config.ResponseSchema = schema
	lr.Config.ResponseMIMEType = "application/json"
}

type LlmResponse struct { // Placeholder for models.LlmResponse
	Content         *Content    `json:"content,omitempty"`
	ErrorCode       string      `json:"errorCode,omitempty"`
	ErrorMessage    string      `json:"errorMessage,omitempty"`
	Partial         bool        `json:"partial,omitempty"`
	CustomMetadata  map[string]interface{} `json:"customMetadata,omitempty"`
}


type Tool interface { // Placeholder for tools.BaseTool
	GetName() string
	GetDescription() string
	GetDeclaration() (*ToolDeclaration, error) // Placeholder for genai.FunctionDeclaration
	ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error
}

type ToolDeclaration struct { // Placeholder for genai.FunctionDeclaration
	Name        string      `json:"name"`
	Description string      `json:"description"`
	Parameters  interface{} `json:"parameters"`
}

type FunctionTool struct { // Placeholder for tools.FunctionTool
	BaseToolImpl
	fn interface{}
}

func NewFunctionTool(fn interface{}) *FunctionTool {
	// Simplified name/description extraction
	name := "unknown_function"
	// In Go, reflection for function name/doc is more involved
	// For now, users might need to provide this explicitly if FunctionTool wraps a raw func
	return &FunctionTool{BaseToolImpl: BaseToolImpl{ToolName: name}, fn: fn}
}

func (ft *FunctionTool) GetName() string { return ft.ToolName }
func (ft *FunctionTool) GetDescription() string { return ft.ToolDescription }
func (ft *FunctionTool) GetDeclaration() (*ToolDeclaration, error) { /* TODO */ return nil, nil }
func (ft *FunctionTool) ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error { /* TODO */ return nil }


type BaseToolset interface { // Placeholder for tools.BaseToolset
	GetTools(readonlyCtx *ReadonlyContext) ([]Tool, error)
}

type Example struct { // Placeholder for examples.Example
	Input  Content   `json:"input"`
	Output []Content `json:"output"`
}

type BaseExampleProvider interface { // Placeholder for examples.BaseExampleProvider
	GetExamples(query string) ([]Example, error)
}

type Planner interface { // Placeholder for planners.BasePlanner
	// Methods would be defined here
}

type CodeExecutor interface { // Placeholder for codeexecutors.BaseCodeExecutor
	// Methods would be defined here
}

type ToolContext struct { // Placeholder for tools.ToolContext
	InvocationContext *InvocationContext
	State             MutableState
	// ... other fields like FunctionCallID, EventActions
}

// Represents ReadonlyContext
type ReadonlyContext struct {
	InvocationContext *InvocationContext
}

func (rc *ReadonlyContext) GetState() map[string]interface{} {
	// Return a read-only copy or wrapper
	// For simplicity, returning the direct map for now, assuming no modification.
	if rc.InvocationContext != nil && rc.InvocationContext.Session != nil {
		return rc.InvocationContext.Session.State
	}
	return make(map[string]interface{})
}


// LLMProvider interface (simplified from previous example)
type LLMProvider interface {
    GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error)
    GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error)
    // Potentially a Connect method for bidirectional streaming if needed for live mode
}

// MockLLMProvider for LlmAgent
type MockLLM struct {
	BaseLlm
}

func (m *MockLLM) GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error) {
	// Simulate LLM call
	log.Printf("MockLLM GenerateContentStream called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	ch := make(chan *LlmResponse, 1)
	go func() {
		defer close(ch)
		// Simplified: just echo back part of the first user message if available
		responseText := "Default mock response"
		if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
			responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
		}
		ch <- &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}
	}()
	return ch, nil
}

func (m *MockLLM) GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error) {
	log.Printf("MockLLM GenerateContent called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	responseText := "Default mock response"
	if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
		responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
	}
	return &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}, nil
}


// SingleBeforeModelCallback defines the signature for a single before-model callback.
type SingleBeforeModelCallback func(callbackCtx *CallbackContext, llmReq *LlmRequest) (*LlmResponse, error)

// SingleAfterModelCallback defines the signature for a single after-model callback.
type SingleAfterModelCallback func(callbackCtx *CallbackContext, llmResp *LlmResponse) (*LlmResponse, error)

// SingleBeforeToolCallback defines the signature for a single before-tool callback.
type SingleBeforeToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext) (map[string]interface{}, error)

// SingleAfterToolCallback defines the signature for a single after-tool callback.
type SingleAfterToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext, toolResponse map[string]interface{}) (map[string]interface{}, error)

// InstructionProvider defines a function that provides an instruction string.
type InstructionProvider func(readonlyCtx *ReadonlyContext) (string, error)

// ToolUnion represents a type that can be a callable, a BaseTool, or a BaseToolset.
// In Go, this would typically be handled by interfaces or by checking types at runtime.
// For this conceptual translation, we'll use `interface{}` and runtime type assertions.
type ToolUnion interface{}

// ExamplesUnion represents types that can provide examples.
type ExamplesUnion interface{} // list[Example] or BaseExampleProvider

// LlmAgent is an LLM-based Agent.
type LlmAgent struct {
	BaseAgent
	AgentModel                  interface{} // string or *BaseLlm (actual model instance)
	AgentInstruction            interface{} // string or InstructionProvider
	AgentGlobalInstruction      interface{} // string or InstructionProvider
	AgentTools                  []ToolUnion
	AgentGenerateContentConfig  *GenerateContentConfig
	DisallowTransferToParent    bool
	DisallowTransferToPeers     bool
	IncludeContents             string // "default" or "none"
	InputSchema                 interface{} // reflect.Type of a struct, or nil
	OutputSchema                interface{} // reflect.Type of a struct, or nil
	OutputKey                   string
	AgentPlanner                Planner
	AgentCodeExecutor           CodeExecutor
	AgentExamples               ExamplesUnion
	AgentBeforeModelCallbacks   []SingleBeforeModelCallback
	AgentAfterModelCallbacks    []SingleAfterModelCallback
	AgentBeforeToolCallbacks    []SingleBeforeToolCallback
	AgentAfterToolCallbacks     []SingleAfterToolCallback

	// Internal flow strategy
	llmFlow LlmFlow
}

// LlmFlow defines the behavior of the LLM agent.
type LlmFlow interface {
	RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error)
	RunLive(invocationCtx *InvocationContext) (<-chan *Event, error)
}

// NewLlmAgent creates a new LlmAgent.
// This constructor attempts to mirror Pydantic's initialization logic.
func NewLlmAgent(name, description string, model interface{}, instruction interface{}) (*LlmAgent, error) {
	base, err := NewBaseAgent(name, description)
	if err != nil {
		return nil, err
	}

	agent := &LlmAgent{
		BaseAgent:        *base,
		AgentModel:       model,
		AgentInstruction: instruction,
		IncludeContents:  "default",
		// Default to AutoFlow which includes SingleFlow + agent transfer
		llmFlow: newAutoFlow(), // Placeholder for flow initialization
	}

	// Simulating Pydantic's model_post_init and field_validator logic
	if err := agent.validateAndProcessConfig(); err != nil {
		return nil, err
	}

	return agent, nil
}

func (a *LlmAgent) validateAndProcessConfig() error {
	if err := a.checkOutputSchema(); err != nil {
		return err
	}
	if err := a.validateGenerateContentConfig(); err != nil {
		return err
	}
	// In Python, Pydantic's model_post_init calls __set_parent_agent_for_sub_agents
	// This is handled by AddSubAgent in Go's BaseAgent.

	// Determine flow based on transfer disallowed flags
	if a.DisallowTransferToParent && a.DisallowTransferToPeers && len(a.Subs) == 0 {
		a.llmFlow = newSingleFlow() // Placeholder
	} else {
		a.llmFlow = newAutoFlow() // Placeholder
	}

	return nil
}


// RunAsync implements the Agent interface.
// It delegates to the internal LlmFlow.
func (a *LlmAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx) // from BaseAgent
	a.agentInvocationContext = ctx // Store context for this agent's run
	return a.llmFlow.RunAsync(ctx)
}

// RunLive implements the Agent interface.
func (a *LlmAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx)
	a.agentInvocationContext = ctx
	return a.llmFlow.RunLive(ctx)
}


func (a *LlmAgent) CanonicalModel() (BaseLlm, error) {
	if modelInst, ok := a.AgentModel.(BaseLlm); ok {
		return modelInst, nil
	}
	if modelStr, ok := a.AgentModel.(string); ok && modelStr != "" {
		// Placeholder for LLMRegistry.NewLlm(modelStr)
		// In a real Go ADK, this would look up and instantiate the model.
		return MockLLM{BaseLlm{ModelName: modelStr}}, nil
	}

	current := a.ParentAgent()
	for current != nil {
		if llmAgent, ok := current.(*LlmAgent); ok {
			return llmAgent.CanonicalModel()
		}
		current = current.ParentAgent()
	}
	return MockLLM{}, fmt.Errorf("no model found for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentInstruction.(func(ctx *ReadonlyContext) (string, error)); ok { // Allow func type as well
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, fmt.Errorf("invalid instruction type for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentGlobalInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentGlobalInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentGlobalInstruction.(func(ctx *ReadonlyContext) (string, error)); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, nil // Global instruction can be empty
}

func (a *LlmAgent) CanonicalTools(ctx *ReadonlyContext) ([]Tool, error) {
	var resolvedTools []Tool
	for _, toolUnion := range a.AgentTools {
		switch t := toolUnion.(type) {
		case Tool:
			resolvedTools = append(resolvedTools, t)
		case func(interface{}) interface{}: // Simplified check for a callable
			// This is a very basic check. Robustly handling arbitrary callables
			// and converting them to tools.Tool would require more reflection
			// or specific registration mechanisms.
			// For now, wrapping it in a conceptual FunctionTool.
			resolvedTools = append(resolvedTools, NewFunctionTool(t))
		case BaseToolset:
			toolsFromSet, err := t.GetTools(ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to get tools from toolset: %w", err)
			}
			resolvedTools = append(resolvedTools, toolsFromSet...)
		default:
			return nil, fmt.Errorf("unsupported tool type: %T", t)
		}
	}
	return resolvedTools, nil
}


func (a *LlmAgent) GetCanonicalBeforeModelCallbacks() []SingleBeforeModelCallback {
	return a.AgentBeforeModelCallbacks
}

func (a *LlmAgent) GetCanonicalAfterModelCallbacks() []SingleAfterModelCallback {
	return a.AgentAfterModelCallbacks
}

func (a *LlmAgent) GetCanonicalBeforeToolCallbacks() []SingleBeforeToolCallback {
	return a.AgentBeforeToolCallbacks
}

func (a *LlmAgent) GetCanonicalAfterToolCallbacks() []SingleAfterToolCallback {
	return a.AgentAfterToolCallbacks
}

func (a *LlmAgent) maybeSaveOutputToState(event *Event) {
	if a.OutputKey == "" || !eventIsFinalResponse(event) || event.Content == nil || len(event.Content.Parts) == 0 {
		return
	}

	var resultData interface{}
	var combinedText []string
	for _, part := range event.Content.Parts {
		if part.Text != "" {
			combinedText = append(combinedText, part.Text)
		}
		// In a real scenario, you'd handle other part types like function calls/responses if they constituted the "output"
	}
	fullText := strings.Join(combinedText, "\n")


	if a.OutputSchema != nil {
		// In Go, Pydantic's model_validate_json would be equivalent to json.Unmarshal into a struct.
		// This requires a.OutputSchema to be a reflect.Type of the target struct.
		// For simplicity, this placeholder assumes fullText is valid JSON for the schema.
		// A real implementation would use json.Unmarshal(byte(fullText), &targetStructInstance)
		// and then potentially convert targetStructInstance to map[string]interface{} if needed.
		var tempMap map[string]interface{}
		// This is a simplified placeholder. Real unmarshalling and validation needed.
		if err := json.Unmarshal([]byte(fullText), &tempMap); err == nil {
			resultData = tempMap
		} else {
			log.Printf("Warning: Failed to parse output for key '%s' against schema: %v. Storing raw text.", a.OutputKey, err)
			resultData = fullText // Fallback to raw text
		}
	} else {
		resultData = fullText
	}

	if event.Actions.StateDelta == nil {
		event.Actions.StateDelta = make(map[string]interface{})
	}
	event.Actions.StateDelta[a.OutputKey] = resultData
}

func (a *LlmAgent) checkOutputSchema() error {
	if a.OutputSchema == nil {
		return nil
	}

	if !a.DisallowTransferToParent || !a.DisallowTransferToPeers {
		log.Println(
			fmt.Sprintf("Warning: Invalid config for agent %s: outputSchema cannot co-exist with agent transfer configurations. Setting disallowTransferToParent=true, disallowTransferToPeers=true", a.Name),
		)
		a.DisallowTransferToParent = true
		a.DisallowTransferToPeers = true
	}

	if len(a.SubAgents()) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, subAgents must be empty to disable agent transfer", a.Name)
	}

	if len(a.AgentTools) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, tools must be empty", a.Name)
	}
	return nil
}

func (a *LlmAgent) validateGenerateContentConfig() error {
	if a.AgentGenerateContentConfig == nil {
		a.AgentGenerateContentConfig = &GenerateContentConfig{} // Ensure it's not nil
		return nil
	}
	if a.AgentGenerateContentConfig.ThinkingConfig != nil {
		return fmt.Errorf("thinkingConfig should be set via LlmAgent.Planner")
	}
	if len(a.AgentGenerateContentConfig.Tools) > 0 {
		return fmt.Errorf("all tools must be set via LlmAgent.Tools")
	}
	if a.AgentGenerateContentConfig.SystemInstruction != "" {
		return fmt.Errorf("system instruction must be set via LlmAgent.Instruction or LlmAgent.GlobalInstruction")
	}
	if a.AgentGenerateContentConfig.ResponseSchema != nil {
		return fmt.Errorf("response schema must be set via LlmAgent.OutputSchema")
	}
	return nil
}

// Helper, would be part of event logic
func eventIsFinalResponse(e *Event) bool {
	if e.Actions.StateDelta["skipSummarization"] == true { // Assuming skipSummarization is stored in StateDelta
		return true
	}
	// Simplified: In Python ADK, this checks for function calls/responses and partial flags.
	// Here, we'll assume an event is final if it's not marked partial and has content.
	return e.Content != nil && !e.Partial // Placeholder for e.Partial
}

// --- Placeholder Flow Implementations ---
// In a real Go ADK, these would be more fleshed out and likely in their own package.

type SingleFlow struct {
	// RequestProcessors  []BaseLlmRequestProcessor  // Placeholder
	// ResponseProcessors []BaseLlmResponseProcessor // Placeholder
}

func newSingleFlow() *SingleFlow {
	// Initialize processors
	return &SingleFlow{}
}

func (sf *SingleFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)
		llmReq := &LlmRequest{} // Initialize LlmRequest

		// Simulate preprocessing (would iterate through sf.RequestProcessors)
		if err := sf.preprocessAsync(invocationCtx, llmReq); err != nil {
			log.Printf("Error during preprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()} // Conceptual error event
			return
		}

		llmAgent, ok := invocationCtx.Agent.(*LlmAgent)
		if !ok {
			log.Printf("Error: agent is not an LlmAgent")
			return
		}
		llm, err := llmAgent.CanonicalModel()
		if err != nil {
			log.Printf("Error getting canonical model: %v", err)
			return
		}
		
		// Simplified LLM call for SingleFlow
		// A real implementation would handle streaming, function calling loops, etc.
		var llmResponse *LlmResponse

		// This is a simplification. The Python ADK has a loop for multiple LLM calls if tools are used.
		// For this Go conceptual translation, we'll assume a single LLM call or a very simplified tool use.
		if provider, ok := llm.(LLMProvider); ok {
			// Use GenerateContent for non-streaming for simplicity in this conceptual flow
			llmResponse, err = provider.GenerateContent(context.Background(), llmReq)
			if err != nil {
				log.Printf("Error calling LLM: %v", err)
				// outCh <- &Event{Error: err.Error()}
				return
			}
		} else {
			log.Printf("Error: LLM model does not implement LLMProvider interface")
			return
		}
		

		modelResponseEvent := &Event{
			ID:            newID(), // Generate new event ID
			InvocationID:  invocationCtx.InvocationID,
			Author:        invocationCtx.Agent.GetName(),
			Branch:        invocationCtx.Branch,
			Content:       llmResponse.Content, // Simplified
			// Actions, ErrorCode, ErrorMessage, etc. would be populated
		}

		// Simulate postprocessing (would iterate through sf.ResponseProcessors)
		if err := sf.postprocessAsync(invocationCtx, llmReq, llmResponse, modelResponseEvent, outCh); err != nil {
			log.Printf("Error during postprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()}
			return
		}
		
		// If not handled by post-processors (e.g. tool calls), yield the direct LLM response event
		if modelResponseEvent.Content != nil || modelResponseEvent.Actions.StateDelta != nil { // Yield if there's something to yield
			outCh <- modelResponseEvent
		}

	}()
	return outCh, nil
}


func (sf *SingleFlow) RunLive(invocationCtx *InvocationContext) (<-chan *Event, error) {
	// Simplified RunLive, a real one would be more complex with bidirectional streaming
	return sf.RunAsync(invocationCtx) // Fallback for now
}

func (sf *SingleFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	// Conceptual: Iterate through request_processors from Python ADK
	// Example: basic.request_processor, instructions.request_processor, etc.
	// For this Go version, we'll call helper methods directly for some core logic.
	agent := invocationCtx.Agent.(*LlmAgent) // Assuming LlmAgent

	// Basic processor logic
	modelInstance, err := agent.CanonicalModel()
	if err != nil {
		return err
	}
	llmReq.Model = modelInstance.ModelName // Assuming BaseLlm has ModelName
	if agent.AgentGenerateContentConfig != nil {
		cfgCopy := *agent.AgentGenerateContentConfig
		llmReq.Config = &cfgCopy
	} else {
		llmReq.Config = &GenerateContentConfig{}
	}
	if agent.OutputSchema != nil {
		llmReq.SetOutputSchema(agent.OutputSchema)
	}
	// ... (add live_connect_config population from RunConfig)

	// Instructions processor logic
	// Global Instruction
	rootAgent := agent.RootAgent().(*LlmAgent) // Assuming LlmAgent
	if rootAgent.AgentGlobalInstruction != nil {
		instr, _, err := rootAgent.CanonicalGlobalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		// Simplified: In Python, instructions_utils.inject_session_state is used.
		// For Go, this would involve a similar template processing.
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	// Agent Instruction
	if agent.AgentInstruction != nil {
		instr, _, err := agent.CanonicalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	
	// Identity processor
	var si []string
	si = append(si, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName()))
	if agent.GetDescription() != "" {
		si = append(si, fmt.Sprintf(" The description about you is \"%s\"", agent.GetDescription()))
	}
	llmReq.AppendInstructions(si)

	// Contents processor logic (simplified)
	if agent.IncludeContents != "none" {
		llmReq.Contents = getContentsForLlm(invocationCtx.Branch, invocationCtx.Session.Events, agent.GetName())
	}

	// Tool processing (adding declarations to llmReq.Config.Tools)
	tools, err := agent.CanonicalTools(&ReadonlyContext{InvocationContext: invocationCtx})
	if err != nil { return err}
	if len(tools) > 0 {
		if llmReq.Config.Tools == nil {
			llmReq.Config.Tools = make([]Tool, 0)
		}
	}
	for _, tool := range tools {
		// Simplified: directly appending, Python ADK has more complex logic for FunctionDeclaration
		llmReq.Config.Tools = append(llmReq.Config.Tools, tool) // This assumes tool itself is the declaration or can provide it
		
		// Store tool in llmReq.ToolsDict for later execution lookup by name
		if llmReq.ToolsDict == nil {
			llmReq.ToolsDict = make(map[string]Tool)
		}
		llmReq.ToolsDict[tool.GetName()] = tool
	}

	return nil
}

func (sf *SingleFlow) postprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest, llmResp *LlmResponse, modelResponseEvent *Event, outCh chan<- *Event) error {
	// Conceptual: Iterate through response_processors
	// Example: _nl_planning.response_processor, _code_execution.response_processor
	
	// Handle function calls (simplified from functions.py)
	if llmResp.Content != nil {
		var functionCalls []struct{ Name string; Args map[string]interface{}; ID string } // Simplified representation
		for _, part := range llmResp.Content.Parts {
			// Placeholder: In Python this uses part.function_call
			// For Go, we'd check a similar field if LlmResponse.Content.Parts had FunctionCall type
			// For this example, assume FunctionCall is identified by a specific structure in Part
			// For now, this logic is highly simplified and conceptual
			if strings.HasPrefix(part.Text, "CALL_TOOL:") { // Very basic trigger
				parts := strings.SplitN(strings.TrimPrefix(part.Text, "CALL_TOOL:"), "(", 2)
				name := parts[0]
				argsStr := ""
				if len(parts) > 1 {
					argsStr = strings.TrimSuffix(parts[1], ")")
				}
				var args map[string]interface{}
				json.Unmarshal([]byte(argsStr), &args) // Simplified arg parsing
				functionCalls = append(functionCalls, struct{ Name string; Args map[string]interface{}; ID string }{Name: name, Args: args, ID: newID()})

				// Populate client function call ID if LLM didn't provide one
				modelResponseEvent.Content.Parts[0].Text = "" // Clear text part conceptually
				// modelResponseEvent.Content.Parts[0].FunctionCall = &genai.FunctionCall{Name: name, Args: args, ID: ...}
			}
		}

		if len(functionCalls) > 0 {
			// In Python ADK, this calls functions.handle_function_calls_async
			// which then calls tool.run_async and creates a response event.
			// This is a very complex part involving callbacks, state updates, etc.
			// Here's a highly simplified conceptual flow:
			var toolResponseParts []Part
			for _, fc := range functionCalls {
				tool, ok := llmReq.ToolsDict[fc.Name]
				if !ok {
					log.Printf("Tool %s not found", fc.Name)
					toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Error: Tool %s not found", fc.Name)})
					continue
				}
				// Conceptual tool execution
				// toolResult, err := tool.Execute(context.Background(), fc.Args) // This is complex
				// Simplified:
				toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Tool %s called with %v (mocked)", fc.Name, fc.Args)})
			}
			
			// Create a new event for tool responses and send it
			toolResponseEvent := &Event{
				ID:            newID(),
				InvocationID:  invocationCtx.InvocationID,
				Author:        invocationCtx.Agent.GetName(),
				Branch:        invocationCtx.Branch,
				Content:       &Content{Role: "model", Parts: toolResponseParts}, // Simplified: role should be 'user' for function response in Gemini
			}
			outCh <- toolResponseEvent
			
			// The original LLM response event (modelResponseEvent) that *contained* the function call
			// is also yielded. Then, the flow would typically make *another* LLM call with the tool responses.
			// For simplicity, we will stop here in this conceptual snippet.
			// The python ADK has a loop in _run_one_step_async for this.
			modelResponseEvent.Content = nil // After tool call, the original content is usually superseded or followed by tool response processing
		}
	}
	
	return nil
}

type AutoFlow struct {
	SingleFlow
	// agentTransferRequestProcessor // Placeholder for agent_transfer.request_processor
}

func newAutoFlow() *AutoFlow {
	return &AutoFlow{SingleFlow: *newSingleFlow()}
}

func (af *AutoFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	if err := af.SingleFlow.preprocessAsync(invocationCtx, llmReq); err != nil {
		return err
	}
	// Agent transfer logic
	agent := invocationCtx.Agent.(*LlmAgent)
	transferTargets := getTransferTargets(agent)
	if len(transferTargets) > 0 {
		llmReq.AppendInstructions([]string{buildTargetAgentsInstructions(agent, transferTargets)})
		// Conceptually add transfer_to_agent tool
		// transferTool := NewFunctionTool(transferToAgent) // transferToAgent needs to be defined
		// ... add transferTool to llmReq.Config.Tools and llmReq.ToolsDict
	}
	return nil
}

// Helper to simulate Python's list comprehension and filtering for contents
func getContentsForLlm(currentBranch string, allEvents []*Event, agentName string) []Content {
	var contents []Content
	// This is a simplified version of Python ADK's _get_contents, which has complex logic
	// for rearranging events, handling async function responses, and filtering by branch.
	for _, event := range allEvents {
		if event.Content != nil && len(event.Content.Parts) > 0 {
			// Simplified branch check and foreign event conversion
			if isEventOnBranch(currentBranch, event) && !isAuthEvent(event) {
				if isOtherAgentReply(agentName, event) {
					contents = append(contents, *convertForeignEvent(event))
				} else {
					// Deep copy content before modifying (e.g. removing client func call ID)
					contentCopy := *event.Content 
					// removeClientFunctionCallID(&contentCopy) // Placeholder for actual logic
					contents = append(contents, contentCopy)
				}
			}
		}
	}
	return contents
}

// Simplified placeholders for helper functions from Python's contents.py
func isEventOnBranch(invocationBranch string, event *Event) bool { 
	if invocationBranch == "" || event.Branch == "" { return true }
	return strings.HasPrefix(invocationBranch, event.Branch)
}
func isAuthEvent(event *Event) bool { return false /* TODO */ }
func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return event.Author != currentAgentName && event.Author != "user"
}
func convertForeignEvent(event *Event) *Content { 
	// Simplified conversion
	return &Content{Role: "user", Parts: []Part{{Text: fmt.Sprintf("Context from %s: %s", event.Author, event.Content.Parts[0].Text)}}}
}
func newID() string { return "temp-id" } // Placeholder for ID generation

func getTransferTargets(agent *LlmAgent) []Agent {
    var targets []Agent
    targets = append(targets, agent.SubAgents()...) // Assuming SubAgents returns []Agent

    if agent.ParentAgent() != nil {
        if llmParent, ok := agent.ParentAgent().(*LlmAgent); ok {
            if !agent.DisallowTransferToParent {
                targets = append(targets, llmParent)
            }
            if !agent.DisallowTransferToPeers {
                for _, peerAgent := range llmParent.SubAgents() {
                    if peerAgent.GetName() != agent.GetName() {
                        targets = append(targets, peerAgent)
                    }
                }
            }
        }
    }
    return targets
}

func buildTargetAgentsInfo(targetAgent Agent) string {
    return fmt.Sprintf("\nAgent name: %s\nAgent description: %s\n", targetAgent.GetName(), targetAgent.GetDescription())
}

func buildTargetAgentsInstructions(agent *LlmAgent, targetAgents []Agent) string {
    var sb strings.Builder
    sb.WriteString("\nYou have a list of other agents to transfer to:\n")
    for _, target := range targetAgents {
        sb.WriteString(buildTargetAgentsInfo(target))
    }
    sb.WriteString("\nIf you are the best to answer the question according to your description, you can answer it.")
    sb.WriteString("\nIf another agent is better for answering the question according to its description, call `transfer_to_agent` function to transfer the question to that agent. When transferring, do not generate any text other than the function call.\n")

    if agent.ParentAgent() != nil {
        sb.WriteString(fmt.Sprintf("\nYour parent agent is %s. If neither the other agents nor you are best for answering the question according to the descriptions, transfer to your parent agent. If you don't have parent agent, try answer by yourself.\n", agent.ParentAgent().GetName()))
    }
    return sb.String()
}

// Placeholder for instruction template processing
func processInstructionTemplate(template string, ctx *ReadonlyContext) (string, error) {
	// In Python, this uses string.Formatter with a custom class to access state.
	// In Go, this would require a more explicit template engine or string replacement.
	// This is a very simplified version.
	processed := template
	// Regex to find {key} or {key?}
	re := regexp.MustCompile(`{([^}]+)}`)
	state := ctx.GetState() // Assuming GetState returns map[string]interface{}

	processed = re.ReplaceAllStringFunc(processed, func(match string) string {
		keyWithOptionalMarker := strings.Trim(match, "{} ")
		key := strings.TrimSuffix(keyWithOptionalMarker, "?")
		isOptional := strings.HasSuffix(keyWithOptionalMarker, "?")

		// Handle artifact.filename format
		if strings.HasPrefix(key, "artifact.") {
			// artifactName := strings.TrimPrefix(key, "artifact.")
			// artifactContent, err := loadArtifact(ctx.InvocationContext, artifactName) // Placeholder
			// if err != nil {
			// 	if isOptional { return "" }
			// 	log.Printf("Error loading artifact %s: %v", artifactName, err)
			// 	return match // return original on error
			// }
			// return artifactContent
			return "[ARTIFACT_CONTENT_PLACEHOLDER]" // Simplified
		}

		val, ok := state[key]
		if ok {
			return fmt.Sprintf("%v", val)
		}
		if isOptional {
			return ""
		}
		log.Printf("Warning: Context variable not found and not optional: `%s`", key)
		return match // Return original placeholder if not found and not optional
	})
	return processed, nil
}

# /Users/gopher/Desktop/workspace/adk-go/agents/interfaces/interfaces.go
package agents

import (
	"context"

	// Corrected import paths, assuming types are in their respective /types/ subdirectories
	// commontypes "github.com/KennethanCeyer/adk-go/common/types"
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	plannertypes "github.com/KennethanCeyer/adk-go/planners/types"
	toolstypes "github.com/KennethanCeyer/adk-go/tools/types"
	// InvocationContext and ReadonlyContext would be defined in adk-go/agents/invocation
	// invocation "github.com/KennethanCeyer/adk-go/agents/invocation"
)

// Placeholders for types from sub-packages or sibling packages to avoid initial import cycles.
// These should be replaced with fully qualified types once all packages are correctly defined.
type InvocationContextPlaceholder any // To be *invocation.InvocationContext
type ReadonlyContextPlaceholder any // To be *invocation.ReadonlyContext


type Agent interface {
	GetName() string
	GetDescription() string
}

type LlmAgent interface {
	Agent

	GetModelIdentifier() string
	GetSystemInstruction() string
	GetTools() []toolstypes.Tool
	GetAuthHandler() authtypes.AuthHandler
	GetPlanner() plannertypes.Planner
	GetGenerateContentConfig() *modelstypes.GenerateContentConfig
	GetOutputSchema() any
	GetIncludeContents() string
	IsRootAgent() bool
	GetParentAgent() LlmAgent
	RootAgent() LlmAgent

	PredictAsync(ctx context.Context, invocationCtx InvocationContextPlaceholder, request *modelstypes.LlmRequest) (<-chan *eventstypes.Event, error)

	CanonicalInstruction(ctx ReadonlyContextPlaceholder) (instruction string, bypassStateInjection bool, err error)
	CanonicalGlobalInstruction(ctx ReadonlyContextPlaceholder) (instruction string, bypassStateInjection bool, err error)
}

# /Users/gopher/Desktop/workspace/adk-go/models/types/types.go
package types

type Part struct {
	Text             *string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
	// InlineData *Blob // Example for future extension
}

type Blob struct {
	MimeType string
	Data     []byte
}

type Content struct {
	Parts []Part
	Role  string
}

type FunctionCall struct {
	ID   string
	Name string
	Args any
}

type FunctionResponse struct {
	ID       string
	Name     string
	Response any
}

type GenerateContentConfig struct {
	Temperature       *float32
	TopP              *float32
	TopK              *int32
	CandidateCount    *int32
	MaxOutputTokens   *int32
	StopSequences     []string
	SystemInstruction *Content
	ResponseSchema    any
	ResponseMimeType  string
}

type LlmRequest struct {
	ModelIdentifier string
	Contents        []Content
	Config          *GenerateContentConfig
}

type LlmResponse struct {
	Content *Content
	// Other fields like UsageMetadata, FinishReason, SafetyRatings can be added here.
}

# /Users/gopher/Desktop/workspace/adk-go/README.md
# ADK-Go: Agent Development Kit in Go (Conceptual & Executable Example)

[![Go Reference](https://pkg.go.dev/badge/github.com/KennethanCeyer/adk-go.svg)](https://pkg.go.dev/github.com/KennethanCeyer/adk-go)

This repository provides a conceptual and partially executable example of porting the core ideas of an Agent Development Kit (ADK) from Python to Go. The primary goal is to demonstrate how agent functionalities, tool usage, and LLM interactions can be structured in Go, rather than a direct 1:1 port of the entire Python ADK.

This README focuses on a **minimal, executable "Hello World" agent** that uses a simple tool and interacts with a real LLM (Google Gemini).

## Project Structure (Conceptual)

adk-go/
â”œâ”€â”€ cmd/
â”‚ â””â”€â”€ helloworld_runner/ # Executable for the HelloWorld agent
â”‚ â””â”€â”€ main.go
â”œâ”€â”€ pkg/
â”‚ â”œâ”€â”€ adk/ # Core ADK interfaces and simple agent runner
â”‚ â”‚ â”œâ”€â”€ agent.go # Agent, Tool, LLMProvider interfaces
â”‚ â”‚ â”œâ”€â”€ runner.go # Simple CLI runner
â”‚ â”‚ â””â”€â”€ types.go # Core data structures (Message, Part, etc.)
â”‚ â”œâ”€â”€ llmproviders/ # LLM interaction implementations
â”‚ â”‚ â””â”€â”€ gemini.go # Google Gemini LLM provider
â”‚ â””â”€â”€ tools/ # Example tool implementations
â”‚ â””â”€â”€ rolldie.go
â”œâ”€â”€ examples/
â”‚ â””â”€â”€ helloworld/ # HelloWorld agent definition
â”‚ â””â”€â”€ agent.go
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ README.md

## Prerequisites

1.  **Go**: Version 1.20 or later. ([Installation Guide](https://go.dev/doc/install))
2.  **Google Cloud Project**: A Google Cloud Project with the Vertex AI API (or Generative Language API) enabled.
3.  **API Key**: An API key for Google Gemini. You can obtain this from Google AI Studio or your Google Cloud console.
    - **Important**: Treat your API key like a password. Do not commit it to your repository. Use environment variables.
4.  **Environment Variable**: Set the `GEMINI_API_KEY` environment variable:
    ```bash
    export GEMINI_API_KEY="YOUR_API_KEY_HERE"
    ```

## Core Concepts & Types (pkg/adk/types.go, pkg/adk/agent.go)

The core of this simplified ADK-Go revolves around a few key interfaces and structs:

- **`Message`**: Represents a message in the conversation (user or model).
  ```go
  type Message struct {
      Role    string `json:"role"` // "user" or "model"
      Parts   []Part `json:"parts"`
  }
  ```
- **`Part`**: Represents a piece of content in a message (text, function call, function response).
  ```go
  type Part struct {
      Text             *string           `json:"text,omitempty"`
      FunctionCall     *FunctionCall     `json:"functionCall,omitempty"`
      FunctionResponse *FunctionResponse `json:"functionResponse,omitempty"`
  }
  ```
- **`FunctionCall`**: Represents an LLM's request to call a tool.
  ```go
  type FunctionCall struct {
      Name string `json:"name"`
      Args any    `json:"args"` // Usually map[string]any
  }
  ```
- **`FunctionResponse`**: Represents the result of a tool execution.
  ```go
  type FunctionResponse struct {
      Name     string `json:"name"`
      Response any    `json:"response"` // Usually map[string]any
  }
  ```
- **`Tool` Interface**: Defines the contract for any tool an agent can use.
  ```go
  type Tool interface {
      Name() string
      Description() string
      Parameters() any // JSON schema for parameters
      Execute(ctx context.Context, args any) (any, error)
  }
  ```
- **`LLMProvider` Interface**: Defines how to interact with an LLM.
  ```go
  type LLMProvider interface {
      GenerateContent(
          ctx context.Context,
          modelName string,
          systemInstruction string,
          tools []Tool, // Tools available to the LLM
          history []Message, // Conversation history
          latestMessage Message, // The latest user input or tool response to process
      ) (Message, error) // Returns the LLM's response message
  }
  ```
- **`Agent` Interface**: Defines the core agent logic.
  ```go
  type Agent interface {
      Name() string
      SystemInstruction() string
      Tools() []Tool
      ModelName() string
      LLMProvider() LLMProvider
      Process(ctx context.Context, history []Message, latestMessage Message) (Message, error)
  }
  ```

## Setup & Running the HelloWorld Agent

1.  **Clone the Repository (or create files as below):**
    If this code were in a Git repository:

    ```bash
    # git clone [https://github.com/KennethanCeyer/adk-go.git](https://github.com/KennethanCeyer/adk-go.git)
    # cd adk-go
    ```

    For now, you'll be creating these files manually based on the code snippets.

2.  **Initialize Go Module:**
    In the root directory of your project (`adk-go`), run:

    ```bash
    go mod init [github.com/KennethanCeyer/adk-go](https://github.com/KennethanCeyer/adk-go)
    ```

3.  **Install Dependencies:**
    The Gemini LLM provider will use Google's generative AI Go SDK.

    ```bash
    go get google.golang.org/api/iterator \
           [cloud.google.com/go/vertexai/apiv1/vertexaipb](https://cloud.google.com/go/vertexai/apiv1/vertexaipb) \
           [github.com/google/generative-ai-go/genai](https://github.com/google/generative-ai-go/genai)
    ```

    (Note: `google.golang.org/api/iterator` and `cloud.google.com/go/vertexai/apiv1/vertexaipb` might be indirect dependencies of `genai` or specific to Vertex AI usage. The `genai` package is the primary one for Gemini directly.)

4.  **Create the Code Files:**
    Create the directory structure and files as outlined in the "Project Structure" section. Populate them with the Go code provided in the subsequent sections of this response.

5.  **Set your `GEMINI_API_KEY` environment variable** as described in "Prerequisites".

6.  **Run the HelloWorld Agent:**

    ```bash
    go run ./cmd/helloworld_runner/main.go
    ```

7.  **Interact with the Agent:**
    The runner will prompt you for input. Try typing:
    - `Hello`
    - `Can you roll a 6-sided die for me?`
    - `Roll a d20`
    - Type `exit` or `quit` to terminate.

# /Users/gopher/Desktop/workspace/adk-go/common/types/types.go
package types

type Timestamp float64

type UserInfo struct {
	UserID string
}

type BranchID string

type InvocationID string

type SessionID string

type AgentID string

type ToolID string

type EventID string

type ErrorCode string

# /Users/gopher/Desktop/workspace/adk-go/sessions/types/types.go
package types

import (
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	eventstypes "github.com/KennethanCeyer/adk-go/events/types"
)

type State map[string]any

type Session struct {
	ID             commontypes.SessionID
	AppName        string
	UserID         string
	CreationTime   commontypes.Timestamp
	LastAccessTime commontypes.Timestamp
	State          State
	Events         []*eventstypes.Event `json:"-"`
	Metadata       map[string]string    `json:",omitempty"`
}

# /Users/gopher/Desktop/workspace/adk-go/planners/types/types.go
package types

// Forward declarations for types. Replace with actual imports when types are fully defined.
type AnyLlmRequest any
type AnyReadonlyContext any
type AnyCallbackContext any
type AnyPart any

type Planner interface {
	Name() string
	BuildPlanningInstruction(readonlyCtx AnyReadonlyContext, llmReq AnyLlmRequest) (instruction string, err error)
	ProcessPlanningResponse(callbackCtx AnyCallbackContext, responseParts []AnyPart) (processedParts []AnyPart, err error)
	ApplyThinkingConfig(llmReq AnyLlmRequest)
}

# /Users/gopher/Desktop/workspace/adk-go/events/types/types.go
package types

import (
	authtypes "github.com/KennethanCeyer/adk-go/auth/types"
	commontypes "github.com/KennethanCeyer/adk-go/common/types"
	modelstypes "github.com/KennethanCeyer/adk-go/models/types"
)

type EventActions struct {
	StateDelta             map[string]any
	RequestedAuthConfigs []authtypes.AuthConfigName
	EscalateTo             commontypes.AgentID
	TransferToAgent        commontypes.AgentID
	SkipSummarization      bool
	ProducedArtifacts      []string
	ConsumedArtifacts      []string
	CustomMetadata         map[string]any
	TurnComplete           bool
}

type Event struct {
	ID                commontypes.EventID
	InvocationID      commontypes.InvocationID
	SessionID         commontypes.SessionID
	ParentEventID     commontypes.EventID `json:",omitempty"`
	Timestamp         commontypes.Timestamp
	Author            string
	Role              string `json:",omitempty"`
	Content           *modelstypes.Content
	Actions           *EventActions `json:",omitempty"`
	Branch            commontypes.BranchID `json:",omitempty"`
	IsPartial         bool `json:",omitempty"`
	ErrorCode         commontypes.ErrorCode `json:",omitempty"`
	ErrorMessage      string `json:",omitempty"`
	LlmResponse       *modelstypes.LlmResponse `json:",omitempty"`
	ObservedLatencyMs int64 `json:",omitempty"`
}

func (e *Event) GetFunctionCalls() []modelstypes.FunctionCall {
	if e == nil || e.Content == nil {
		return nil
	}
	var calls []modelstypes.FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

func (e *Event) GetFunctionResponses() []modelstypes.FunctionResponse {
	if e == nil || e.Content == nil {
		return nil
	}
	var responses []modelstypes.FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/base_llm_flow.go
package llmflows

import (
	"fmt"
	"log"
	"reflect"
	"runtime"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/sessions"
	// "github.com/KennethanCeyer/adk-go/utils"
)

// Placeholder types from assumed packages (if not already fully defined in processors)
type InvocationContext = processors.InvocationContext
type LlmRequest = models.LlmRequest
type LlmResponse = models.LlmResponse
type Event = events.Event
type EventActions = events.EventActions
type LlmAgent = agents.LlmAgent // This should be the primary agent interface
type Content = models.Content   // Assuming models package defines Content, Part etc.
type Part = models.Part
type FunctionCall = models.FunctionCall
type FunctionResponse = models.FunctionResponse
type State = sessions.State
type RunConfig = agents.RunConfig // Assuming RunConfig is in agents

// BaseLlmFlow is the base for LLM-based flows.
type BaseLlmFlow struct {
	RequestProcessors  []processors.BaseLlmRequestProcessor
	ResponseProcessors []processors.BaseLlmResponseProcessor
}

// NewBaseLlmFlow creates a new BaseLlmFlow.
func NewBaseLlmFlow(
	requestProcessors []processors.BaseLlmRequestProcessor,
	responseProcessors []processors.BaseLlmResponseProcessor) *BaseLlmFlow {
	return &BaseLlmFlow{
		RequestProcessors:  requestProcessors,
		ResponseProcessors: responseProcessors,
	}
}

// RunAsync executes the LLM flow.
func (f *BaseLlmFlow) RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error) {
	outputEventCh := make(chan *Event)

	go func() {
		defer close(outputEventCh)

		llmReq := &LlmRequest{} // Initialize with appropriate defaults if any
		if llmAgent.GetGenerateContentConfig() != nil { // Assuming LlmAgent has GetGenerateContentConfig
			llmReq.Config = llmAgent.GetGenerateContentConfig()
		} else {
			llmReq.Config = &models.GenerateContentConfig{} // Ensure config is not nil
		}


		// 1. Pre-LLM processing
		for _, processor := range f.RequestProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name() // Get processor name for logging
			log.Printf("Running request processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmReq)
			if err != nil {
				log.Printf("Error running request processor %s: %v", processorName, err)
				outputEventCh <- &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in request processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				}
				return
			}
			// Drain events from processor if any (though these processors typically modify llmReq)
			for event := range eventCh {
				log.Printf("Event from request processor %s: %v", processorName, event.ID)
				outputEventCh <- event
			}
		}

		// 2. Call LLM
		// In Python ADK, this is `llm_agent.predict_async(llm_req, invocation_context=invocation_ctx)`
		// which internally calls the model connection.
		// Here, we assume LlmAgent has a method to interact with the LLM.
		log.Printf("Sending request to LLM for agent: %s, model: %s", llmAgent.GetName(), llmReq.Model)
		// For debugging: log.Printf("LLM Request Contents: %+v", llmReq.Contents)
		// For debugging: log.Printf("LLM Request System Instruction: %+v", llmReq.Config.SystemInstruction)


		llmRespCh, err := llmAgent.PredictAsync(invocationCtx, llmReq) // Assuming LlmAgent has PredictAsync
		if err != nil {
			log.Printf("Error initiating LLM prediction for agent %s: %v", llmAgent.GetName(), err)
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error calling LLM: %v", err)}}},
				ErrorCode:    "llm_call_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// Wait for LLM response (event containing LlmResponse)
		modelResponseEvent, ok := <-llmRespCh
		if !ok {
			log.Printf("LLM response channel closed unexpectedly for agent %s", llmAgent.GetName())
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: "LLM response channel closed unexpectedly"}}},
				ErrorCode:    "llm_response_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// The event from PredictAsync should contain the LlmResponse.
		// We need a way to extract it. Let's assume Event has an LlmResponse field or a method.
		var llmResponse *LlmResponse
		if modelResponseEvent.LlmResponse != nil { // Assuming Event has a direct LlmResponse field
			llmResponse = modelResponseEvent.LlmResponse
		} else {
			// Fallback: if LlmResponse is embedded in Content.Parts as a special Part type,
			// or if the event itself *is* the LlmResponse structure (less likely for an Event).
			// This part needs clarification based on how PredictAsync structures its output event.
			log.Printf("Warning: LlmResponse not directly found in modelResponseEvent for agent %s. Event: %+v", llmAgent.GetName(), modelResponseEvent)
			// For now, let's assume if LlmResponse is nil, the event content itself is the raw model output.
            // This means the response processors might need to handle raw Content directly.
            // To make it work with current ResponseProcessor interface, we'll construct a dummy LlmResponse if Content exists.
            if modelResponseEvent.Content != nil && llmResponse == nil {
                llmResponse = &LlmResponse{Content: modelResponseEvent.Content}
                 // Also ensure the event itself is passed, as it contains actions, etc.
            } else if llmResponse == nil {
                 log.Printf("Error: No LlmResponse or Content in modelResponseEvent for agent %s.", llmAgent.GetName())
                 outputEventCh <- &Event{
                    ID:           utils.NewEventID(),
                    InvocationID: invocationCtx.InvocationID,
                    Author:       llmAgent.GetName(),
                    Branch:       invocationCtx.Branch,
                    Content:      &Content{Parts: []Part{{Text: "Invalid or empty LLM response event"}}},
                    ErrorCode:    "llm_response_invalid",
                    Timestamp:    utils.GetTimestamp(),
                }
                return
            }
		}


		// 3. Post-LLM processing
		// The Python ADK iterates response_processors and then sends the modelResponseEvent.
		// If processors yield further events, those are sent too.
		// The original modelResponseEvent is sent *after* all processors complete.

		var accumulatedProcessorEvents []*Event

		for _, processor := range f.ResponseProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name()
			log.Printf("Running response processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmResponse, modelResponseEvent) // Pass the original event for context (e.g., actions)
			if err != nil {
				log.Printf("Error running response processor %s: %v", processorName, err)
				// Decide if this error is fatal or if we should continue with other processors
				// For now, accumulate and proceed, then send error event.
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in response processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				})
				continue // Or return if fatal
			}
			for procEvent := range eventCh {
				log.Printf("Event from response processor %s: %v", processorName, procEvent.ID)
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, procEvent)
			}
		}

		// Send the (potentially modified by processors) modelResponseEvent first
		outputEventCh <- modelResponseEvent

		// Then send any events generated by the response processors
		for _, procEvent := range accumulatedProcessorEvents {
			outputEventCh <- procEvent
		}

	}()

	return outputEventCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/contents.go
package processors

import (
	"fmt"
	"log"
	"sort"
	"strings"
	"time"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions" // For REQUEST_EUC_FUNCTION_CALL_NAME & removeClientFunctionCallID
)

// InvocationContext defines the context for a single invocation of an agent.
type InvocationContext struct {
	InvocationID    string
	Agent           LlmAgent // Assuming LlmAgent or a more general Agent interface
	Session         *Session // Assuming this contains Events and State
	Branch          string
	UserContent     *Content
	ArtifactService interface{} // Placeholder for actual ArtifactService type
	// ... other fields like SessionService, MemoryService, RunConfig, etc.
}

// LlmAgent represents an LLM-based agent.
// This is a simplified interface based on usage in the Python code.
type LlmAgent interface {
	GetName() string
	GetIncludeContents() string // "default", "none"
	// ... other methods like GetPlanner, RootAgent, CanonicalInstruction etc.
}

// Session represents a user session.
type Session struct {
	ID     string
	Events []*Event
	State  map[string]interface{}
	AppName string
	UserID  string
	// ... other session fields
}

// Event represents an event in the system.
type Event struct {
	ID            string
	InvocationID  string
	Author        string
	Content       *Content
	Actions       *EventActions
	Timestamp     float64
	Branch        string
	// ... other event fields like TurnComplete, Partial, ErrorCode, ErrorMessage, etc.
}

// GetFunctionCalls extracts function calls from the event's content parts.
func (e *Event) GetFunctionCalls() []FunctionCall {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var calls []FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

// GetFunctionResponses extracts function responses from the event's content parts.
func (e *Event) GetFunctionResponses() []FunctionResponse {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var responses []FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}


// Content represents the content of a message.
type Content struct {
	Role  string
	Parts []Part
}

// Part represents a part of a message's content.
type Part struct {
	Text             string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
	// ... other part types like InlineData, FileData, etc.
}

// FunctionCall represents a function call requested by the LLM.
type FunctionCall struct {
	ID   string
	Name string
	Args map[string]interface{}
}

// FunctionResponse represents the result of a function call.
type FunctionResponse struct {
	ID       string
	Name     string
	Response map[string]interface{}
}

// EventActions represents actions associated with an event.
type EventActions struct {
	StateDelta             map[string]interface{}
	RequestedAuthConfigs map[string]interface{} // Placeholder for actual AuthConfig type
	// ... other actions like SkipSummarization, TransferToAgent, Escalate
}


// LlmRequest represents a request to the LLM.
type LlmRequest struct {
	Model    string
	Contents []Content
	Config   *GenerateContentConfig // Assuming GenerateContentConfig is defined elsewhere
	// ... other fields like LiveConnectConfig, ToolsDict
}

// GenerateContentConfig holds configuration for LLM content generation.
type GenerateContentConfig struct {
	// Define fields based on google.genai.types.GenerateContentConfig
	// For example:
	// Temperature *float32
	// TopP *float32
	// TopK *int32
	// CandidateCount *int32
	// MaxOutputTokens *int32
	// StopSequences []string
	// SafetySettings []*SafetySetting
	// Tools []*Tool
	// SystemInstruction *Content
	// ResponseSchema interface{}
	// ResponseMimeType string
	// ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig
}


// Placeholder, assuming it's defined in the functions package or similar
const REQUEST_EUC_FUNCTION_CALL_NAME = "adk_request_credential"

// ContentLlmRequestProcessor builds the contents for the LLM request.
type ContentLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *ContentLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		if llmAgent.GetIncludeContents() != "none" { // Assuming LlmAgent has GetIncludeContents()
			if invocationCtx.Session == nil {
				log.Println("Error: InvocationContext.Session is nil")
				return
			}
			llmReq.Contents = getContents(
				invocationCtx.Branch,
				invocationCtx.Session.Events, // Assuming Session has Events []*Event
				llmAgent.GetName(),
			)
		}
		// This processor does not yield events.
	}()
	return outCh, nil
}

func rearrangeEventsForAsyncFunctionResponsesInHistory(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	functionCallIDToResponseEventsIndex := make(map[string]int)
	for i, event := range events {
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			for _, fr := range responses {
				functionCallIDToResponseEventsIndex[fr.ID] = i
			}
		}
	}

	var resultEvents []*Event
	processedResponseIndices := make(map[int]bool) // To avoid adding same response event multiple times

	for i, event := range events {
		if _, ok := processedResponseIndices[i]; ok && len(event.GetFunctionResponses()) > 0 { //
			// This response event was already merged with its call, skip it.
			continue //
		}

		if calls := event.GetFunctionCalls(); len(calls) > 0 { //
			resultEvents = append(resultEvents, event) // Add the function call event itself

			var responseEventsToMerge []*Event
			var indicesToMarkProcessed []int

			for _, fc := range calls {
				if responseIdx, ok := functionCallIDToResponseEventsIndex[fc.ID]; ok { //
					isAlreadyProcessedByAnotherCall := false
					for _, resEvent := range resultEvents {
						if resEvent == events[responseIdx] {
							isAlreadyProcessedByAnotherCall = true
							break
						}
						// Also check if the response event's content (parts) were merged into another event.
						// For simplicity, we rely on direct event object comparison or index tracking.
					}
					if !isAlreadyProcessedByAnotherCall && !processedResponseIndices[responseIdx] { //
						responseEventsToMerge = append(responseEventsToMerge, events[responseIdx])
						indicesToMarkProcessed = append(indicesToMarkProcessed, responseIdx)
					}
				}
			}

			if len(responseEventsToMerge) > 0 {
				sort.SliceStable(responseEventsToMerge, func(i, j int) bool { //
					var idxI, idxJ int = -1, -1
					for k, e := range events {
						if e == responseEventsToMerge[i] {
							idxI = k
						}
						if e == responseEventsToMerge[j] {
							idxJ = k
						}
						if idxI != -1 && idxJ != -1 {
							break
						}
					}
					return idxI < idxJ
				})
				mergedRespEvent := mergeFunctionResponseEvents(responseEventsToMerge)
				if mergedRespEvent != nil {
					resultEvents = append(resultEvents, mergedRespEvent)
					for _, idx := range indicesToMarkProcessed { //
						processedResponseIndices[idx] = true
					}
				}
			}
		} else {
			if !(len(event.GetFunctionResponses()) > 0 && processedResponseIndices[i]) { //
				resultEvents = append(resultEvents, event)
			}
		}
	}
	return resultEvents
}

func rearrangeEventsForLatestFunctionResponse(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	lastEvent := events[len(events)-1]
	responsesInLastEvent := lastEvent.GetFunctionResponses()
	if len(responsesInLastEvent) == 0 { //
		return events
	}

	responseIDs := make(map[string]bool)
	for _, fr := range responsesInLastEvent {
		responseIDs[fr.ID] = true
	}

	if len(events) >= 2 { //
		eventBeforeLast := events[len(events)-2]
		callsInEventBeforeLast := eventBeforeLast.GetFunctionCalls()
		allCallsMatched := true
		if len(callsInEventBeforeLast) == len(responseIDs) && len(callsInEventBeforeLast) > 0 { //
			for _, fc := range callsInEventBeforeLast {
				if !responseIDs[fc.ID] {
					allCallsMatched = false
					break
				}
			}
			if allCallsMatched {
				return events
			}
		}
	}

	functionCallEventIdx := -1 //
	for i := len(events) - 2; i >= 0; i-- { //
		event := events[i]
		calls := event.GetFunctionCalls()
		if len(calls) > 0 {
			foundMatch := false
			for _, fc := range calls {
				if responseIDs[fc.ID] {
					foundMatch = true
					for _, otherFc := range calls {
						responseIDs[otherFc.ID] = true
					}
					break
				}
			}
			if foundMatch {
				functionCallEventIdx = i
				break
			}
		}
	}

	if functionCallEventIdx == -1 { //
		log.Printf("Warning: No matching function call event found for function response IDs: %v", getKeys(responseIDs))
		return events //
	}

	var functionResponseEventsToMerge []*Event
	for i := functionCallEventIdx + 1; i < len(events)-1; i++ { //
		event := events[i]
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			isRelevantResponse := false
			for _, fr := range responses {
				if responseIDs[fr.ID] {
					isRelevantResponse = true
					break
				}
			}
			if isRelevantResponse {
				functionResponseEventsToMerge = append(functionResponseEventsToMerge, event)
			}
		}
	}
	functionResponseEventsToMerge = append(functionResponseEventsToMerge, lastEvent)

	resultEvents := make([]*Event, 0, functionCallEventIdx+2) //
	resultEvents = append(resultEvents, events[:functionCallEventIdx+1]...)
	if merged := mergeFunctionResponseEvents(functionResponseEventsToMerge); merged != nil { //
		resultEvents = append(resultEvents, merged)
	}

	return resultEvents
}

func getContents(currentBranch string, historyEvents []*Event, agentName string) []Content {
	var filteredEvents []*Event
	for _, event := range historyEvents {
		if event.Content == nil || len(event.Content.Parts) == 0 { //
			continue
		}
		// Go: we assume an empty text part is still a valid part unless explicitly filtered.
		// For now, let's assume empty text part implies no meaningful content for LLM.
		hasMeaningfulText := false //
		for _, p := range event.Content.Parts {
			if p.Text != "" {
				hasMeaningfulText = true
				break
			}
		}
		if !hasMeaningfulText && len(event.GetFunctionCalls()) == 0 && len(event.GetFunctionResponses()) == 0 { //
			continue
		}

		if !isEventOnBranch(currentBranch, event) {
			continue
		}
		if isAuthEvent(event) {
			continue
		}

		if isOtherAgentReply(agentName, event) {
			filteredEvents = append(filteredEvents, convertForeignEventToGo(event))
		} else {
			filteredEvents = append(filteredEvents, event)
		}
	}

	// Python calls _rearrange_events_for_latest_function_response first, then _rearrange_events_for_async_function_responses_in_history.
	resultEventsStage1 := rearrangeEventsForLatestFunctionResponse(filteredEvents)
	resultEventsStage2 := rearrangeEventsForAsyncFunctionResponsesInHistory(resultEventsStage1)

	var contents []Content
	for _, event := range resultEventsStage2 {
		if event.Content != nil {
			contentCopy := deepCopyContent(*event.Content)
			removeClientFunctionCallID(&contentCopy) // Placeholder
			contents = append(contents, contentCopy)
		}
	}
	return contents
}

func isEventOnBranch(currentBranch string, event *Event) bool {
	// Placeholder for logic to check if event is on the current branch
	// This depends on how Branch is structured and compared.
	if event == nil {
		return false
	}
	return event.Branch == "" || currentBranch == "" || strings.HasPrefix(event.Branch, currentBranch) || strings.HasPrefix(currentBranch, event.Branch)
}

func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return currentAgentName != "" && event.Author != currentAgentName && event.Author != "user"
}

func convertForeignEventToGo(event *Event) *Event {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return event
	}

	newContent := Content{Role: "user"} //
	newContent.Parts = append(newContent.Parts, Part{Text: "For context:"})

	for _, part := range event.Content.Parts {
		if part.Text != "" {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] said: %s", event.Author, part.Text)})
		} else if part.FunctionCall != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] called tool `%s` with parameters: %v", event.Author, part.FunctionCall.Name, part.FunctionCall.Args)})
		} else if part.FunctionResponse != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] `%s` tool returned result: %v", event.Author, part.FunctionResponse.Name, part.FunctionResponse.Response)})
		} else {
			// This part needs careful consideration for how to represent non-text/non-FC/non-FR parts.
			// For simplicity, appending as-is if possible, or a placeholder.
			// newContent.Parts = append(newContent.Parts, part) // This assumes Part is copyable
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] provided non-text data.", event.Author)}) //
		}
	}

	newEvent := *event // Shallow copy for metadata
	newEvent.Author = "user" //
	newEvent.Content = &newContent
	return &newEvent
}

func mergeFunctionResponseEvents(functionResponseEvents []*Event) *Event {
	if len(functionResponseEvents) == 0 {
		return nil
	}
	if len(functionResponseEvents) == 1 {
		return functionResponseEvents[0]
	}

	baseEvent := functionResponseEvents[0]
	// mergedParts := make([]Part, 0) // Not used directly in Go like Python

	// Map to store the latest response part for each function call ID
	latestResponsesForID := make(map[string]Part)
	var nonResponseParts []Part // To collect text parts or other non-FR parts in order

	for _, event := range functionResponseEvents {
		if event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil {
				latestResponsesForID[part.FunctionResponse.ID] = part
			} else {
				nonResponseParts = append(nonResponseParts, part)
			}
		}
	}

	// The Python code implicitly relies on the order of parts in the *first* event
	// if it contained multiple function responses, and then updates them.
	// For Go, if the first event had multiple FCs, its FRs would be the base.
	// If subsequent events provide FRs for *different* FC IDs not in the first event, they are appended.
	// This simplified version will collect all unique FRs based on their latest appearance.
	tempParts := make([]Part, 0, len(latestResponsesForID)+len(nonResponseParts)) //
	tempParts = append(tempParts, nonResponseParts...)                            //
	// To maintain order similar to Python (where original order of FCs in the *first* event matters,
	// and then other unique FRs are appended/merged), we need a more sophisticated merge.
	// For now, this collects unique FRs; their order relative to nonResponseParts and each other might differ from Python's.
	// A better approach would be to iterate through the function calls of the *triggering* event (if available)
	// and append responses in that order, or sort collected FRs by some original call order.

	// Example: Iterate through first event's FRs if they exist as a base order
	if baseEvent.Content != nil {
		for _, part := range baseEvent.Content.Parts {
			if part.FunctionResponse != nil {
				if respPart, ok := latestResponsesForID[part.FunctionResponse.ID]; ok {
					tempParts = append(tempParts, respPart)
					delete(latestResponsesForID, part.FunctionResponse.ID) // Mark as added
				}
			}
		}
	}
	// Append any remaining unique FRs (those not in the first event's call order)
	for _, part := range latestResponsesForID { // Order from map is not guaranteed.
		tempParts = append(tempParts, part)
	}

	mergedEvent := &Event{ //
		ID:           newEventID(), // Assuming newEventID() is defined
		InvocationID: baseEvent.InvocationID,
		Author:       baseEvent.Author,
		Branch:       baseEvent.Branch,
		Content:      &Content{Role: "user", Parts: tempParts}, // Gemini expects FRs to be role "user"
		Actions:      &EventActions{},                          // Initialize to avoid nil pointer if baseEvent.Actions is nil
		Timestamp:    baseEvent.Timestamp,                      // Or use latest timestamp
	}

	// Merge actions from all events (simplified)
	for _, event := range functionResponseEvents {
		if event.Actions != nil && event.Actions.StateDelta != nil { //
			if mergedEvent.Actions.StateDelta == nil {
				mergedEvent.Actions.StateDelta = make(map[string]interface{})
			}
			for k, v := range event.Actions.StateDelta {
				mergedEvent.Actions.StateDelta[k] = v
			}
		}
		// ... merge other actions like RequestedAuthConfigs, ArtifactDelta etc.
	}

	return mergedEvent
}

// newEventID generates a new unique ID for an event.
// This is a placeholder; a robust UUID generation should be used.
func newEventID() string {
	// In a real implementation, use something like github.com/google/uuid
	return fmt.Sprintf("evt_%d", time.Now().UnixNano())
}


func isAuthEvent(event *Event) bool {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return false
	}
	for _, part := range event.Content.Parts {
		if part.FunctionCall != nil && part.FunctionCall.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
		if part.FunctionResponse != nil && part.FunctionResponse.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
	}
	return false
}

// removeClientFunctionCallID placeholder
func removeClientFunctionCallID(content *Content) {
	// In Python, this removes IDs starting with AF_FUNCTION_CALL_ID_PREFIX
	// For Go, this would iterate through content.Parts and nil out IDs if they match a pattern.
	// For example:
	// const afFunctionCallIDPrefix = "adk-"
	// if content == nil || content.Parts == nil {
	// 	return
	// }
	// for i := range content.Parts {
	// 	if content.Parts[i].FunctionCall != nil && strings.HasPrefix(content.Parts[i].FunctionCall.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionCall.ID = "" // Or based on how Gemini handles empty vs nil
	// 	}
	// 	if content.Parts[i].FunctionResponse != nil && strings.HasPrefix(content.Parts[i].FunctionResponse.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionResponse.ID = ""
	// 	}
	// }
}

// deepCopyContent performs a deep copy of the Content struct.
func deepCopyContent(original Content) Content { //
	// A real deep copy is needed here. This is a placeholder.
	// For Go structs, manual copying or a library might be needed for true deep copies.
	copied := original //
	if original.Parts != nil {
		copied.Parts = make([]Part, len(original.Parts))
		for i, p := range original.Parts {
			// Deep copy each part. This is simplified.
			// If Part contains pointers or slices, those need deep copying too.
			copiedPart := p
			if p.FunctionCall != nil {
				fcCopy := *p.FunctionCall
				if p.FunctionCall.Args != nil {
					fcCopy.Args = deepCopyMap(p.FunctionCall.Args)
				}
				copiedPart.FunctionCall = &fcCopy
			}
			if p.FunctionResponse != nil {
				frCopy := *p.FunctionResponse
				if p.FunctionResponse.Response != nil {
					frCopy.Response = deepCopyMap(p.FunctionResponse.Response)
				}
				copiedPart.FunctionResponse = &frCopy
			}
			// Add deep copy for other Part fields if necessary
			copied.Parts[i] = copiedPart
		}
	}
	return copied
}

// deepCopyMap creates a deep copy of a map[string]interface{}.
// This is a simplified version; a robust deep copy library might be better for complex nested structures.
func deepCopyMap(originalMap map[string]interface{}) map[string]interface{} {
	if originalMap == nil {
		return nil
	}
	copyMap := make(map[string]interface{})
	for key, value := range originalMap {
		switch v := value.(type) {
		case map[string]interface{}:
			copyMap[key] = deepCopyMap(v)
		case []interface{}:
			copyMap[key] = deepCopySlice(v)
		default:
			copyMap[key] = v // Assumes primitive types are copyable by value
		}
	}
	return copyMap
}

// deepCopySlice creates a deep copy of a slice of interface{}.
func deepCopySlice(originalSlice []interface{}) []interface{} {
	if originalSlice == nil {
		return nil
	}
	copySlice := make([]interface{}, len(originalSlice))
	for i, value := range originalSlice {
		switch v := value.(type) {
		case map[string]interface{}:
			copySlice[i] = deepCopyMap(v)
		case []interface{}:
			copySlice[i] = deepCopySlice(v)
		default:
			copySlice[i] = v
		}
	}
	return copySlice
}


func getKeys(m map[string]bool) []string { //
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}

// Placeholder for LlmAgent.GetIncludeContents() method.
// This should be part of the LlmAgent interface/struct definition.
// func (a *LlmAgentStruct) GetIncludeContents() string { return a.IncludeContents }

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/instructions.go
package processors

import (
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/utils" // For instructions_utils
)

// ReadonlyContext provides a read-only view of the invocation context.
type ReadonlyContext struct { //
	InvocationContext *InvocationContext
	// Potentially other fields if ReadonlyContext offers more than just InvocationContext access
}


// InstructionsLlmRequestProcessor handles instructions and global instructions for LLM flow.
type InstructionsLlmRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *InstructionsLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in InstructionsLlmRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx} //

		// Append global instructions if set.
		// In Python ADK, global_instruction is on the root agent.
		// Assuming LlmAgent interface has a RootAgent() method that returns the root agent (LlmAgent type).
		var rootLlmAgent LlmAgent
		if ra := llmAgent.RootAgent(); ra != nil {
			var rOk bool
			rootLlmAgent, rOk = ra.(LlmAgent)
			if !rOk {
				log.Println("Warning: Root agent is not an LlmAgent, cannot process global instructions.")
			}
		}


		if rootLlmAgent != nil { //
			// Assuming LlmAgent interface has CanonicalGlobalInstruction
			globalInstructionStr, bypassGlobal, err := rootLlmAgent.CanonicalGlobalInstruction(readonlyCtx) //
			if err != nil {
				log.Printf("Error getting canonical global instruction: %v", err) //
			}
			if globalInstructionStr != "" {
				var processedGlobalInstr string
				if !bypassGlobal {
					processedGlobalInstr, err = injectSessionState(globalInstructionStr, readonlyCtx) //
					if err != nil {
						log.Printf("Error injecting state into global instruction: %v", err) //
						processedGlobalInstr = globalInstructionStr                               // Use raw on error
					}
				} else {
					processedGlobalInstr = globalInstructionStr
				}
				// Assuming LlmRequest has AppendInstructions
				if llmReq.Config == nil {
					llmReq.Config = &GenerateContentConfig{}
				}
				if llmReq.Config.SystemInstruction == nil {
					llmReq.Config.SystemInstruction = &Content{}
				}
				// This needs a proper AppendInstructions method on LlmRequest or its Config
				currentSysInstruction := ""
				if len(llmReq.Config.SystemInstruction.Parts) > 0 {
					currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
				}
				if currentSysInstruction != "" {
					currentSysInstruction += "\n\n"
				}
				llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedGlobalInstr}}

			}
		}

		// Append agent instructions if set.
		// Assuming LlmAgent interface has CanonicalInstruction
		agentInstructionStr, bypassAgent, err := llmAgent.CanonicalInstruction(readonlyCtx) //
		if err != nil {
			log.Printf("Error getting canonical agent instruction: %v", err) //
		}
		if agentInstructionStr != "" {
			var processedAgentInstr string
			if !bypassAgent {
				processedAgentInstr, err = injectSessionState(agentInstructionStr, readonlyCtx) //
				if err != nil {
					log.Printf("Error injecting state into agent instruction: %v", err) //
					processedAgentInstr = agentInstructionStr                             // Use raw on error
				}
			} else {
				processedAgentInstr = agentInstructionStr
			}
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedAgentInstr}}
		}
		// No events are yielded by this specific processor.
	}()
	return outCh, nil
}

// injectSessionState populates values in the instruction template.
// This is a Go equivalent of instructions_utils.inject_session_state.
func injectSessionState(template string, readonlyCtx *ReadonlyContext) (string, error) {
	if readonlyCtx == nil || readonlyCtx.InvocationContext == nil || readonlyCtx.InvocationContext.Session == nil {
		return template, fmt.Errorf("readonlyCtx or its internal fields are nil")
	}
	invocationCtx := readonlyCtx.InvocationContext //

	// Go's regexp package doesn't support direct async replacement functions.
	// This will be synchronous for now.
	// Pattern to find {var_name} or {var_name?} or {artifact.file_name}
	// For simplicity, a basic version is used here. A more robust parser might be needed.
	re := regexp.MustCompile(`{([^}]+)}`) //

	var replacementErr error

	result := re.ReplaceAllStringFunc(template, func(match string) string { //
		if replacementErr != nil {
			return match
		}

		varNameWithMarker := strings.Trim(match, "{} ")
		isOptional := strings.HasSuffix(varNameWithMarker, "?")
		varName := strings.TrimSuffix(varNameWithMarker, "?")

		if strings.HasPrefix(varName, "artifact.") { //
			artifactName := strings.TrimPrefix(varName, "artifact.")
			if invocationCtx.ArtifactService == nil { //
				replacementErr = fmt.Errorf("artifact service is not initialized")
				return match
			}
			// Conceptual: ArtifactService would need a synchronous LoadArtifact or this needs to be async.
			// For this snippet, assuming a conceptual synchronous load or pre-loaded artifact map.
			// This part is commented out as ArtifactService and its methods are not fully defined here.
			/*
				artifactPart, err := invocationCtx.ArtifactService.LoadArtifactSync( //
					invocationCtx.Session.AppName, // Assuming AppName field
					invocationCtx.Session.UserID,  // Assuming UserID field
					invocationCtx.Session.ID,      // Assuming ID field
					artifactName,
				)
				if err != nil {
					if isOptional { return "" }
					replacementErr = fmt.Errorf("artifact '%s' not found: %w", artifactName, err)
					return match
				}
				if artifactPart != nil {
					return artifactPart.Text // Assuming text artifact for simplicity
				}
			*/
			return fmt.Sprintf("[ARTIFACT:%s]", artifactName) // Placeholder
		}

		if !isValidStateName(varName) { //
			return match
		}

		if val, ok := invocationCtx.Session.State[varName]; ok { //
			return fmt.Sprintf("%v", val)
		}
		if isOptional { //
			return ""
		}
		replacementErr = fmt.Errorf("context variable not found: `%s`", varName) //
		return match
	})

	if replacementErr != nil {
		return "", replacementErr
	}
	return result, nil
}

// isValidStateName (simplified) checks if a variable name could be a state key.
func isValidStateName(varName string) bool {
	parts := strings.Split(varName, ":")
	if len(parts) == 1 {
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(varName) //
	}
	if len(parts) == 2 {
		// In Python ADK, prefixes are "app:", "user:", "temp:"
		// Simplified check for now.
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[0]) &&
			regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[1]) //
	}
	return false
}

// LlmAgent interface needs to be fully defined, including these methods for the above to compile:
// RootAgent() BaseAgent // Assuming BaseAgent is a common interface for all agents
// CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error)
// CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error)

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/base_llm_processors.go
package processors

// BaseLlmRequestProcessor defines the interface for processing LLM requests.
type BaseLlmRequestProcessor interface {
	RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error)
}

// BaseLlmResponseProcessor defines the interface for processing LLM responses.
type BaseLlmResponseProcessor interface {
	RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error)
}

// BaseLlmProcessor is a conceptual base for processors if common utility methods were needed.
// In Go, composition or helper functions are often preferred over deep inheritance hierarchies.
// For now, interfaces suffice.

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/basic.go
package processors

import (
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, etc.
)

// BaseLlm defines the interface for a Large Language Model.
type BaseLlm interface {
	ModelName() string
	// Potentially other methods like GenerateContent, Connect etc.
}

// MockLLMForFlow is a placeholder for testing, representing a specific LLM implementation.
type MockLLMForFlow struct {
	ModelName string
}

func (m *MockLLMForFlow) ModelName() string {
	return m.ModelName
}
// Ensure MockLLMForFlow implements BaseLlm
var _ BaseLlm = &MockLLMForFlow{}


// RunConfig defines configuration for an agent run.
type RunConfig struct {
	// Define fields based on google.adk.agents.run_config.RunConfig
	// Example fields (conceptual):
	// ResponseModalities []string
	// SpeechConfig interface{}
	// OutputAudioTranscription bool
	// InputAudioTranscription bool
	// MaxLlmCalls int
	// StreamingMode string
}


// LlmAgent interface part related to BasicRequestProcessor
type LlmAgentForBasicProcessor interface {
	LlmAgent // Embed the general LlmAgent interface
	CanonicalModel() (BaseLlm, error)
	GetGenerateContentConfig() *GenerateContentConfig
	GetOutputSchema() interface{}
}


// BasicRequestProcessor handles basic information to build the LLM request.
type BasicRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *BasicRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) { //
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in BasicRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgentForBasicProcessor) // Use the more specific interface
		if !ok {
			// If agent is not an LlmAgent, this processor might not apply,
			// or it might indicate an issue with the agent setup.
			// For now, we simply return without error.
			log.Println("Error: Agent in InvocationContext is not of type LlmAgentForBasicProcessor or compatible interface")
			return
		}

		canonicalModel, err := llmAgent.CanonicalModel() //
		if err != nil {
			log.Printf("Error getting canonical model for agent %s: %v", llmAgent.GetName(), err) //
			return
		}

		if canonicalModel != nil { //
			llmReq.Model = canonicalModel.ModelName() //
		} else {
			log.Printf("Warning: CanonicalModel is nil for agent %s, model name might not be set correctly in LlmRequest.", llmAgent.GetName()) //
		}


		if cfg := llmAgent.GetGenerateContentConfig(); cfg != nil { //
			cfgCopy := *cfg // Shallow copy
			llmReq.Config = &cfgCopy //
		} else {
			llmReq.Config = &GenerateContentConfig{} // Ensure config is not nil
		}

		if outputSchema := llmAgent.GetOutputSchema(); outputSchema != nil { //
			// Assuming LlmRequest has a SetOutputSchema method
			// llmReq.SetOutputSchema(outputSchema) // This depends on LlmRequest's definition
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			llmReq.Config.ResponseSchema = outputSchema
			llmReq.Config.ResponseMimeType = "application/json" // Typically for structured output

		}

		if invocationCtx.RunConfig != nil { //
			// Conceptual assignments - actual fields depend on RunConfig and LlmRequest.LiveConnectConfig definitions
			// if llmReq.LiveConnectConfig == nil { llmReq.LiveConnectConfig = &LiveConnectConfig{} }
			// llmReq.LiveConnectConfig.ResponseModalities = invocationCtx.RunConfig.ResponseModalities
			// llmReq.LiveConnectConfig.SpeechConfig = invocationCtx.RunConfig.SpeechConfig
			// llmReq.LiveConnectConfig.OutputAudioTranscription = invocationCtx.RunConfig.OutputAudioTranscription
			// llmReq.LiveConnectConfig.InputAudioTranscription = invocationCtx.RunConfig.InputAudioTranscription
			// These would be direct assignments if LiveConnectConfig fields match RunConfig fields.
		}

		// No events are yielded by this specific processor, it only modifies llmReq.
	}()
	return outCh, nil
}

// These GetGenerateContentConfig and GetOutputSchema methods should be part of the LlmAgent implementation.
// Example implementation for a hypothetical LlmAgentStruct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentGenerateContentConfig *GenerateContentConfig
    OutputSchema interface{}
}

func (a *LlmAgentStruct) GetGenerateContentConfig() *GenerateContentConfig { //
    return a.AgentGenerateContentConfig
}

func (a *LlmAgentStruct) GetOutputSchema() interface{} { //
    return a.OutputSchema
}
*/

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/identity.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
)

// IdentityLlmRequestProcessor gives the agent identity from the framework.
type IdentityLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *IdentityLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in IdentityLlmRequestProcessor")
			return
		}
		agent := invocationCtx.Agent // Assuming Agent interface has GetName() and GetDescription()

		var instructions []string
		instructions = append(instructions, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName())) //
		if desc := agent.GetDescription(); desc != "" { //
			instructions = append(instructions, fmt.Sprintf(" The description about you is \"%s\"", desc)) //
		}

		// Assuming LlmRequest has AppendInstructions method or similar mechanism
		if llmReq.Config == nil {
			llmReq.Config = &GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &Content{}
		}
		currentSysInstruction := ""
		if len(llmReq.Config.SystemInstruction.Parts) > 0 {
			currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
		}

		addedInstruction := strings.Join(instructions, "")
		if currentSysInstruction != "" {
			currentSysInstruction += "\n\n" // Ensure separation if there's existing instruction
		}
		llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + addedInstruction}}
		// This processor does not yield events, it modifies llmReq.
	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/nlplanning.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/planners" // For PlanReActPlanner, BuiltInPlanner
	// "github.com/google/generative-ai-go/genai" // For genai.Part, genai.ThinkingConfig
)

// Planner defines the interface for agent planners.
type Planner interface { // [cite: 2133]
	BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) // [cite: 2133]
	ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) // [cite: 2133]
	// ApplyThinkingConfig is specific to BuiltInPlanner in Python, might be part of a more specific interface in Go
	ApplyThinkingConfig(llmReq *LlmRequest)
}

// BuiltInPlanner is a placeholder for planners.BuiltInPlanner [cite: 2133]
type BuiltInPlanner struct {
	ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig [cite: 2133]
}

// ApplyThinkingConfig applies the thinking configuration to the LLM request.
func (bip *BuiltInPlanner) ApplyThinkingConfig(llmReq *LlmRequest) { // [cite: 2133]
	if bip.ThinkingConfig != nil && llmReq.Config != nil {
		// llmReq.Config.ThinkingConfig = bip.ThinkingConfig // This depends on GenerateContentConfig definition
		// For example, if GenerateContentConfig has a field `ThinkingConfig interface{}`
		// then the above line would work.
		log.Println("BuiltInPlanner.ApplyThinkingConfig: ThinkingConfig application is conceptual.")
	}
}
// BuildPlanningInstruction for BuiltInPlanner.
func (bip *BuiltInPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) { // [cite: 2133]
	return "", nil // Python version returns None, so empty string and no error for Go
}
// ProcessPlanningResponse for BuiltInPlanner.
func (bip *BuiltInPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) { // [cite: 2133, 2134]
	return responseParts, nil
}

// PlanReActPlanner is a placeholder for planners.PlanReActPlanner [cite: 2134]
type PlanReActPlanner struct{}

// Constants for PlanReActPlanner tags [cite: 2134]
const (
	PlanningTag    = "/*PLANNING*/"    // [cite: 2134]
	RePlanningTag  = "/*REPLANNING*/"  // [cite: 2134]
	ReasoningTag   = "/*REASONING*/"   // [cite: 2134]
	ActionTag      = "/*ACTION*/"      // [cite: 2134]
	FinalAnswerTag = "/*FINAL_ANSWER*/" // [cite: 2134]
)

// BuildPlanningInstruction for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) {
	// This directly translates the Python _build_nl_planner_instruction method [cite: 2134]
	highLevelPreamble := fmt.Sprintf(` When answering the question, try to leverage the available tools to gather the information instead of your memorized knowledge. [cite: 2134, 2135]
Follow this process when answering the question: (1) first come up with a plan in natural language text format; [cite: 2135, 2136]
(2) Then use tools to execute the plan and provide reasoning between tool code snippets to make a summary of current state and next step. [cite: 2136, 2137]
Tool code snippets and reasoning should be interleaved with each other. (3) In the end, return one final answer. [cite: 2137]
Follow this format when answering the question: (1) The planning part should be under %s. [cite: 2138]
(2) The tool code snippets should be under %s, and the reasoning parts should be under %s. [cite: 2139]
(3) The final answer part should be under %s. `, PlanningTag, ActionTag, ReasoningTag, FinalAnswerTag) // [cite: 2139, 2140]

	planningPreamble := fmt.Sprintf(` Below are the requirements for the planning: The plan is made to answer the user query if following the plan. The plan is coherent and covers all aspects of information from user query, and only involves the tools that are accessible by the agent. The plan contains the decomposed steps as a numbered list where each step should use one or multiple available tools. By reading the plan, you can intuitively know which tools to trigger or what actions to take. [cite: 2140, 2141]
If the initial plan cannot be successfully executed, you should learn from previous execution results and revise your plan. The revised plan should be be under %s. Then use tools to follow the new plan. `, RePlanningTag) // [cite: 2141]

	// The Python original has reasoning_preamble, final_answer_preamble, tool_code_without_python_libraries_preamble, user_input_preamble
	// These would be similarly defined and concatenated. For brevity, I'm showing the pattern.
	// Assuming those other preamble parts are defined similarly:
	reasoningPreamble := ` Below are the requirements for the reasoning: The reasoning makes a summary of the current trajectory based on the user query and tool outputs. Based on the tool outputs and plan, the reasoning also comes up with instructions to the next steps, making the trajectory closer to the final answer. `
	finalAnswerPreamble := ` Below are the requirements for the final answer: The final answer should be precise and follow query formatting requirements. Some queries may not be answerable with the available tools and information. In those cases, inform the user why you cannot process their query and ask for more information. `
	toolCodeWithoutPythonLibrariesPreamble := ` Below are the requirements for the tool code: **Custom Tools:** The available tools are described in the context and can be directly used. - Code must be valid self-contained Python snippets with no imports and no references to tools or Python libraries that are not in the context. - You cannot use any parameters or fields that are not explicitly defined in the APIs in the context. - The code snippets should be readable, efficient, and directly relevant to the user query and reasoning steps. - When using the tools, you should use the library name together with the function name, e.g., vertex_search.search(). - If Python libraries are not provided in the context, NEVER write your own code other than the function calls using the provided tools. `
	userInputPreamble := ` VERY IMPORTANT instruction that you MUST follow in addition to the above instructions: You should ask for clarification if you need more information to answer the question. You should prefer using the information available in the context instead of repeated tool use. `

	return strings.Join([]string{
		highLevelPreamble,
		planningPreamble,
		reasoningPreamble,
		finalAnswerPreamble,
		toolCodeWithoutPythonLibrariesPreamble,
		userInputPreamble,
	}, "\n\n"), nil
}

// markAsThought marks a response part as a thought.
// In Go, `Part` doesn't have a direct `Thought` field like in Python's genai types.
// This might be handled by a wrapper struct or by convention (e.g., specific role or metadata).
// For this conceptual translation, we'll assume a way to mark it, or it's implicit in the planner's logic.
func (prp *PlanReActPlanner) markAsThought(part *Part) {
	// Conceptual: If Part struct had a Thought field: part.Thought = true
	// Or, this could modify the Part's role or add metadata if the Go ADK defines such conventions.
	// For now, this is a no-op unless Part struct is extended.
	log.Printf("Marking part as thought (conceptual): %v", part.Text)
}


// ProcessPlanningResponse for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) {
	if responseParts == nil { // [cite: 2134]
		return nil, nil
	}

	var preservedParts []Part
	firstFCPartIndex := -1

	for i, part := range responseParts {
		if part.FunctionCall != nil { // [cite: 2134]
			if part.FunctionCall.Name == "" {
				continue
			}
			preservedParts = append(preservedParts, part)
			if firstFCPartIndex == -1 {
				firstFCPartIndex = i
			}
		} else {
			// Handle non-function call parts based on tags
			if part.Text != "" {
				// Split by FINAL_ANSWER_TAG first
				if strings.Contains(part.Text, FINAL_ANSWER_TAG) {
					splits := strings.SplitN(part.Text, FINAL_ANSWER_TAG, 2)
					reasoningText := strings.TrimSpace(splits[0])
					finalAnswerText := ""
					if len(splits) > 1 {
						finalAnswerText = strings.TrimSpace(splits[1])
					}

					if reasoningText != "" {
						reasoningPart := Part{Text: reasoningText}
						prp.markAsThought(&reasoningPart) // Conceptual
						preservedParts = append(preservedParts, reasoningPart)
					}
					if finalAnswerText != "" {
						preservedParts = append(preservedParts, Part{Text: finalAnswerText})
					}
				} else {
					// If no FINAL_ANSWER_TAG, check for other thought tags
					isThought := false
					for _, tag := range []string{PlanningTag, ReasoningTag, ActionTag, RePlanningTag} {
						if strings.HasPrefix(part.Text, tag) {
							isThought = true
							break
						}
					}
					currentPart := part
					if isThought {
						prp.markAsThought(&currentPart) // Conceptual
					}
					preservedParts = append(preservedParts, currentPart)
				}
			}
		}
	}
	return preservedParts, nil
}

// ApplyThinkingConfig for PlanReActPlanner (not directly applicable as it doesn't use genai.ThinkingConfig)
func (prp *PlanReActPlanner) ApplyThinkingConfig(llmReq *LlmRequest) {
	// PlanReActPlanner builds instructions directly, doesn't use genai.ThinkingConfig
}


// LlmAgent interface needs to be fully defined for the below to be valid outside this file.
// For example, within an LlmAgent struct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentPlanner Planner
}

func (a *LlmAgentStruct) GetPlanner() Planner {
    return a.AgentPlanner
}
*/

// NlPlanningRequestProcessor handles NL planning for LLM flow.
type NlPlanningRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *NlPlanningRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in NlPlanningRequestProcessor")
			return
		}

		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}
		// Python has: if isinstance(planner, BuiltInPlanner): planner.apply_thinking_config(llm_req)
		// In Go, this would typically be handled by type assertion or checking an interface method.
		if bp, ok := planner.(*BuiltInPlanner); ok { // [cite: 2133]
			bp.ApplyThinkingConfig(llmReq) // [cite: 2133]
		}


		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx}
		instruction, err := planner.BuildPlanningInstruction(readonlyCtx, llmReq) // [cite: 2133]
		if err != nil {
			log.Printf("Error building planning instruction: %v", err)
			// Optionally send an error event or handle error
			return
		}
		if instruction != "" {
			// Assuming LlmRequest has AppendInstructions
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + instruction}}
		}
	}()
	return outCh, nil
}

// NlPlanningResponseProcessor processes LLM responses for NL planning.
type NlPlanningResponseProcessor struct{}

// RunAsync implements the BaseLlmResponseProcessor interface (conceptual).
func (p *NlPlanningResponseProcessor) RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || llmResp == nil || llmResp.Content == nil {
			log.Println("Error: InvocationContext, Agent, or LlmResponse content is nil in NlPlanningResponseProcessor")
			return
		}
		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}

		callbackCtx := &CallbackContext{InvocationContext: invocationCtx, EventActions: modelResponseEvent.Actions} // [cite: 2133]
		if callbackCtx.EventActions == nil { // Ensure EventActions is not nil
		    callbackCtx.EventActions = &EventActions{}
		}


		processedParts, err := planner.ProcessPlanningResponse(callbackCtx, llmResp.Content.Parts) // [cite: 2134]
		if err != nil {
			log.Printf("Error processing planning response: %v", err)
			// Optionally send an error event or handle error
			return
		}

		if processedParts != nil {
			llmResp.Content.Parts = processedParts
		}

		// If callbackCtx.State has changed (meaning planner updated state via EventActions)
		// and an event needs to be yielded. In Python, this is handled by the framework.
		// Here, we might need to explicitly yield an event if the state delta implies one.
		// However, the processor's role is usually to modify the llmResp or yield new events based on llmResp processing.
		// If state changes in callbackCtx are meant to produce a new event, that logic would go here.
		// For now, this processor primarily modifies the llmResp. If an event with updated actions
		// is needed, it would be constructed and sent to outCh.
		// This part of the Python code doesn't explicitly yield an Event from this processor.

	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/auto_flow.go
package llmflows

import (
	"fmt"
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
)

// AutoFlow dynamically selects and runs the appropriate flow for an agent.
// In Go, the dynamic creation of the LlmAgent itself as done in Python's AutoFlow
// (creating an LlmAgent on the fly with specific tools/planners based on config)
// is less straightforward due to static typing.
// Typically, the LlmAgent would already be constructed with its configuration.
// This AutoFlow equivalent will focus on selecting the flow and running it.
type AutoFlow struct {
	// Configuration for AutoFlow, if any, can go here.
	// For example, a map of agent types to flow constructors.
}

// NewAutoFlow creates a new AutoFlow.
func NewAutoFlow() *AutoFlow {
	return &AutoFlow{}
}

// RunAsync executes the appropriate flow for the given agent.
func (af *AutoFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	if invocationCtx == nil || invocationCtx.Agent == nil {
		err := fmt.Errorf("AutoFlow: InvocationContext or Agent is nil")
		log.Println(err)
		// Return a channel that immediately closes or sends an error event
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	agent := invocationCtx.Agent // Agent from InvocationContext

	// The Python AutoFlow can dynamically create an LlmAgent if the provided agent
	// is just a configuration. In Go, we'd expect `agent` to already be a runnable LlmAgent.
	// If `agent` is a configuration struct, a factory would be needed here to build the LlmAgent.

	llmAgent, ok := agent.(LlmAgent) // Assuming the agent in context is already an LlmAgent
	if !ok {
		err := fmt.Errorf("AutoFlow: agent in InvocationContext is not an LlmAgent, but %T", agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	// --- Flow Selection Logic ---
	// In Python, this checks agent.flow or agent.tools, agent.planner etc.
	// For Go, we'll assume a simple logic: if the agent is an LlmAgent, use SingleFlow.
	// More complex logic could be based on LlmAgent's properties or specific type.

	var selectedFlow interface { // Using interface{} to represent a generic Flow type
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	}

	// Example: If LlmAgent has a GetFlowType() method or similar configuration.
	// flowType := llmAgent.GetFlowType() // Hypothetical method
	// switch flowType {
	// case "single":
	//  selectedFlow = NewSingleFlow()
	// case "sequential":
	//  selectedFlow = NewSequentialFlow() // If SequentialFlow exists
	// default:
	//  selectedFlow = NewSingleFlow() // Default
	// }

	// For now, directly use SingleFlow if it's an LlmAgent
	if _, isLlmAgent := agent.(LlmAgent); isLlmAgent {
		log.Printf("AutoFlow: Using SingleFlow for LlmAgent: %s", llmAgent.GetName())
		selectedFlow = NewSingleFlow()
	} else {
		// Handle other agent types if necessary, or default
		err := fmt.Errorf("AutoFlow: Unhandled agent type %T for flow selection. Defaulting to SingleFlow if possible or erroring.", agent)
		log.Println(err)
        // Attempt to use SingleFlow if the base agent can be cast.
        // This part is tricky without knowing all agent types.
        // For robustnes, it's better to ensure llmAgent is correctly typed above.
		log.Println("AutoFlow: Defaulting to SingleFlow (or error if not LlmAgent).")
		selectedFlow = NewSingleFlow() // This will run if the initial cast to LlmAgent succeeded.
	}


	if selectedFlow == nil {
		err := fmt.Errorf("AutoFlow: Could not determine a flow for agent: %s (%T)", llmAgent.GetName(), agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}

	// Type assert selectedFlow to the expected Flow interface/type to call RunAsync
	// This assumes all flows (SingleFlow, etc.) implement this method signature.
	concreteFlow, flowOk := selectedFlow.(interface {
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	})
	if !flowOk {
		err := fmt.Errorf("AutoFlow: Selected flow does not implement the required RunAsync method signature.")
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}


	log.Printf("AutoFlow: Running flow for agent: %s", llmAgent.GetName())
	return concreteFlow.RunAsync(invocationCtx, llmAgent)
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/single_flow.go
package llmflows

import (
	"github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
)

// SingleFlow represents a simple LLM flow with a standard set of processors.
type SingleFlow struct {
	*BaseLlmFlow
}

// NewSingleFlow creates a new SingleFlow.
func NewSingleFlow() *SingleFlow {
	requestProcessors := []processors.BaseLlmRequestProcessor{
		&processors.BasicRequestProcessor{},
		&processors.IdentityLlmRequestProcessor{},
		&processors.ToolLlmRequestProcessor{}, // Assuming ToolLlmRequestProcessor is defined
		&processors.NlPlanningRequestProcessor{},
		&processors.InstructionsLlmRequestProcessor{},
		&processors.ContentLlmRequestProcessor{},
		&processors.CodeExecutionRequestProcessor{}, // Assuming CodeExecutionRequestProcessor is defined
	}

	responseProcessors := []processors.BaseLlmResponseProcessor{
		&processors.NlPlanningResponseProcessor{},
		&processors.FunctionCallResponseProcessor{}, // Assuming FunctionCallResponseProcessor is defined
		&processors.CodeExecutionResponseProcessor{},// Assuming CodeExecutionResponseProcessor is defined
		&processors.ToolUsageResponseProcessor{},    // Assuming ToolUsageResponseProcessor is defined
	}

	baseFlow := NewBaseLlmFlow(requestProcessors, responseProcessors)
	return &SingleFlow{BaseLlmFlow: baseFlow}
}

// Ensure SingleFlow implements an expected Flow interface if one exists
// type Flow interface {
//    RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error)
// }
// var _ Flow = &SingleFlow{}

# /Users/gopher/Desktop/workspace/adk-go/golang_code.txt
# /Users/gopher/Desktop/workspace/adk-go/auth/auth_preprocessor.go
package auth

import (
	"fmt"
	"log"
	"strings"
	"time"
	// "github.com/KennethanCeyer/adk-go/agents/invocation"
	// agentiface "github.com/KennethanCeyer/adk-go/agents/interfaces"
	// eventstypes "github.com/KennethanCeyer/adk-go/events/types"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions"
	// modelstypes "github.com/KennethanCeyer/adk-go/models/types"
	// sessionstypes "github.com/KennethanCeyer/adk-go/sessions/types"
	// authtypes "github.com/KennethanCeyer/adk-go/auth/types"
)

const defaultAuthInstructionTemplate = `
You have access to a set of tools that may require user authorization.
If you need to use a tool that requires authorization, you must first request authorization by calling the "%s" function.
The function call will return one of the following:
- A success message: "Successfully obtained credential."
- An error message: "Failed to obtain credential. <reason>"
- A message indicating that user interaction is required: "User interaction is required to obtain credential. <auth_url>"
Only if the function call returns a success message, you can then proceed to use the tool that requires authorization.
Otherwise, you must inform the user about the error or the required user interaction.

Current user:
<User>
%s
</User>
`

// AuthLlmRequestProcessor handles auth information to build the LLM request.
type AuthLlmRequestProcessor struct{}

// RunAsync implements the processors.BaseLlmRequestProcessor interface.
func (p *AuthLlmRequestProcessor) RunAsync(
	invocationCtx *invocation.InvocationContext,
	llmReq *modelstypes.LlmRequest,
) (<-chan *eventstypes.Event, error) {
	outCh := make(chan *eventstypes.Event)

	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || invocationCtx.Session == nil {
			log.Println("AuthLlmRequestProcessor: InvocationContext, Agent, or Session is nil.")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(agentiface.LlmAgent)
		if !ok {
			log.Printf("AuthLlmRequestProcessor: Agent type assertion to LlmAgent failed for agent: %s", invocationCtx.Agent.GetName())
			return
		}

		// Python: auth_handler = llm_agent.auth_handler
		authHandler := llmAgent.GetAuthHandler() // Assuming LlmAgent interface has GetAuthHandler()
		if authHandler == nil {
			// No auth handler configured for this agent, so nothing to process.
			return
		}

		// Python: auth_configs = auth_handler.get_all_auth_configs()
		authConfigs := authHandler.GetAllAuthConfigs()
		if len(authConfigs) == 0 {
			return
		}

		userInfo := p.buildUserInfo(invocationCtx.Session, authConfigs, authHandler)
		authInstruction := p.buildAuthInstruction(userInfo)

		// Append auth instruction to LLM request
		if llmReq.Config == nil {
			llmReq.Config = &modelstypes.GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &modelstypes.Content{}
		}

		currentSysInstructionText := ""
		if len(llmReq.Config.SystemInstruction.Parts) > 0 {
			if llmReq.Config.SystemInstruction.Parts[0].Text != nil {
				currentSysInstructionText = *llmReq.Config.SystemInstruction.Parts[0].Text
			}
		}

		if currentSysInstructionText != "" {
			currentSysInstructionText += "\n\n"
		}
		newInstructionText := currentSysInstructionText + authInstruction
		llmReq.Config.SystemInstruction.Parts = []modelstypes.Part{{Text: &newInstructionText}}

		// Python: if not auth_handler.is_all_tools_authorized(llm_req.tools_dict):
		// llm_req.tools_dict.update(auth_handler.auth_tool.tools_dict)
		// Go: llmReq.Tools are adk.Tool, AuthTool's tools are also adk.Tool.
		// Need to check if AuthTool needs to be added or if its presence implies specific handling.
		// The `tools_dict` in Python seems to be a dictionary of tools available to the LLM.
		// In Go, `llmReq.Tools` (if such a field exists and holds `[]adk.Tool` or `[]*genai.Tool`)
		// would be the place to add AuthTool.

		// This logic assumes `llmReq.Tools` is a slice of `*genai.Tool` or similar,
		// and `authHandler.GetAuthTool()` returns an `adk.Tool` which needs conversion.
		// For simplicity, let's assume AuthTool is conditionally added if not all existing tools are authorized.
		// The definition of `IsAllToolsAuthorized` and how tools are represented in `llmReq` is crucial here.

		// For now, a conceptual check:
		// if !authHandler.IsAllToolsAuthorized(llmReq.Tools) { // Assuming llmReq.Tools exists
		// 	 authTool := authHandler.GetAuthTool() // Assuming this returns adk.Tool
		// 	 if authTool != nil {
		// 		 // Convert authTool to the format expected by llmReq.Tools and append if not already present.
		// 		 // This part requires knowing the type of llmReq.Tools and how adk.Tool maps to it.
		// 		 // For instance, if llmReq.Tools is []*genai.Tool:
		// 		 // genaiAuthTool := convertADKToolToGenaiTool(authTool) // Placeholder conversion
		// 		 // llmReq.Tools = append(llmReq.Tools, genaiAuthTool)
		//      log.Println("AuthLlmRequestProcessor: AuthTool would be added to LLM request if necessary.")
		// 	 }
		// }
		// The Python `tools_dict.update` implies adding/overwriting. In Go, ensure no duplicates if appending.
		// The exact mechanism for adding tools to `llmReq` depends on `LlmRequest`'s structure.
		// If `LlmRequest` uses `*genai.GenerativeModel` from the `llmproviders/gemini.go` example,
		// then `model.Tools` is set there based on `LlmAgent.Tools()`.
		// So, `AuthTool` should be part of `LlmAgent.Tools()` if needed by the LLM.
		// This processor's role might be more about ensuring the *instructions* are set correctly
		// and that `AuthTool` is available to the agent if any tool requires auth.

		// Check if any recent event contains a function response for auth.
		// If so, it means the auth flow was just completed (or failed).
		// In this case, we might not need to add the general auth instruction again immediately,
		// or we might need to adjust the instruction.
		// Python's `_is_auth_flow_completed_in_prior_turn`
		if p.isAuthFlowCompletedInPriorTurn(invocationCtx.Session.Events) {
			// Potentially modify or skip adding the generic auth instruction if auth just completed.
			// For now, the current logic adds it regardless.
			log.Println("AuthLlmRequestProcessor: Auth flow was recently completed or attempted.")
		}

	}()

	return outCh, nil
}

func (p *AuthLlmRequestProcessor) buildUserInfo(
	session *sessionstypes.Session,
	authConfigs []authtypes.AuthConfig,
	authHandler AuthHandler, // Assuming AuthHandler interface is defined
) string {
	if session == nil {
		return "User information not available."
	}
	var userInfoParts []string
	userInfoParts = append(userInfoParts, fmt.Sprintf("  User ID: %s", session.UserID)) // Assuming UserID field
	userInfoParts = append(userInfoParts, fmt.Sprintf("  App ID: %s", session.AppName))   // Assuming AppName field

	for _, authConfig := range authConfigs {
		// Python: credential = auth_handler.get_credential(auth_config.name, readonly_context)
		// Need ReadonlyContext here. For simplicity, assume session state is enough for now,
		// or GetCredential takes InvocationContext directly.
		// Let's assume GetCredential can work with just the session for this simplified buildUserInfo.
		credential, err := authHandler.GetCredential(authConfig.Name, session) // Simplified
		if err != nil {
			log.Printf("AuthLlmRequestProcessor: Error getting credential for %s: %v", authConfig.Name, err)
		}

		if credential != nil && !credential.IsExpired() { // Assuming AuthCredential has IsExpired
			userInfoParts = append(userInfoParts, fmt.Sprintf("  Authorized for: %s (Scope: %s)", authConfig.Name, authConfig.Scope))
		} else {
			status := "Not authorized"
			if credential != nil && credential.IsExpired() {
				status = "Authorization expired"
			}
			userInfoParts = append(userInfoParts, fmt.Sprintf("  %s for: %s (Scope: %s)", status, authConfig.Name, authConfig.Scope))
		}
	}
	return strings.Join(userInfoParts, "\n")
}

func (p *AuthLlmRequestProcessor) buildAuthInstruction(userInfo string) string {
	return fmt.Sprintf(defaultAuthInstructionTemplate, functions.REQUEST_EUC_FUNCTION_CALL_NAME, userInfo)
}

func (p *AuthLlmRequestProcessor) isAuthFlowCompletedInPriorTurn(historyEvents []*eventstypes.Event) bool {
	if len(historyEvents) == 0 {
		return false
	}
	// Check the last few events for an auth function response
	const turnsToLookBack = 2 // Arbitrary number, Python uses 1 typically
	startIndex := len(historyEvents) - (turnsToLookBack * 2) // Approx user+model turns
	if startIndex < 0 {
		startIndex = 0
	}

	for i := len(historyEvents) - 1; i >= startIndex; i-- {
		event := historyEvents[i]
		if event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil && part.FunctionResponse.Name == functions.REQUEST_EUC_FUNCTION_CALL_NAME {
				return true
			}
			// Also check for model's function call to auth tool in prior model turn
			if part.FunctionCall != nil && part.FunctionCall.Name == functions.REQUEST_EUC_FUNCTION_CALL_NAME {
				// This indicates the model *just* called it, response will be next.
				// Depending on exact definition of "completed", this might also count.
				// For now, only considering the FunctionResponse.
			}
		}
	}
	return false
}


// --- Helper and Interface Stubs (to be defined in their respective packages) ---

// AuthHandler interface stub (should be in auth/types.go or handler file)
type AuthHandler interface {
	GetAllAuthConfigs() []authtypes.AuthConfig
	GetCredential(authConfigName string, session *sessionstypes.Session) (authtypes.AuthCredential, error) // Simplified signature
	// IsAllToolsAuthorized(tools []adk.Tool) bool // Or whatever type llmReq.Tools is
	// GetAuthTool() adk.Tool
}


// Placeholder for newEventID and GetTimestamp (should be in a utils package)
func newEventID() string {
	return fmt.Sprintf("evt-%d", time.Now().UnixNano())
}

func getTimestamp() float64 {
	return float64(time.Now().UnixNano()) / 1e9
}

# /Users/gopher/Desktop/workspace/adk-go/agents/base_agent.go
package agents

import (
	"fmt"
	"regexp"
	// Hypothetical Go ADK packages
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/genai/types" // For genai.Content
	// "github.com/KennethanCeyer/adk-go/telemetry"
)

// Forward declarations for types that would be defined in other packages.
// In a real Go ADK, these would be imported.
type Content struct { // Placeholder for genai.types.Content
	Role  string `json:"role,omitempty"`
	Parts []Part `json:"parts,omitempty"`
}

type Part struct { // Placeholder for genai.types.Part
	Text string `json:"text,omitempty"`
	// Other fields like FunctionCall, FunctionResponse, InlineData would be here
}

type Event struct { // Placeholder for events.Event
	InvocationID string       `json:"invocationId,omitempty"`
	Author       string       `json:"author,omitempty"`
	Branch       string       `json:"branch,omitempty"`
	Content      *Content     `json:"content,omitempty"`
	Actions      EventActions `json:"actions,omitempty"` // Placeholder
	ID           string       `json:"id,omitempty"`
	Timestamp    float64      `json:"timestamp,omitempty"`
	// ... other Event fields
}

type EventActions struct { // Placeholder for events.EventActions
	StateDelta map[string]interface{} `json:"stateDelta,omitempty"`
	// ... other EventActions fields
}

type InvocationContext struct { // Placeholder
	Agent            Agent
	Branch           string
	InvocationID     string
	Session          *Session // Placeholder
	EndInvocation    bool
	UserContent      *Content
	ArtifactService  interface{} // Placeholder
	SessionService   interface{} // Placeholder
	MemoryService    interface{} // Placeholder
	LiveRequestQueue interface{} // Placeholder for LiveRequestQueue
	RunConfig        *RunConfig  // Placeholder
}

type Session struct { // Placeholder for sessions.Session
	State map[string]interface{}
	// ... other Session fields
}

type RunConfig struct { // Placeholder for RunConfig
	// ... RunConfig fields
}

type CallbackContext struct { // Placeholder
	InvocationContext *InvocationContext
	EventActions      EventActions
	State             MutableState // Placeholder for a state map that tracks deltas
}

type MutableState map[string]interface{} // Simplified placeholder

func (ms MutableState) HasDelta() bool {
	// In a real implementation, this would check if _delta has items
	return len(ms) > 0 // Simplified
}

// Agent is the base interface for all agents in the Agent Development Kit.
type Agent interface {
	GetName() string
	GetDescription() string
	ParentAgent() Agent
	SetParentAgent(parent Agent) error
	SubAgents() []Agent
	AddSubAgent(subAgent Agent)
	RunAsync(parentCtx *InvocationContext) (<-chan *Event, error)
	RunLive(parentCtx *InvocationContext) (<-chan *Event, error)
	RootAgent() Agent
	FindAgent(name string) Agent
	FindSubAgent(name string) Agent
	GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback
	GetCanonicalAfterAgentCallbacks() []SingleAgentCallback
}

// SingleAgentCallback defines the signature for agent lifecycle callbacks.
type SingleAgentCallback func(callbackCtx *CallbackContext) (*Content, error)

// BaseAgent provides a common structure for agents.
type BaseAgent struct {
	Name                   string                `json:"name"`
	Description            string                `json:"description,omitempty"`
	Parent                 Agent                 `json:"-"` // Exclude from JSON, handle manually
	Subs                   []Agent               `json:"subAgents,omitempty"`
	BeforeAgentCallback    []SingleAgentCallback `json:"-"` // Callbacks are not typically part of serialized state
	AfterAgentCallback     []SingleAgentCallback `json:"-"`
	agentInvocationContext *InvocationContext    // Internal context for the current run
}

// NewBaseAgent creates a new BaseAgent.
// Name must be a valid Go identifier and not "user".
func NewBaseAgent(name string, description string) (*BaseAgent, error) {
	if !isValidIdentifier(name) {
		return nil, fmt.Errorf("invalid agent name: `%s`. Agent name must be a valid identifier", name)
	}
	if name == "user" {
		return nil, fmt.Errorf("agent name cannot be `user`. `user` is reserved for end-user's input")
	}
	return &BaseAgent{
		Name:        name,
		Description: description,
		Subs:        make([]Agent, 0),
	}, nil
}

func (ba *BaseAgent) GetName() string {
	return ba.Name
}

func (ba *BaseAgent) GetDescription() string {
	return ba.Description
}

func (ba *BaseAgent) ParentAgent() Agent {
	return ba.Parent
}

func (ba *BaseAgent) SetParentAgent(parent Agent) error {
	if ba.Parent != nil && ba.Parent != parent {
		return fmt.Errorf("agent `%s` already has a parent agent: `%s`, cannot add to: `%s`", ba.Name, ba.Parent.GetName(), parent.GetName())
	}
	ba.Parent = parent
	return nil
}

func (ba *BaseAgent) SubAgents() []Agent {
	return ba.Subs
}

func (ba *BaseAgent) AddSubAgent(subAgent Agent) {
	if err := subAgent.SetParentAgent(ba); err != nil {
		// Or handle error as appropriate, e.g. log and skip
		fmt.Printf("Error setting parent for sub-agent %s: %v\n", subAgent.GetName(), err)
		return
	}
	ba.Subs = append(ba.Subs, subAgent)
}

// RunAsync is the entry method to run an agent via text-based conversation.
func (ba *BaseAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	// In Go, we'd typically use channels for async generators.
	// The actual implementation of _runAsyncImpl would push events to this channel.
	// The telemetry.tracer logic would wrap the core processing.
	// For this snippet, we'll simulate the channel.

	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run [%s]", ba.Name)):
		
		event, err := ba.handleBeforeAgentCallback(ctx)
		if err != nil {
			// ì–´ë–»ê²Œ ì—ëŸ¬ë¥¼ outChìœ¼ë¡œ ë³´ë‚¼ ì§€ ê³ ë¯¼ í•„ìš”. Eventì— Error í•„ë“œ ì¶”ê°€?
			// For now, log and potentially send an error event.
			fmt.Printf("Error in beforeAgentCallback for %s: %v\n", ba.Name, err)
			// outCh <- &events.Event{Error: err.Error()} // Conceptual
			return
		}
		if event != nil {
			outCh <- event
		}
		if ctx.EndInvocation {
			return
		}

		err = ba.runAsyncImpl(ctx, outCh) // Pass channel to implementation
		if err != nil {
			fmt.Printf("Error in runAsyncImpl for %s: %v\n", ba.Name, err)
			return
		}

		if ctx.EndInvocation {
			return
		}

		event, err = ba.handleAfterAgentCallback(ctx)
		if err != nil {
			fmt.Printf("Error in afterAgentCallback for %s: %v\n", ba.Name, err)
			return
		}
		if event != nil {
			outCh <- event
		}
	}()

	return outCh, nil
}

// runAsyncImpl is the core logic to run this agent via text-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runAsyncImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	// This is where the Python _run_async_impl's `yield Event(...)` would happen.
	// In Go, we send to the channel.
	// Example: outCh <- &events.Event{Author: ba.Name, Content: ...}
	return fmt.Errorf("_runAsyncImpl for %T is not implemented", ba)
}

// RunLive is the entry method to run an agent via video/audio-based conversation.
func (ba *BaseAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run_live [%s]", ba.Name)):
		err := ba.runLiveImpl(ctx, outCh) // Pass channel
		if err != nil {
			fmt.Printf("Error in runLiveImpl for %s: %v\n", ba.Name, err)
			// Potentially send an error event
		}
	}()
	return outCh, nil
}

// runLiveImpl is the core logic to run this agent via video/audio-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runLiveImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	return fmt.Errorf("_runLiveImpl for %T is not implemented", ba)
}

func (ba *BaseAgent) RootAgent() Agent {
	current := ba
	for current.ParentAgent() != nil {
		current = current.ParentAgent().(*BaseAgent) // Assuming BaseAgent for simplicity
	}
	return current
}

func (ba *BaseAgent) FindAgent(name string) Agent {
	if ba.Name == name {
		return ba
	}
	return ba.FindSubAgent(name)
}

func (ba *BaseAgent) FindSubAgent(name string) Agent {
	for _, subAgent := range ba.Subs {
		if found := subAgent.FindAgent(name); found != nil {
			return found
		}
	}
	return nil
}

func (ba *BaseAgent) createInvocationContext(parentCtx *InvocationContext) *InvocationContext {
	// Create a copy and update agent and branch
	// This is a simplified deep copy. Real implementation needs care.
	ctxCopy := *parentCtx
	ctxCopy.Agent = ba
	if parentCtx.Branch != "" {
		ctxCopy.Branch = fmt.Sprintf("%s.%s", parentCtx.Branch, ba.Name)
	} else {
		ctxCopy.Branch = ba.Name
	}
	return &ctxCopy
}

func (ba *BaseAgent) GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback {
	return ba.BeforeAgentCallback
}

func (ba *BaseAgent) GetCanonicalAfterAgentCallbacks() []SingleAgentCallback {
	return ba.AfterAgentCallback
}

func (ba *BaseAgent) handleBeforeAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalBeforeAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalBeforeAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("beforeAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions, // Assuming EventActions is part of CallbackContext
			}
			ctx.EndInvocation = true
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			Actions:      callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

func (ba *BaseAgent) handleAfterAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalAfterAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalAfterAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("afterAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions,
			}
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			// Content might be nil if callback only changed state
			Actions: callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

// isValidIdentifier checks if a string is a valid Go identifier (simplified).
// Go identifiers are more restrictive than Python's.
func isValidIdentifier(name string) bool {
	if name == "" {
		return false
	}
	// Regex for a simplified Go identifier: starts with a letter, followed by letters or digits.
	// Go allows Unicode letters/digits. This regex is simplified.
	// Actual Go spec: letter { letter | unicode_digit } .
	// For ADK agent names, we might enforce stricter ASCII for interoperability or simplicity.
	matched, _ := regexp.MatchString(`^[a-zA-Z_][a-zA-Z0-9_]*$`, name)
	return matched
}

# /Users/gopher/Desktop/workspace/adk-go/agents/llm_agent.go
package agents

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/tools" // Placeholder for tools package
	// "github.com/KennethanCeyer/adk-go/models" // Placeholder for models package
	// "github.com/KennethanCeyer/adk-go/examples" // Placeholder for examples package
	// "github.com/KennethanCeyer/adk-go/planners" // Placeholder for planners package
	// "github.com/KennethanCeyer/adk-go/codeexecutors" // Placeholder for code executors
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, genai.Content etc.
)

// --- Forward declarations / Placeholders ---
// These would be actual types in a full Go ADK.

type BaseLlm struct { // Placeholder for models.BaseLlm
	ModelName string
}

type GenerateContentConfig struct { // Placeholder for genai.GenerateContentConfig
	SystemInstruction   string        `json:"systemInstruction,omitempty"`
	Tools               []Tool        `json:"tools,omitempty"`         // Placeholder for genai.Tool
	ResponseSchema      interface{}   `json:"responseSchema,omitempty"` // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string        `json:"responseMimeType,omitempty"`
	ThinkingConfig      interface{}   `json:"thinkingConfig,omitempty"`   // Placeholder for genai.ThinkingConfig
	Temperature         *float32      `json:"temperature,omitempty"`
	SafetySettings      []interface{} `json:"safetySettings,omitempty"` // Placeholder for genai.SafetySetting
}

type LlmRequest struct { // Placeholder for models.LlmRequest
	Model               string
	Config              *GenerateContentConfig
	Contents            []Content
	ToolsDict           map[string]Tool // Placeholder
	LiveConnectConfig   interface{}     // Placeholder for genai.LiveConnectConfig
	SystemInstruction   string          // Added for simplicity, original ADK appends to config
	OutputSchema        interface{}     // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string
}

func (lr *LlmRequest) AppendInstructions(instructions []string) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	if lr.Config.SystemInstruction != "" {
		lr.Config.SystemInstruction += "\n\n"
	}
	lr.Config.SystemInstruction += strings.Join(instructions, "\n\n")
}

func (lr *LlmRequest) SetOutputSchema(schema interface{}) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	lr.Config.ResponseSchema = schema
	lr.Config.ResponseMIMEType = "application/json"
}

type LlmResponse struct { // Placeholder for models.LlmResponse
	Content         *Content    `json:"content,omitempty"`
	ErrorCode       string      `json:"errorCode,omitempty"`
	ErrorMessage    string      `json:"errorMessage,omitempty"`
	Partial         bool        `json:"partial,omitempty"`
	CustomMetadata  map[string]interface{} `json:"customMetadata,omitempty"`
}


type Tool interface { // Placeholder for tools.BaseTool
	GetName() string
	GetDescription() string
	GetDeclaration() (*ToolDeclaration, error) // Placeholder for genai.FunctionDeclaration
	ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error
}

type ToolDeclaration struct { // Placeholder for genai.FunctionDeclaration
	Name        string      `json:"name"`
	Description string      `json:"description"`
	Parameters  interface{} `json:"parameters"`
}

type FunctionTool struct { // Placeholder for tools.FunctionTool
	BaseToolImpl
	fn interface{}
}

func NewFunctionTool(fn interface{}) *FunctionTool {
	// Simplified name/description extraction
	name := "unknown_function"
	// In Go, reflection for function name/doc is more involved
	// For now, users might need to provide this explicitly if FunctionTool wraps a raw func
	return &FunctionTool{BaseToolImpl: BaseToolImpl{ToolName: name}, fn: fn}
}

func (ft *FunctionTool) GetName() string { return ft.ToolName }
func (ft *FunctionTool) GetDescription() string { return ft.ToolDescription }
func (ft *FunctionTool) GetDeclaration() (*ToolDeclaration, error) { /* TODO */ return nil, nil }
func (ft *FunctionTool) ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error { /* TODO */ return nil }


type BaseToolset interface { // Placeholder for tools.BaseToolset
	GetTools(readonlyCtx *ReadonlyContext) ([]Tool, error)
}

type Example struct { // Placeholder for examples.Example
	Input  Content   `json:"input"`
	Output []Content `json:"output"`
}

type BaseExampleProvider interface { // Placeholder for examples.BaseExampleProvider
	GetExamples(query string) ([]Example, error)
}

type Planner interface { // Placeholder for planners.BasePlanner
	// Methods would be defined here
}

type CodeExecutor interface { // Placeholder for codeexecutors.BaseCodeExecutor
	// Methods would be defined here
}

type ToolContext struct { // Placeholder for tools.ToolContext
	InvocationContext *InvocationContext
	State             MutableState
	// ... other fields like FunctionCallID, EventActions
}

// Represents ReadonlyContext
type ReadonlyContext struct {
	InvocationContext *InvocationContext
}

func (rc *ReadonlyContext) GetState() map[string]interface{} {
	// Return a read-only copy or wrapper
	// For simplicity, returning the direct map for now, assuming no modification.
	if rc.InvocationContext != nil && rc.InvocationContext.Session != nil {
		return rc.InvocationContext.Session.State
	}
	return make(map[string]interface{})
}


// LLMProvider interface (simplified from previous example)
type LLMProvider interface {
    GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error)
    GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error)
    // Potentially a Connect method for bidirectional streaming if needed for live mode
}

// MockLLMProvider for LlmAgent
type MockLLM struct {
	BaseLlm
}

func (m *MockLLM) GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error) {
	// Simulate LLM call
	log.Printf("MockLLM GenerateContentStream called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	ch := make(chan *LlmResponse, 1)
	go func() {
		defer close(ch)
		// Simplified: just echo back part of the first user message if available
		responseText := "Default mock response"
		if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
			responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
		}
		ch <- &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}
	}()
	return ch, nil
}

func (m *MockLLM) GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error) {
	log.Printf("MockLLM GenerateContent called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	responseText := "Default mock response"
	if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
		responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
	}
	return &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}, nil
}


// SingleBeforeModelCallback defines the signature for a single before-model callback.
type SingleBeforeModelCallback func(callbackCtx *CallbackContext, llmReq *LlmRequest) (*LlmResponse, error)

// SingleAfterModelCallback defines the signature for a single after-model callback.
type SingleAfterModelCallback func(callbackCtx *CallbackContext, llmResp *LlmResponse) (*LlmResponse, error)

// SingleBeforeToolCallback defines the signature for a single before-tool callback.
type SingleBeforeToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext) (map[string]interface{}, error)

// SingleAfterToolCallback defines the signature for a single after-tool callback.
type SingleAfterToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext, toolResponse map[string]interface{}) (map[string]interface{}, error)

// InstructionProvider defines a function that provides an instruction string.
type InstructionProvider func(readonlyCtx *ReadonlyContext) (string, error)

// ToolUnion represents a type that can be a callable, a BaseTool, or a BaseToolset.
// In Go, this would typically be handled by interfaces or by checking types at runtime.
// For this conceptual translation, we'll use `interface{}` and runtime type assertions.
type ToolUnion interface{}

// ExamplesUnion represents types that can provide examples.
type ExamplesUnion interface{} // list[Example] or BaseExampleProvider

// LlmAgent is an LLM-based Agent.
type LlmAgent struct {
	BaseAgent
	AgentModel                  interface{} // string or *BaseLlm (actual model instance)
	AgentInstruction            interface{} // string or InstructionProvider
	AgentGlobalInstruction      interface{} // string or InstructionProvider
	AgentTools                  []ToolUnion
	AgentGenerateContentConfig  *GenerateContentConfig
	DisallowTransferToParent    bool
	DisallowTransferToPeers     bool
	IncludeContents             string // "default" or "none"
	InputSchema                 interface{} // reflect.Type of a struct, or nil
	OutputSchema                interface{} // reflect.Type of a struct, or nil
	OutputKey                   string
	AgentPlanner                Planner
	AgentCodeExecutor           CodeExecutor
	AgentExamples               ExamplesUnion
	AgentBeforeModelCallbacks   []SingleBeforeModelCallback
	AgentAfterModelCallbacks    []SingleAfterModelCallback
	AgentBeforeToolCallbacks    []SingleBeforeToolCallback
	AgentAfterToolCallbacks     []SingleAfterToolCallback

	// Internal flow strategy
	llmFlow LlmFlow
}

// LlmFlow defines the behavior of the LLM agent.
type LlmFlow interface {
	RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error)
	RunLive(invocationCtx *InvocationContext) (<-chan *Event, error)
}

// NewLlmAgent creates a new LlmAgent.
// This constructor attempts to mirror Pydantic's initialization logic.
func NewLlmAgent(name, description string, model interface{}, instruction interface{}) (*LlmAgent, error) {
	base, err := NewBaseAgent(name, description)
	if err != nil {
		return nil, err
	}

	agent := &LlmAgent{
		BaseAgent:        *base,
		AgentModel:       model,
		AgentInstruction: instruction,
		IncludeContents:  "default",
		// Default to AutoFlow which includes SingleFlow + agent transfer
		llmFlow: newAutoFlow(), // Placeholder for flow initialization
	}

	// Simulating Pydantic's model_post_init and field_validator logic
	if err := agent.validateAndProcessConfig(); err != nil {
		return nil, err
	}

	return agent, nil
}

func (a *LlmAgent) validateAndProcessConfig() error {
	if err := a.checkOutputSchema(); err != nil {
		return err
	}
	if err := a.validateGenerateContentConfig(); err != nil {
		return err
	}
	// In Python, Pydantic's model_post_init calls __set_parent_agent_for_sub_agents
	// This is handled by AddSubAgent in Go's BaseAgent.

	// Determine flow based on transfer disallowed flags
	if a.DisallowTransferToParent && a.DisallowTransferToPeers && len(a.Subs) == 0 {
		a.llmFlow = newSingleFlow() // Placeholder
	} else {
		a.llmFlow = newAutoFlow() // Placeholder
	}

	return nil
}


// RunAsync implements the Agent interface.
// It delegates to the internal LlmFlow.
func (a *LlmAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx) // from BaseAgent
	a.agentInvocationContext = ctx // Store context for this agent's run
	return a.llmFlow.RunAsync(ctx)
}

// RunLive implements the Agent interface.
func (a *LlmAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx)
	a.agentInvocationContext = ctx
	return a.llmFlow.RunLive(ctx)
}


func (a *LlmAgent) CanonicalModel() (BaseLlm, error) {
	if modelInst, ok := a.AgentModel.(BaseLlm); ok {
		return modelInst, nil
	}
	if modelStr, ok := a.AgentModel.(string); ok && modelStr != "" {
		// Placeholder for LLMRegistry.NewLlm(modelStr)
		// In a real Go ADK, this would look up and instantiate the model.
		return MockLLM{BaseLlm{ModelName: modelStr}}, nil
	}

	current := a.ParentAgent()
	for current != nil {
		if llmAgent, ok := current.(*LlmAgent); ok {
			return llmAgent.CanonicalModel()
		}
		current = current.ParentAgent()
	}
	return MockLLM{}, fmt.Errorf("no model found for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentInstruction.(func(ctx *ReadonlyContext) (string, error)); ok { // Allow func type as well
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, fmt.Errorf("invalid instruction type for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentGlobalInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentGlobalInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentGlobalInstruction.(func(ctx *ReadonlyContext) (string, error)); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, nil // Global instruction can be empty
}

func (a *LlmAgent) CanonicalTools(ctx *ReadonlyContext) ([]Tool, error) {
	var resolvedTools []Tool
	for _, toolUnion := range a.AgentTools {
		switch t := toolUnion.(type) {
		case Tool:
			resolvedTools = append(resolvedTools, t)
		case func(interface{}) interface{}: // Simplified check for a callable
			// This is a very basic check. Robustly handling arbitrary callables
			// and converting them to tools.Tool would require more reflection
			// or specific registration mechanisms.
			// For now, wrapping it in a conceptual FunctionTool.
			resolvedTools = append(resolvedTools, NewFunctionTool(t))
		case BaseToolset:
			toolsFromSet, err := t.GetTools(ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to get tools from toolset: %w", err)
			}
			resolvedTools = append(resolvedTools, toolsFromSet...)
		default:
			return nil, fmt.Errorf("unsupported tool type: %T", t)
		}
	}
	return resolvedTools, nil
}


func (a *LlmAgent) GetCanonicalBeforeModelCallbacks() []SingleBeforeModelCallback {
	return a.AgentBeforeModelCallbacks
}

func (a *LlmAgent) GetCanonicalAfterModelCallbacks() []SingleAfterModelCallback {
	return a.AgentAfterModelCallbacks
}

func (a *LlmAgent) GetCanonicalBeforeToolCallbacks() []SingleBeforeToolCallback {
	return a.AgentBeforeToolCallbacks
}

func (a *LlmAgent) GetCanonicalAfterToolCallbacks() []SingleAfterToolCallback {
	return a.AgentAfterToolCallbacks
}

func (a *LlmAgent) maybeSaveOutputToState(event *Event) {
	if a.OutputKey == "" || !eventIsFinalResponse(event) || event.Content == nil || len(event.Content.Parts) == 0 {
		return
	}

	var resultData interface{}
	var combinedText []string
	for _, part := range event.Content.Parts {
		if part.Text != "" {
			combinedText = append(combinedText, part.Text)
		}
		// In a real scenario, you'd handle other part types like function calls/responses if they constituted the "output"
	}
	fullText := strings.Join(combinedText, "\n")


	if a.OutputSchema != nil {
		// In Go, Pydantic's model_validate_json would be equivalent to json.Unmarshal into a struct.
		// This requires a.OutputSchema to be a reflect.Type of the target struct.
		// For simplicity, this placeholder assumes fullText is valid JSON for the schema.
		// A real implementation would use json.Unmarshal(byte(fullText), &targetStructInstance)
		// and then potentially convert targetStructInstance to map[string]interface{} if needed.
		var tempMap map[string]interface{}
		// This is a simplified placeholder. Real unmarshalling and validation needed.
		if err := json.Unmarshal([]byte(fullText), &tempMap); err == nil {
			resultData = tempMap
		} else {
			log.Printf("Warning: Failed to parse output for key '%s' against schema: %v. Storing raw text.", a.OutputKey, err)
			resultData = fullText // Fallback to raw text
		}
	} else {
		resultData = fullText
	}

	if event.Actions.StateDelta == nil {
		event.Actions.StateDelta = make(map[string]interface{})
	}
	event.Actions.StateDelta[a.OutputKey] = resultData
}

func (a *LlmAgent) checkOutputSchema() error {
	if a.OutputSchema == nil {
		return nil
	}

	if !a.DisallowTransferToParent || !a.DisallowTransferToPeers {
		log.Println(
			fmt.Sprintf("Warning: Invalid config for agent %s: outputSchema cannot co-exist with agent transfer configurations. Setting disallowTransferToParent=true, disallowTransferToPeers=true", a.Name),
		)
		a.DisallowTransferToParent = true
		a.DisallowTransferToPeers = true
	}

	if len(a.SubAgents()) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, subAgents must be empty to disable agent transfer", a.Name)
	}

	if len(a.AgentTools) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, tools must be empty", a.Name)
	}
	return nil
}

func (a *LlmAgent) validateGenerateContentConfig() error {
	if a.AgentGenerateContentConfig == nil {
		a.AgentGenerateContentConfig = &GenerateContentConfig{} // Ensure it's not nil
		return nil
	}
	if a.AgentGenerateContentConfig.ThinkingConfig != nil {
		return fmt.Errorf("thinkingConfig should be set via LlmAgent.Planner")
	}
	if len(a.AgentGenerateContentConfig.Tools) > 0 {
		return fmt.Errorf("all tools must be set via LlmAgent.Tools")
	}
	if a.AgentGenerateContentConfig.SystemInstruction != "" {
		return fmt.Errorf("system instruction must be set via LlmAgent.Instruction or LlmAgent.GlobalInstruction")
	}
	if a.AgentGenerateContentConfig.ResponseSchema != nil {
		return fmt.Errorf("response schema must be set via LlmAgent.OutputSchema")
	}
	return nil
}

// Helper, would be part of event logic
func eventIsFinalResponse(e *Event) bool {
	if e.Actions.StateDelta["skipSummarization"] == true { // Assuming skipSummarization is stored in StateDelta
		return true
	}
	// Simplified: In Python ADK, this checks for function calls/responses and partial flags.
	// Here, we'll assume an event is final if it's not marked partial and has content.
	return e.Content != nil && !e.Partial // Placeholder for e.Partial
}

// --- Placeholder Flow Implementations ---
// In a real Go ADK, these would be more fleshed out and likely in their own package.

type SingleFlow struct {
	// RequestProcessors  []BaseLlmRequestProcessor  // Placeholder
	// ResponseProcessors []BaseLlmResponseProcessor // Placeholder
}

func newSingleFlow() *SingleFlow {
	// Initialize processors
	return &SingleFlow{}
}

func (sf *SingleFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)
		llmReq := &LlmRequest{} // Initialize LlmRequest

		// Simulate preprocessing (would iterate through sf.RequestProcessors)
		if err := sf.preprocessAsync(invocationCtx, llmReq); err != nil {
			log.Printf("Error during preprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()} // Conceptual error event
			return
		}

		llmAgent, ok := invocationCtx.Agent.(*LlmAgent)
		if !ok {
			log.Printf("Error: agent is not an LlmAgent")
			return
		}
		llm, err := llmAgent.CanonicalModel()
		if err != nil {
			log.Printf("Error getting canonical model: %v", err)
			return
		}
		
		// Simplified LLM call for SingleFlow
		// A real implementation would handle streaming, function calling loops, etc.
		var llmResponse *LlmResponse

		// This is a simplification. The Python ADK has a loop for multiple LLM calls if tools are used.
		// For this Go conceptual translation, we'll assume a single LLM call or a very simplified tool use.
		if provider, ok := llm.(LLMProvider); ok {
			// Use GenerateContent for non-streaming for simplicity in this conceptual flow
			llmResponse, err = provider.GenerateContent(context.Background(), llmReq)
			if err != nil {
				log.Printf("Error calling LLM: %v", err)
				// outCh <- &Event{Error: err.Error()}
				return
			}
		} else {
			log.Printf("Error: LLM model does not implement LLMProvider interface")
			return
		}
		

		modelResponseEvent := &Event{
			ID:            newID(), // Generate new event ID
			InvocationID:  invocationCtx.InvocationID,
			Author:        invocationCtx.Agent.GetName(),
			Branch:        invocationCtx.Branch,
			Content:       llmResponse.Content, // Simplified
			// Actions, ErrorCode, ErrorMessage, etc. would be populated
		}

		// Simulate postprocessing (would iterate through sf.ResponseProcessors)
		if err := sf.postprocessAsync(invocationCtx, llmReq, llmResponse, modelResponseEvent, outCh); err != nil {
			log.Printf("Error during postprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()}
			return
		}
		
		// If not handled by post-processors (e.g. tool calls), yield the direct LLM response event
		if modelResponseEvent.Content != nil || modelResponseEvent.Actions.StateDelta != nil { // Yield if there's something to yield
			outCh <- modelResponseEvent
		}

	}()
	return outCh, nil
}


func (sf *SingleFlow) RunLive(invocationCtx *InvocationContext) (<-chan *Event, error) {
	// Simplified RunLive, a real one would be more complex with bidirectional streaming
	return sf.RunAsync(invocationCtx) // Fallback for now
}

func (sf *SingleFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	// Conceptual: Iterate through request_processors from Python ADK
	// Example: basic.request_processor, instructions.request_processor, etc.
	// For this Go version, we'll call helper methods directly for some core logic.
	agent := invocationCtx.Agent.(*LlmAgent) // Assuming LlmAgent

	// Basic processor logic
	modelInstance, err := agent.CanonicalModel()
	if err != nil {
		return err
	}
	llmReq.Model = modelInstance.ModelName // Assuming BaseLlm has ModelName
	if agent.AgentGenerateContentConfig != nil {
		cfgCopy := *agent.AgentGenerateContentConfig
		llmReq.Config = &cfgCopy
	} else {
		llmReq.Config = &GenerateContentConfig{}
	}
	if agent.OutputSchema != nil {
		llmReq.SetOutputSchema(agent.OutputSchema)
	}
	// ... (add live_connect_config population from RunConfig)

	// Instructions processor logic
	// Global Instruction
	rootAgent := agent.RootAgent().(*LlmAgent) // Assuming LlmAgent
	if rootAgent.AgentGlobalInstruction != nil {
		instr, _, err := rootAgent.CanonicalGlobalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		// Simplified: In Python, instructions_utils.inject_session_state is used.
		// For Go, this would involve a similar template processing.
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	// Agent Instruction
	if agent.AgentInstruction != nil {
		instr, _, err := agent.CanonicalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	
	// Identity processor
	var si []string
	si = append(si, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName()))
	if agent.GetDescription() != "" {
		si = append(si, fmt.Sprintf(" The description about you is \"%s\"", agent.GetDescription()))
	}
	llmReq.AppendInstructions(si)

	// Contents processor logic (simplified)
	if agent.IncludeContents != "none" {
		llmReq.Contents = getContentsForLlm(invocationCtx.Branch, invocationCtx.Session.Events, agent.GetName())
	}

	// Tool processing (adding declarations to llmReq.Config.Tools)
	tools, err := agent.CanonicalTools(&ReadonlyContext{InvocationContext: invocationCtx})
	if err != nil { return err}
	if len(tools) > 0 {
		if llmReq.Config.Tools == nil {
			llmReq.Config.Tools = make([]Tool, 0)
		}
	}
	for _, tool := range tools {
		// Simplified: directly appending, Python ADK has more complex logic for FunctionDeclaration
		llmReq.Config.Tools = append(llmReq.Config.Tools, tool) // This assumes tool itself is the declaration or can provide it
		
		// Store tool in llmReq.ToolsDict for later execution lookup by name
		if llmReq.ToolsDict == nil {
			llmReq.ToolsDict = make(map[string]Tool)
		}
		llmReq.ToolsDict[tool.GetName()] = tool
	}

	return nil
}

func (sf *SingleFlow) postprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest, llmResp *LlmResponse, modelResponseEvent *Event, outCh chan<- *Event) error {
	// Conceptual: Iterate through response_processors
	// Example: _nl_planning.response_processor, _code_execution.response_processor
	
	// Handle function calls (simplified from functions.py)
	if llmResp.Content != nil {
		var functionCalls []struct{ Name string; Args map[string]interface{}; ID string } // Simplified representation
		for _, part := range llmResp.Content.Parts {
			// Placeholder: In Python this uses part.function_call
			// For Go, we'd check a similar field if LlmResponse.Content.Parts had FunctionCall type
			// For this example, assume FunctionCall is identified by a specific structure in Part
			// For now, this logic is highly simplified and conceptual
			if strings.HasPrefix(part.Text, "CALL_TOOL:") { // Very basic trigger
				parts := strings.SplitN(strings.TrimPrefix(part.Text, "CALL_TOOL:"), "(", 2)
				name := parts[0]
				argsStr := ""
				if len(parts) > 1 {
					argsStr = strings.TrimSuffix(parts[1], ")")
				}
				var args map[string]interface{}
				json.Unmarshal([]byte(argsStr), &args) // Simplified arg parsing
				functionCalls = append(functionCalls, struct{ Name string; Args map[string]interface{}; ID string }{Name: name, Args: args, ID: newID()})

				// Populate client function call ID if LLM didn't provide one
				modelResponseEvent.Content.Parts[0].Text = "" // Clear text part conceptually
				// modelResponseEvent.Content.Parts[0].FunctionCall = &genai.FunctionCall{Name: name, Args: args, ID: ...}
			}
		}

		if len(functionCalls) > 0 {
			// In Python ADK, this calls functions.handle_function_calls_async
			// which then calls tool.run_async and creates a response event.
			// This is a very complex part involving callbacks, state updates, etc.
			// Here's a highly simplified conceptual flow:
			var toolResponseParts []Part
			for _, fc := range functionCalls {
				tool, ok := llmReq.ToolsDict[fc.Name]
				if !ok {
					log.Printf("Tool %s not found", fc.Name)
					toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Error: Tool %s not found", fc.Name)})
					continue
				}
				// Conceptual tool execution
				// toolResult, err := tool.Execute(context.Background(), fc.Args) // This is complex
				// Simplified:
				toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Tool %s called with %v (mocked)", fc.Name, fc.Args)})
			}
			
			// Create a new event for tool responses and send it
			toolResponseEvent := &Event{
				ID:            newID(),
				InvocationID:  invocationCtx.InvocationID,
				Author:        invocationCtx.Agent.GetName(),
				Branch:        invocationCtx.Branch,
				Content:       &Content{Role: "model", Parts: toolResponseParts}, // Simplified: role should be 'user' for function response in Gemini
			}
			outCh <- toolResponseEvent
			
			// The original LLM response event (modelResponseEvent) that *contained* the function call
			// is also yielded. Then, the flow would typically make *another* LLM call with the tool responses.
			// For simplicity, we will stop here in this conceptual snippet.
			// The python ADK has a loop in _run_one_step_async for this.
			modelResponseEvent.Content = nil // After tool call, the original content is usually superseded or followed by tool response processing
		}
	}
	
	return nil
}

type AutoFlow struct {
	SingleFlow
	// agentTransferRequestProcessor // Placeholder for agent_transfer.request_processor
}

func newAutoFlow() *AutoFlow {
	return &AutoFlow{SingleFlow: *newSingleFlow()}
}

func (af *AutoFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	if err := af.SingleFlow.preprocessAsync(invocationCtx, llmReq); err != nil {
		return err
	}
	// Agent transfer logic
	agent := invocationCtx.Agent.(*LlmAgent)
	transferTargets := getTransferTargets(agent)
	if len(transferTargets) > 0 {
		llmReq.AppendInstructions([]string{buildTargetAgentsInstructions(agent, transferTargets)})
		// Conceptually add transfer_to_agent tool
		// transferTool := NewFunctionTool(transferToAgent) // transferToAgent needs to be defined
		// ... add transferTool to llmReq.Config.Tools and llmReq.ToolsDict
	}
	return nil
}

// Helper to simulate Python's list comprehension and filtering for contents
func getContentsForLlm(currentBranch string, allEvents []*Event, agentName string) []Content {
	var contents []Content
	// This is a simplified version of Python ADK's _get_contents, which has complex logic
	// for rearranging events, handling async function responses, and filtering by branch.
	for _, event := range allEvents {
		if event.Content != nil && len(event.Content.Parts) > 0 {
			// Simplified branch check and foreign event conversion
			if isEventOnBranch(currentBranch, event) && !isAuthEvent(event) {
				if isOtherAgentReply(agentName, event) {
					contents = append(contents, *convertForeignEvent(event))
				} else {
					// Deep copy content before modifying (e.g. removing client func call ID)
					contentCopy := *event.Content 
					// removeClientFunctionCallID(&contentCopy) // Placeholder for actual logic
					contents = append(contents, contentCopy)
				}
			}
		}
	}
	return contents
}

// Simplified placeholders for helper functions from Python's contents.py
func isEventOnBranch(invocationBranch string, event *Event) bool { 
	if invocationBranch == "" || event.Branch == "" { return true }
	return strings.HasPrefix(invocationBranch, event.Branch)
}
func isAuthEvent(event *Event) bool { return false /* TODO */ }
func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return event.Author != currentAgentName && event.Author != "user"
}
func convertForeignEvent(event *Event) *Content { 
	// Simplified conversion
	return &Content{Role: "user", Parts: []Part{{Text: fmt.Sprintf("Context from %s: %s", event.Author, event.Content.Parts[0].Text)}}}
}
func newID() string { return "temp-id" } // Placeholder for ID generation

func getTransferTargets(agent *LlmAgent) []Agent {
    var targets []Agent
    targets = append(targets, agent.SubAgents()...) // Assuming SubAgents returns []Agent

    if agent.ParentAgent() != nil {
        if llmParent, ok := agent.ParentAgent().(*LlmAgent); ok {
            if !agent.DisallowTransferToParent {
                targets = append(targets, llmParent)
            }
            if !agent.DisallowTransferToPeers {
                for _, peerAgent := range llmParent.SubAgents() {
                    if peerAgent.GetName() != agent.GetName() {
                        targets = append(targets, peerAgent)
                    }
                }
            }
        }
    }
    return targets
}

func buildTargetAgentsInfo(targetAgent Agent) string {
    return fmt.Sprintf("\nAgent name: %s\nAgent description: %s\n", targetAgent.GetName(), targetAgent.GetDescription())
}

func buildTargetAgentsInstructions(agent *LlmAgent, targetAgents []Agent) string {
    var sb strings.Builder
    sb.WriteString("\nYou have a list of other agents to transfer to:\n")
    for _, target := range targetAgents {
        sb.WriteString(buildTargetAgentsInfo(target))
    }
    sb.WriteString("\nIf you are the best to answer the question according to your description, you can answer it.")
    sb.WriteString("\nIf another agent is better for answering the question according to its description, call `transfer_to_agent` function to transfer the question to that agent. When transferring, do not generate any text other than the function call.\n")

    if agent.ParentAgent() != nil {
        sb.WriteString(fmt.Sprintf("\nYour parent agent is %s. If neither the other agents nor you are best for answering the question according to the descriptions, transfer to your parent agent. If you don't have parent agent, try answer by yourself.\n", agent.ParentAgent().GetName()))
    }
    return sb.String()
}

// Placeholder for instruction template processing
func processInstructionTemplate(template string, ctx *ReadonlyContext) (string, error) {
	// In Python, this uses string.Formatter with a custom class to access state.
	// In Go, this would require a more explicit template engine or string replacement.
	// This is a very simplified version.
	processed := template
	// Regex to find {key} or {key?}
	re := regexp.MustCompile(`{([^}]+)}`)
	state := ctx.GetState() // Assuming GetState returns map[string]interface{}

	processed = re.ReplaceAllStringFunc(processed, func(match string) string {
		keyWithOptionalMarker := strings.Trim(match, "{} ")
		key := strings.TrimSuffix(keyWithOptionalMarker, "?")
		isOptional := strings.HasSuffix(keyWithOptionalMarker, "?")

		// Handle artifact.filename format
		if strings.HasPrefix(key, "artifact.") {
			// artifactName := strings.TrimPrefix(key, "artifact.")
			// artifactContent, err := loadArtifact(ctx.InvocationContext, artifactName) // Placeholder
			// if err != nil {
			// 	if isOptional { return "" }
			// 	log.Printf("Error loading artifact %s: %v", artifactName, err)
			// 	return match // return original on error
			// }
			// return artifactContent
			return "[ARTIFACT_CONTENT_PLACEHOLDER]" // Simplified
		}

		val, ok := state[key]
		if ok {
			return fmt.Sprintf("%v", val)
		}
		if isOptional {
			return ""
		}
		log.Printf("Warning: Context variable not found and not optional: `%s`", key)
		return match // Return original placeholder if not found and not optional
	})
	return processed, nil
}

# /Users/gopher/Desktop/workspace/adk-go/README.md
# ADK-Go: Agent Development Kit in Go (Conceptual & Executable Example)

[![Go Reference](https://pkg.go.dev/badge/github.com/KennethanCeyer/adk-go.svg)](https://pkg.go.dev/github.com/KennethanCeyer/adk-go)

This repository provides a conceptual and partially executable example of porting the core ideas of an Agent Development Kit (ADK) from Python to Go. The primary goal is to demonstrate how agent functionalities, tool usage, and LLM interactions can be structured in Go, rather than a direct 1:1 port of the entire Python ADK.

This README focuses on a **minimal, executable "Hello World" agent** that uses a simple tool and interacts with a real LLM (Google Gemini).

## Project Structure (Conceptual)

adk-go/
â”œâ”€â”€ cmd/
â”‚ â””â”€â”€ helloworld_runner/ # Executable for the HelloWorld agent
â”‚ â””â”€â”€ main.go
â”œâ”€â”€ pkg/
â”‚ â”œâ”€â”€ adk/ # Core ADK interfaces and simple agent runner
â”‚ â”‚ â”œâ”€â”€ agent.go # Agent, Tool, LLMProvider interfaces
â”‚ â”‚ â”œâ”€â”€ runner.go # Simple CLI runner
â”‚ â”‚ â””â”€â”€ types.go # Core data structures (Message, Part, etc.)
â”‚ â”œâ”€â”€ llmproviders/ # LLM interaction implementations
â”‚ â”‚ â””â”€â”€ gemini.go # Google Gemini LLM provider
â”‚ â””â”€â”€ tools/ # Example tool implementations
â”‚ â””â”€â”€ rolldie.go
â”œâ”€â”€ examples/
â”‚ â””â”€â”€ helloworld/ # HelloWorld agent definition
â”‚ â””â”€â”€ agent.go
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ README.md

## Prerequisites

1.  **Go**: Version 1.20 or later. ([Installation Guide](https://go.dev/doc/install))
2.  **Google Cloud Project**: A Google Cloud Project with the Vertex AI API (or Generative Language API) enabled.
3.  **API Key**: An API key for Google Gemini. You can obtain this from Google AI Studio or your Google Cloud console.
    - **Important**: Treat your API key like a password. Do not commit it to your repository. Use environment variables.
4.  **Environment Variable**: Set the `GEMINI_API_KEY` environment variable:
    ```bash
    export GEMINI_API_KEY="YOUR_API_KEY_HERE"
    ```

## Core Concepts & Types (pkg/adk/types.go, pkg/adk/agent.go)

The core of this simplified ADK-Go revolves around a few key interfaces and structs:

- **`Message`**: Represents a message in the conversation (user or model).
  ```go
  type Message struct {
      Role    string `json:"role"` // "user" or "model"
      Parts   []Part `json:"parts"`
  }
  ```
- **`Part`**: Represents a piece of content in a message (text, function call, function response).
  ```go
  type Part struct {
      Text             *string           `json:"text,omitempty"`
      FunctionCall     *FunctionCall     `json:"functionCall,omitempty"`
      FunctionResponse *FunctionResponse `json:"functionResponse,omitempty"`
  }
  ```
- **`FunctionCall`**: Represents an LLM's request to call a tool.
  ```go
  type FunctionCall struct {
      Name string `json:"name"`
      Args any    `json:"args"` // Usually map[string]any
  }
  ```
- **`FunctionResponse`**: Represents the result of a tool execution.
  ```go
  type FunctionResponse struct {
      Name     string `json:"name"`
      Response any    `json:"response"` // Usually map[string]any
  }
  ```
- **`Tool` Interface**: Defines the contract for any tool an agent can use.
  ```go
  type Tool interface {
      Name() string
      Description() string
      Parameters() any // JSON schema for parameters
      Execute(ctx context.Context, args any) (any, error)
  }
  ```
- **`LLMProvider` Interface**: Defines how to interact with an LLM.
  ```go
  type LLMProvider interface {
      GenerateContent(
          ctx context.Context,
          modelName string,
          systemInstruction string,
          tools []Tool, // Tools available to the LLM
          history []Message, // Conversation history
          latestMessage Message, // The latest user input or tool response to process
      ) (Message, error) // Returns the LLM's response message
  }
  ```
- **`Agent` Interface**: Defines the core agent logic.
  ```go
  type Agent interface {
      Name() string
      SystemInstruction() string
      Tools() []Tool
      ModelName() string
      LLMProvider() LLMProvider
      Process(ctx context.Context, history []Message, latestMessage Message) (Message, error)
  }
  ```

## Setup & Running the HelloWorld Agent

1.  **Clone the Repository (or create files as below):**
    If this code were in a Git repository:

    ```bash
    # git clone [https://github.com/KennethanCeyer/adk-go.git](https://github.com/KennethanCeyer/adk-go.git)
    # cd adk-go
    ```

    For now, you'll be creating these files manually based on the code snippets.

2.  **Initialize Go Module:**
    In the root directory of your project (`adk-go`), run:

    ```bash
    go mod init [github.com/KennethanCeyer/adk-go](https://github.com/KennethanCeyer/adk-go)
    ```

3.  **Install Dependencies:**
    The Gemini LLM provider will use Google's generative AI Go SDK.

    ```bash
    go get google.golang.org/api/iterator \
           [cloud.google.com/go/vertexai/apiv1/vertexaipb](https://cloud.google.com/go/vertexai/apiv1/vertexaipb) \
           [github.com/google/generative-ai-go/genai](https://github.com/google/generative-ai-go/genai)
    ```

    (Note: `google.golang.org/api/iterator` and `cloud.google.com/go/vertexai/apiv1/vertexaipb` might be indirect dependencies of `genai` or specific to Vertex AI usage. The `genai` package is the primary one for Gemini directly.)

4.  **Create the Code Files:**
    Create the directory structure and files as outlined in the "Project Structure" section. Populate them with the Go code provided in the subsequent sections of this response.

5.  **Set your `GEMINI_API_KEY` environment variable** as described in "Prerequisites".

6.  **Run the HelloWorld Agent:**

    ```bash
    go run ./cmd/helloworld_runner/main.go
    ```

7.  **Interact with the Agent:**
    The runner will prompt you for input. Try typing:
    - `Hello`
    - `Can you roll a 6-sided die for me?`
    - `Roll a d20`
    - Type `exit` or `quit` to terminate.

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/base_llm_flow.go
package llmflows

import (
	"fmt"
	"log"
	"reflect"
	"runtime"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/sessions"
	// "github.com/KennethanCeyer/adk-go/utils"
)

// Placeholder types from assumed packages (if not already fully defined in processors)
type InvocationContext = processors.InvocationContext
type LlmRequest = models.LlmRequest
type LlmResponse = models.LlmResponse
type Event = events.Event
type EventActions = events.EventActions
type LlmAgent = agents.LlmAgent // This should be the primary agent interface
type Content = models.Content   // Assuming models package defines Content, Part etc.
type Part = models.Part
type FunctionCall = models.FunctionCall
type FunctionResponse = models.FunctionResponse
type State = sessions.State
type RunConfig = agents.RunConfig // Assuming RunConfig is in agents

// BaseLlmFlow is the base for LLM-based flows.
type BaseLlmFlow struct {
	RequestProcessors  []processors.BaseLlmRequestProcessor
	ResponseProcessors []processors.BaseLlmResponseProcessor
}

// NewBaseLlmFlow creates a new BaseLlmFlow.
func NewBaseLlmFlow(
	requestProcessors []processors.BaseLlmRequestProcessor,
	responseProcessors []processors.BaseLlmResponseProcessor) *BaseLlmFlow {
	return &BaseLlmFlow{
		RequestProcessors:  requestProcessors,
		ResponseProcessors: responseProcessors,
	}
}

// RunAsync executes the LLM flow.
func (f *BaseLlmFlow) RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error) {
	outputEventCh := make(chan *Event)

	go func() {
		defer close(outputEventCh)

		llmReq := &LlmRequest{} // Initialize with appropriate defaults if any
		if llmAgent.GetGenerateContentConfig() != nil { // Assuming LlmAgent has GetGenerateContentConfig
			llmReq.Config = llmAgent.GetGenerateContentConfig()
		} else {
			llmReq.Config = &models.GenerateContentConfig{} // Ensure config is not nil
		}


		// 1. Pre-LLM processing
		for _, processor := range f.RequestProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name() // Get processor name for logging
			log.Printf("Running request processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmReq)
			if err != nil {
				log.Printf("Error running request processor %s: %v", processorName, err)
				outputEventCh <- &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in request processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				}
				return
			}
			// Drain events from processor if any (though these processors typically modify llmReq)
			for event := range eventCh {
				log.Printf("Event from request processor %s: %v", processorName, event.ID)
				outputEventCh <- event
			}
		}

		// 2. Call LLM
		// In Python ADK, this is `llm_agent.predict_async(llm_req, invocation_context=invocation_ctx)`
		// which internally calls the model connection.
		// Here, we assume LlmAgent has a method to interact with the LLM.
		log.Printf("Sending request to LLM for agent: %s, model: %s", llmAgent.GetName(), llmReq.Model)
		// For debugging: log.Printf("LLM Request Contents: %+v", llmReq.Contents)
		// For debugging: log.Printf("LLM Request System Instruction: %+v", llmReq.Config.SystemInstruction)


		llmRespCh, err := llmAgent.PredictAsync(invocationCtx, llmReq) // Assuming LlmAgent has PredictAsync
		if err != nil {
			log.Printf("Error initiating LLM prediction for agent %s: %v", llmAgent.GetName(), err)
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error calling LLM: %v", err)}}},
				ErrorCode:    "llm_call_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// Wait for LLM response (event containing LlmResponse)
		modelResponseEvent, ok := <-llmRespCh
		if !ok {
			log.Printf("LLM response channel closed unexpectedly for agent %s", llmAgent.GetName())
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: "LLM response channel closed unexpectedly"}}},
				ErrorCode:    "llm_response_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// The event from PredictAsync should contain the LlmResponse.
		// We need a way to extract it. Let's assume Event has an LlmResponse field or a method.
		var llmResponse *LlmResponse
		if modelResponseEvent.LlmResponse != nil { // Assuming Event has a direct LlmResponse field
			llmResponse = modelResponseEvent.LlmResponse
		} else {
			// Fallback: if LlmResponse is embedded in Content.Parts as a special Part type,
			// or if the event itself *is* the LlmResponse structure (less likely for an Event).
			// This part needs clarification based on how PredictAsync structures its output event.
			log.Printf("Warning: LlmResponse not directly found in modelResponseEvent for agent %s. Event: %+v", llmAgent.GetName(), modelResponseEvent)
			// For now, let's assume if LlmResponse is nil, the event content itself is the raw model output.
            // This means the response processors might need to handle raw Content directly.
            // To make it work with current ResponseProcessor interface, we'll construct a dummy LlmResponse if Content exists.
            if modelResponseEvent.Content != nil && llmResponse == nil {
                llmResponse = &LlmResponse{Content: modelResponseEvent.Content}
                 // Also ensure the event itself is passed, as it contains actions, etc.
            } else if llmResponse == nil {
                 log.Printf("Error: No LlmResponse or Content in modelResponseEvent for agent %s.", llmAgent.GetName())
                 outputEventCh <- &Event{
                    ID:           utils.NewEventID(),
                    InvocationID: invocationCtx.InvocationID,
                    Author:       llmAgent.GetName(),
                    Branch:       invocationCtx.Branch,
                    Content:      &Content{Parts: []Part{{Text: "Invalid or empty LLM response event"}}},
                    ErrorCode:    "llm_response_invalid",
                    Timestamp:    utils.GetTimestamp(),
                }
                return
            }
		}


		// 3. Post-LLM processing
		// The Python ADK iterates response_processors and then sends the modelResponseEvent.
		// If processors yield further events, those are sent too.
		// The original modelResponseEvent is sent *after* all processors complete.

		var accumulatedProcessorEvents []*Event

		for _, processor := range f.ResponseProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name()
			log.Printf("Running response processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmResponse, modelResponseEvent) // Pass the original event for context (e.g., actions)
			if err != nil {
				log.Printf("Error running response processor %s: %v", processorName, err)
				// Decide if this error is fatal or if we should continue with other processors
				// For now, accumulate and proceed, then send error event.
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in response processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				})
				continue // Or return if fatal
			}
			for procEvent := range eventCh {
				log.Printf("Event from response processor %s: %v", processorName, procEvent.ID)
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, procEvent)
			}
		}

		// Send the (potentially modified by processors) modelResponseEvent first
		outputEventCh <- modelResponseEvent

		// Then send any events generated by the response processors
		for _, procEvent := range accumulatedProcessorEvents {
			outputEventCh <- procEvent
		}

	}()

	return outputEventCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/contents.go
package processors

import (
	"fmt"
	"log"
	"sort"
	"strings"
	"time"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions" // For REQUEST_EUC_FUNCTION_CALL_NAME & removeClientFunctionCallID
)

// InvocationContext defines the context for a single invocation of an agent.
type InvocationContext struct {
	InvocationID    string
	Agent           LlmAgent // Assuming LlmAgent or a more general Agent interface
	Session         *Session // Assuming this contains Events and State
	Branch          string
	UserContent     *Content
	ArtifactService interface{} // Placeholder for actual ArtifactService type
	// ... other fields like SessionService, MemoryService, RunConfig, etc.
}

// LlmAgent represents an LLM-based agent.
// This is a simplified interface based on usage in the Python code.
type LlmAgent interface {
	GetName() string
	GetIncludeContents() string // "default", "none"
	// ... other methods like GetPlanner, RootAgent, CanonicalInstruction etc.
}

// Session represents a user session.
type Session struct {
	ID     string
	Events []*Event
	State  map[string]interface{}
	AppName string
	UserID  string
	// ... other session fields
}

// Event represents an event in the system.
type Event struct {
	ID            string
	InvocationID  string
	Author        string
	Content       *Content
	Actions       *EventActions
	Timestamp     float64
	Branch        string
	// ... other event fields like TurnComplete, Partial, ErrorCode, ErrorMessage, etc.
}

// GetFunctionCalls extracts function calls from the event's content parts.
func (e *Event) GetFunctionCalls() []FunctionCall {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var calls []FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

// GetFunctionResponses extracts function responses from the event's content parts.
func (e *Event) GetFunctionResponses() []FunctionResponse {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var responses []FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}


// Content represents the content of a message.
type Content struct {
	Role  string
	Parts []Part
}

// Part represents a part of a message's content.
type Part struct {
	Text             string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
	// ... other part types like InlineData, FileData, etc.
}

// FunctionCall represents a function call requested by the LLM.
type FunctionCall struct {
	ID   string
	Name string
	Args map[string]interface{}
}

// FunctionResponse represents the result of a function call.
type FunctionResponse struct {
	ID       string
	Name     string
	Response map[string]interface{}
}

// EventActions represents actions associated with an event.
type EventActions struct {
	StateDelta             map[string]interface{}
	RequestedAuthConfigs map[string]interface{} // Placeholder for actual AuthConfig type
	// ... other actions like SkipSummarization, TransferToAgent, Escalate
}


// LlmRequest represents a request to the LLM.
type LlmRequest struct {
	Model    string
	Contents []Content
	Config   *GenerateContentConfig // Assuming GenerateContentConfig is defined elsewhere
	// ... other fields like LiveConnectConfig, ToolsDict
}

// GenerateContentConfig holds configuration for LLM content generation.
type GenerateContentConfig struct {
	// Define fields based on google.genai.types.GenerateContentConfig
	// For example:
	// Temperature *float32
	// TopP *float32
	// TopK *int32
	// CandidateCount *int32
	// MaxOutputTokens *int32
	// StopSequences []string
	// SafetySettings []*SafetySetting
	// Tools []*Tool
	// SystemInstruction *Content
	// ResponseSchema interface{}
	// ResponseMimeType string
	// ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig
}


// Placeholder, assuming it's defined in the functions package or similar
const REQUEST_EUC_FUNCTION_CALL_NAME = "adk_request_credential"

// ContentLlmRequestProcessor builds the contents for the LLM request.
type ContentLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *ContentLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		if llmAgent.GetIncludeContents() != "none" { // Assuming LlmAgent has GetIncludeContents()
			if invocationCtx.Session == nil {
				log.Println("Error: InvocationContext.Session is nil")
				return
			}
			llmReq.Contents = getContents(
				invocationCtx.Branch,
				invocationCtx.Session.Events, // Assuming Session has Events []*Event
				llmAgent.GetName(),
			)
		}
		// This processor does not yield events.
	}()
	return outCh, nil
}

func rearrangeEventsForAsyncFunctionResponsesInHistory(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	functionCallIDToResponseEventsIndex := make(map[string]int)
	for i, event := range events {
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			for _, fr := range responses {
				functionCallIDToResponseEventsIndex[fr.ID] = i
			}
		}
	}

	var resultEvents []*Event
	processedResponseIndices := make(map[int]bool) // To avoid adding same response event multiple times

	for i, event := range events {
		if _, ok := processedResponseIndices[i]; ok && len(event.GetFunctionResponses()) > 0 { //
			// This response event was already merged with its call, skip it.
			continue //
		}

		if calls := event.GetFunctionCalls(); len(calls) > 0 { //
			resultEvents = append(resultEvents, event) // Add the function call event itself

			var responseEventsToMerge []*Event
			var indicesToMarkProcessed []int

			for _, fc := range calls {
				if responseIdx, ok := functionCallIDToResponseEventsIndex[fc.ID]; ok { //
					isAlreadyProcessedByAnotherCall := false
					for _, resEvent := range resultEvents {
						if resEvent == events[responseIdx] {
							isAlreadyProcessedByAnotherCall = true
							break
						}
						// Also check if the response event's content (parts) were merged into another event.
						// For simplicity, we rely on direct event object comparison or index tracking.
					}
					if !isAlreadyProcessedByAnotherCall && !processedResponseIndices[responseIdx] { //
						responseEventsToMerge = append(responseEventsToMerge, events[responseIdx])
						indicesToMarkProcessed = append(indicesToMarkProcessed, responseIdx)
					}
				}
			}

			if len(responseEventsToMerge) > 0 {
				sort.SliceStable(responseEventsToMerge, func(i, j int) bool { //
					var idxI, idxJ int = -1, -1
					for k, e := range events {
						if e == responseEventsToMerge[i] {
							idxI = k
						}
						if e == responseEventsToMerge[j] {
							idxJ = k
						}
						if idxI != -1 && idxJ != -1 {
							break
						}
					}
					return idxI < idxJ
				})
				mergedRespEvent := mergeFunctionResponseEvents(responseEventsToMerge)
				if mergedRespEvent != nil {
					resultEvents = append(resultEvents, mergedRespEvent)
					for _, idx := range indicesToMarkProcessed { //
						processedResponseIndices[idx] = true
					}
				}
			}
		} else {
			if !(len(event.GetFunctionResponses()) > 0 && processedResponseIndices[i]) { //
				resultEvents = append(resultEvents, event)
			}
		}
	}
	return resultEvents
}

func rearrangeEventsForLatestFunctionResponse(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	lastEvent := events[len(events)-1]
	responsesInLastEvent := lastEvent.GetFunctionResponses()
	if len(responsesInLastEvent) == 0 { //
		return events
	}

	responseIDs := make(map[string]bool)
	for _, fr := range responsesInLastEvent {
		responseIDs[fr.ID] = true
	}

	if len(events) >= 2 { //
		eventBeforeLast := events[len(events)-2]
		callsInEventBeforeLast := eventBeforeLast.GetFunctionCalls()
		allCallsMatched := true
		if len(callsInEventBeforeLast) == len(responseIDs) && len(callsInEventBeforeLast) > 0 { //
			for _, fc := range callsInEventBeforeLast {
				if !responseIDs[fc.ID] {
					allCallsMatched = false
					break
				}
			}
			if allCallsMatched {
				return events
			}
		}
	}

	functionCallEventIdx := -1 //
	for i := len(events) - 2; i >= 0; i-- { //
		event := events[i]
		calls := event.GetFunctionCalls()
		if len(calls) > 0 {
			foundMatch := false
			for _, fc := range calls {
				if responseIDs[fc.ID] {
					foundMatch = true
					for _, otherFc := range calls {
						responseIDs[otherFc.ID] = true
					}
					break
				}
			}
			if foundMatch {
				functionCallEventIdx = i
				break
			}
		}
	}

	if functionCallEventIdx == -1 { //
		log.Printf("Warning: No matching function call event found for function response IDs: %v", getKeys(responseIDs))
		return events //
	}

	var functionResponseEventsToMerge []*Event
	for i := functionCallEventIdx + 1; i < len(events)-1; i++ { //
		event := events[i]
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			isRelevantResponse := false
			for _, fr := range responses {
				if responseIDs[fr.ID] {
					isRelevantResponse = true
					break
				}
			}
			if isRelevantResponse {
				functionResponseEventsToMerge = append(functionResponseEventsToMerge, event)
			}
		}
	}
	functionResponseEventsToMerge = append(functionResponseEventsToMerge, lastEvent)

	resultEvents := make([]*Event, 0, functionCallEventIdx+2) //
	resultEvents = append(resultEvents, events[:functionCallEventIdx+1]...)
	if merged := mergeFunctionResponseEvents(functionResponseEventsToMerge); merged != nil { //
		resultEvents = append(resultEvents, merged)
	}

	return resultEvents
}

func getContents(currentBranch string, historyEvents []*Event, agentName string) []Content {
	var filteredEvents []*Event
	for _, event := range historyEvents {
		if event.Content == nil || len(event.Content.Parts) == 0 { //
			continue
		}
		// Go: we assume an empty text part is still a valid part unless explicitly filtered.
		// For now, let's assume empty text part implies no meaningful content for LLM.
		hasMeaningfulText := false //
		for _, p := range event.Content.Parts {
			if p.Text != "" {
				hasMeaningfulText = true
				break
			}
		}
		if !hasMeaningfulText && len(event.GetFunctionCalls()) == 0 && len(event.GetFunctionResponses()) == 0 { //
			continue
		}

		if !isEventOnBranch(currentBranch, event) {
			continue
		}
		if isAuthEvent(event) {
			continue
		}

		if isOtherAgentReply(agentName, event) {
			filteredEvents = append(filteredEvents, convertForeignEventToGo(event))
		} else {
			filteredEvents = append(filteredEvents, event)
		}
	}

	// Python calls _rearrange_events_for_latest_function_response first, then _rearrange_events_for_async_function_responses_in_history.
	resultEventsStage1 := rearrangeEventsForLatestFunctionResponse(filteredEvents)
	resultEventsStage2 := rearrangeEventsForAsyncFunctionResponsesInHistory(resultEventsStage1)

	var contents []Content
	for _, event := range resultEventsStage2 {
		if event.Content != nil {
			contentCopy := deepCopyContent(*event.Content)
			removeClientFunctionCallID(&contentCopy) // Placeholder
			contents = append(contents, contentCopy)
		}
	}
	return contents
}

func isEventOnBranch(currentBranch string, event *Event) bool {
	// Placeholder for logic to check if event is on the current branch
	// This depends on how Branch is structured and compared.
	if event == nil {
		return false
	}
	return event.Branch == "" || currentBranch == "" || strings.HasPrefix(event.Branch, currentBranch) || strings.HasPrefix(currentBranch, event.Branch)
}

func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return currentAgentName != "" && event.Author != currentAgentName && event.Author != "user"
}

func convertForeignEventToGo(event *Event) *Event {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return event
	}

	newContent := Content{Role: "user"} //
	newContent.Parts = append(newContent.Parts, Part{Text: "For context:"})

	for _, part := range event.Content.Parts {
		if part.Text != "" {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] said: %s", event.Author, part.Text)})
		} else if part.FunctionCall != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] called tool `%s` with parameters: %v", event.Author, part.FunctionCall.Name, part.FunctionCall.Args)})
		} else if part.FunctionResponse != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] `%s` tool returned result: %v", event.Author, part.FunctionResponse.Name, part.FunctionResponse.Response)})
		} else {
			// This part needs careful consideration for how to represent non-text/non-FC/non-FR parts.
			// For simplicity, appending as-is if possible, or a placeholder.
			// newContent.Parts = append(newContent.Parts, part) // This assumes Part is copyable
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] provided non-text data.", event.Author)}) //
		}
	}

	newEvent := *event // Shallow copy for metadata
	newEvent.Author = "user" //
	newEvent.Content = &newContent
	return &newEvent
}

func mergeFunctionResponseEvents(functionResponseEvents []*Event) *Event {
	if len(functionResponseEvents) == 0 {
		return nil
	}
	if len(functionResponseEvents) == 1 {
		return functionResponseEvents[0]
	}

	baseEvent := functionResponseEvents[0]
	// mergedParts := make([]Part, 0) // Not used directly in Go like Python

	// Map to store the latest response part for each function call ID
	latestResponsesForID := make(map[string]Part)
	var nonResponseParts []Part // To collect text parts or other non-FR parts in order

	for _, event := range functionResponseEvents {
		if event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil {
				latestResponsesForID[part.FunctionResponse.ID] = part
			} else {
				nonResponseParts = append(nonResponseParts, part)
			}
		}
	}

	// The Python code implicitly relies on the order of parts in the *first* event
	// if it contained multiple function responses, and then updates them.
	// For Go, if the first event had multiple FCs, its FRs would be the base.
	// If subsequent events provide FRs for *different* FC IDs not in the first event, they are appended.
	// This simplified version will collect all unique FRs based on their latest appearance.
	tempParts := make([]Part, 0, len(latestResponsesForID)+len(nonResponseParts)) //
	tempParts = append(tempParts, nonResponseParts...)                            //
	// To maintain order similar to Python (where original order of FCs in the *first* event matters,
	// and then other unique FRs are appended/merged), we need a more sophisticated merge.
	// For now, this collects unique FRs; their order relative to nonResponseParts and each other might differ from Python's.
	// A better approach would be to iterate through the function calls of the *triggering* event (if available)
	// and append responses in that order, or sort collected FRs by some original call order.

	// Example: Iterate through first event's FRs if they exist as a base order
	if baseEvent.Content != nil {
		for _, part := range baseEvent.Content.Parts {
			if part.FunctionResponse != nil {
				if respPart, ok := latestResponsesForID[part.FunctionResponse.ID]; ok {
					tempParts = append(tempParts, respPart)
					delete(latestResponsesForID, part.FunctionResponse.ID) // Mark as added
				}
			}
		}
	}
	// Append any remaining unique FRs (those not in the first event's call order)
	for _, part := range latestResponsesForID { // Order from map is not guaranteed.
		tempParts = append(tempParts, part)
	}

	mergedEvent := &Event{ //
		ID:           newEventID(), // Assuming newEventID() is defined
		InvocationID: baseEvent.InvocationID,
		Author:       baseEvent.Author,
		Branch:       baseEvent.Branch,
		Content:      &Content{Role: "user", Parts: tempParts}, // Gemini expects FRs to be role "user"
		Actions:      &EventActions{},                          // Initialize to avoid nil pointer if baseEvent.Actions is nil
		Timestamp:    baseEvent.Timestamp,                      // Or use latest timestamp
	}

	// Merge actions from all events (simplified)
	for _, event := range functionResponseEvents {
		if event.Actions != nil && event.Actions.StateDelta != nil { //
			if mergedEvent.Actions.StateDelta == nil {
				mergedEvent.Actions.StateDelta = make(map[string]interface{})
			}
			for k, v := range event.Actions.StateDelta {
				mergedEvent.Actions.StateDelta[k] = v
			}
		}
		// ... merge other actions like RequestedAuthConfigs, ArtifactDelta etc.
	}

	return mergedEvent
}

// newEventID generates a new unique ID for an event.
// This is a placeholder; a robust UUID generation should be used.
func newEventID() string {
	// In a real implementation, use something like github.com/google/uuid
	return fmt.Sprintf("evt_%d", time.Now().UnixNano())
}


func isAuthEvent(event *Event) bool {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return false
	}
	for _, part := range event.Content.Parts {
		if part.FunctionCall != nil && part.FunctionCall.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
		if part.FunctionResponse != nil && part.FunctionResponse.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
	}
	return false
}

// removeClientFunctionCallID placeholder
func removeClientFunctionCallID(content *Content) {
	// In Python, this removes IDs starting with AF_FUNCTION_CALL_ID_PREFIX
	// For Go, this would iterate through content.Parts and nil out IDs if they match a pattern.
	// For example:
	// const afFunctionCallIDPrefix = "adk-"
	// if content == nil || content.Parts == nil {
	// 	return
	// }
	// for i := range content.Parts {
	// 	if content.Parts[i].FunctionCall != nil && strings.HasPrefix(content.Parts[i].FunctionCall.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionCall.ID = "" // Or based on how Gemini handles empty vs nil
	// 	}
	// 	if content.Parts[i].FunctionResponse != nil && strings.HasPrefix(content.Parts[i].FunctionResponse.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionResponse.ID = ""
	// 	}
	// }
}

// deepCopyContent performs a deep copy of the Content struct.
func deepCopyContent(original Content) Content { //
	// A real deep copy is needed here. This is a placeholder.
	// For Go structs, manual copying or a library might be needed for true deep copies.
	copied := original //
	if original.Parts != nil {
		copied.Parts = make([]Part, len(original.Parts))
		for i, p := range original.Parts {
			// Deep copy each part. This is simplified.
			// If Part contains pointers or slices, those need deep copying too.
			copiedPart := p
			if p.FunctionCall != nil {
				fcCopy := *p.FunctionCall
				if p.FunctionCall.Args != nil {
					fcCopy.Args = deepCopyMap(p.FunctionCall.Args)
				}
				copiedPart.FunctionCall = &fcCopy
			}
			if p.FunctionResponse != nil {
				frCopy := *p.FunctionResponse
				if p.FunctionResponse.Response != nil {
					frCopy.Response = deepCopyMap(p.FunctionResponse.Response)
				}
				copiedPart.FunctionResponse = &frCopy
			}
			// Add deep copy for other Part fields if necessary
			copied.Parts[i] = copiedPart
		}
	}
	return copied
}

// deepCopyMap creates a deep copy of a map[string]interface{}.
// This is a simplified version; a robust deep copy library might be better for complex nested structures.
func deepCopyMap(originalMap map[string]interface{}) map[string]interface{} {
	if originalMap == nil {
		return nil
	}
	copyMap := make(map[string]interface{})
	for key, value := range originalMap {
		switch v := value.(type) {
		case map[string]interface{}:
			copyMap[key] = deepCopyMap(v)
		case []interface{}:
			copyMap[key] = deepCopySlice(v)
		default:
			copyMap[key] = v // Assumes primitive types are copyable by value
		}
	}
	return copyMap
}

// deepCopySlice creates a deep copy of a slice of interface{}.
func deepCopySlice(originalSlice []interface{}) []interface{} {
	if originalSlice == nil {
		return nil
	}
	copySlice := make([]interface{}, len(originalSlice))
	for i, value := range originalSlice {
		switch v := value.(type) {
		case map[string]interface{}:
			copySlice[i] = deepCopyMap(v)
		case []interface{}:
			copySlice[i] = deepCopySlice(v)
		default:
			copySlice[i] = v
		}
	}
	return copySlice
}


func getKeys(m map[string]bool) []string { //
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}

// Placeholder for LlmAgent.GetIncludeContents() method.
// This should be part of the LlmAgent interface/struct definition.
// func (a *LlmAgentStruct) GetIncludeContents() string { return a.IncludeContents }

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/instructions.go
package processors

import (
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/utils" // For instructions_utils
)

// ReadonlyContext provides a read-only view of the invocation context.
type ReadonlyContext struct { //
	InvocationContext *InvocationContext
	// Potentially other fields if ReadonlyContext offers more than just InvocationContext access
}


// InstructionsLlmRequestProcessor handles instructions and global instructions for LLM flow.
type InstructionsLlmRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *InstructionsLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in InstructionsLlmRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx} //

		// Append global instructions if set.
		// In Python ADK, global_instruction is on the root agent.
		// Assuming LlmAgent interface has a RootAgent() method that returns the root agent (LlmAgent type).
		var rootLlmAgent LlmAgent
		if ra := llmAgent.RootAgent(); ra != nil {
			var rOk bool
			rootLlmAgent, rOk = ra.(LlmAgent)
			if !rOk {
				log.Println("Warning: Root agent is not an LlmAgent, cannot process global instructions.")
			}
		}


		if rootLlmAgent != nil { //
			// Assuming LlmAgent interface has CanonicalGlobalInstruction
			globalInstructionStr, bypassGlobal, err := rootLlmAgent.CanonicalGlobalInstruction(readonlyCtx) //
			if err != nil {
				log.Printf("Error getting canonical global instruction: %v", err) //
			}
			if globalInstructionStr != "" {
				var processedGlobalInstr string
				if !bypassGlobal {
					processedGlobalInstr, err = injectSessionState(globalInstructionStr, readonlyCtx) //
					if err != nil {
						log.Printf("Error injecting state into global instruction: %v", err) //
						processedGlobalInstr = globalInstructionStr                               // Use raw on error
					}
				} else {
					processedGlobalInstr = globalInstructionStr
				}
				// Assuming LlmRequest has AppendInstructions
				if llmReq.Config == nil {
					llmReq.Config = &GenerateContentConfig{}
				}
				if llmReq.Config.SystemInstruction == nil {
					llmReq.Config.SystemInstruction = &Content{}
				}
				// This needs a proper AppendInstructions method on LlmRequest or its Config
				currentSysInstruction := ""
				if len(llmReq.Config.SystemInstruction.Parts) > 0 {
					currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
				}
				if currentSysInstruction != "" {
					currentSysInstruction += "\n\n"
				}
				llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedGlobalInstr}}

			}
		}

		// Append agent instructions if set.
		// Assuming LlmAgent interface has CanonicalInstruction
		agentInstructionStr, bypassAgent, err := llmAgent.CanonicalInstruction(readonlyCtx) //
		if err != nil {
			log.Printf("Error getting canonical agent instruction: %v", err) //
		}
		if agentInstructionStr != "" {
			var processedAgentInstr string
			if !bypassAgent {
				processedAgentInstr, err = injectSessionState(agentInstructionStr, readonlyCtx) //
				if err != nil {
					log.Printf("Error injecting state into agent instruction: %v", err) //
					processedAgentInstr = agentInstructionStr                             // Use raw on error
				}
			} else {
				processedAgentInstr = agentInstructionStr
			}
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedAgentInstr}}
		}
		// No events are yielded by this specific processor.
	}()
	return outCh, nil
}

// injectSessionState populates values in the instruction template.
// This is a Go equivalent of instructions_utils.inject_session_state.
func injectSessionState(template string, readonlyCtx *ReadonlyContext) (string, error) {
	if readonlyCtx == nil || readonlyCtx.InvocationContext == nil || readonlyCtx.InvocationContext.Session == nil {
		return template, fmt.Errorf("readonlyCtx or its internal fields are nil")
	}
	invocationCtx := readonlyCtx.InvocationContext //

	// Go's regexp package doesn't support direct async replacement functions.
	// This will be synchronous for now.
	// Pattern to find {var_name} or {var_name?} or {artifact.file_name}
	// For simplicity, a basic version is used here. A more robust parser might be needed.
	re := regexp.MustCompile(`{([^}]+)}`) //

	var replacementErr error

	result := re.ReplaceAllStringFunc(template, func(match string) string { //
		if replacementErr != nil {
			return match
		}

		varNameWithMarker := strings.Trim(match, "{} ")
		isOptional := strings.HasSuffix(varNameWithMarker, "?")
		varName := strings.TrimSuffix(varNameWithMarker, "?")

		if strings.HasPrefix(varName, "artifact.") { //
			artifactName := strings.TrimPrefix(varName, "artifact.")
			if invocationCtx.ArtifactService == nil { //
				replacementErr = fmt.Errorf("artifact service is not initialized")
				return match
			}
			// Conceptual: ArtifactService would need a synchronous LoadArtifact or this needs to be async.
			// For this snippet, assuming a conceptual synchronous load or pre-loaded artifact map.
			// This part is commented out as ArtifactService and its methods are not fully defined here.
			/*
				artifactPart, err := invocationCtx.ArtifactService.LoadArtifactSync( //
					invocationCtx.Session.AppName, // Assuming AppName field
					invocationCtx.Session.UserID,  // Assuming UserID field
					invocationCtx.Session.ID,      // Assuming ID field
					artifactName,
				)
				if err != nil {
					if isOptional { return "" }
					replacementErr = fmt.Errorf("artifact '%s' not found: %w", artifactName, err)
					return match
				}
				if artifactPart != nil {
					return artifactPart.Text // Assuming text artifact for simplicity
				}
			*/
			return fmt.Sprintf("[ARTIFACT:%s]", artifactName) // Placeholder
		}

		if !isValidStateName(varName) { //
			return match
		}

		if val, ok := invocationCtx.Session.State[varName]; ok { //
			return fmt.Sprintf("%v", val)
		}
		if isOptional { //
			return ""
		}
		replacementErr = fmt.Errorf("context variable not found: `%s`", varName) //
		return match
	})

	if replacementErr != nil {
		return "", replacementErr
	}
	return result, nil
}

// isValidStateName (simplified) checks if a variable name could be a state key.
func isValidStateName(varName string) bool {
	parts := strings.Split(varName, ":")
	if len(parts) == 1 {
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(varName) //
	}
	if len(parts) == 2 {
		// In Python ADK, prefixes are "app:", "user:", "temp:"
		// Simplified check for now.
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[0]) &&
			regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[1]) //
	}
	return false
}

// LlmAgent interface needs to be fully defined, including these methods for the above to compile:
// RootAgent() BaseAgent // Assuming BaseAgent is a common interface for all agents
// CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error)
// CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error)

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/base_llm_processors.go
package processors

// BaseLlmRequestProcessor defines the interface for processing LLM requests.
type BaseLlmRequestProcessor interface {
	RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error)
}

// BaseLlmResponseProcessor defines the interface for processing LLM responses.
type BaseLlmResponseProcessor interface {
	RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error)
}

// BaseLlmProcessor is a conceptual base for processors if common utility methods were needed.
// In Go, composition or helper functions are often preferred over deep inheritance hierarchies.
// For now, interfaces suffice.

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/basic.go
package processors

import (
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, etc.
)

// BaseLlm defines the interface for a Large Language Model.
type BaseLlm interface {
	ModelName() string
	// Potentially other methods like GenerateContent, Connect etc.
}

// MockLLMForFlow is a placeholder for testing, representing a specific LLM implementation.
type MockLLMForFlow struct {
	ModelName string
}

func (m *MockLLMForFlow) ModelName() string {
	return m.ModelName
}
// Ensure MockLLMForFlow implements BaseLlm
var _ BaseLlm = &MockLLMForFlow{}


// RunConfig defines configuration for an agent run.
type RunConfig struct {
	// Define fields based on google.adk.agents.run_config.RunConfig
	// Example fields (conceptual):
	// ResponseModalities []string
	// SpeechConfig interface{}
	// OutputAudioTranscription bool
	// InputAudioTranscription bool
	// MaxLlmCalls int
	// StreamingMode string
}


// LlmAgent interface part related to BasicRequestProcessor
type LlmAgentForBasicProcessor interface {
	LlmAgent // Embed the general LlmAgent interface
	CanonicalModel() (BaseLlm, error)
	GetGenerateContentConfig() *GenerateContentConfig
	GetOutputSchema() interface{}
}


// BasicRequestProcessor handles basic information to build the LLM request.
type BasicRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *BasicRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) { //
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in BasicRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgentForBasicProcessor) // Use the more specific interface
		if !ok {
			// If agent is not an LlmAgent, this processor might not apply,
			// or it might indicate an issue with the agent setup.
			// For now, we simply return without error.
			log.Println("Error: Agent in InvocationContext is not of type LlmAgentForBasicProcessor or compatible interface")
			return
		}

		canonicalModel, err := llmAgent.CanonicalModel() //
		if err != nil {
			log.Printf("Error getting canonical model for agent %s: %v", llmAgent.GetName(), err) //
			return
		}

		if canonicalModel != nil { //
			llmReq.Model = canonicalModel.ModelName() //
		} else {
			log.Printf("Warning: CanonicalModel is nil for agent %s, model name might not be set correctly in LlmRequest.", llmAgent.GetName()) //
		}


		if cfg := llmAgent.GetGenerateContentConfig(); cfg != nil { //
			cfgCopy := *cfg // Shallow copy
			llmReq.Config = &cfgCopy //
		} else {
			llmReq.Config = &GenerateContentConfig{} // Ensure config is not nil
		}

		if outputSchema := llmAgent.GetOutputSchema(); outputSchema != nil { //
			// Assuming LlmRequest has a SetOutputSchema method
			// llmReq.SetOutputSchema(outputSchema) // This depends on LlmRequest's definition
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			llmReq.Config.ResponseSchema = outputSchema
			llmReq.Config.ResponseMimeType = "application/json" // Typically for structured output

		}

		if invocationCtx.RunConfig != nil { //
			// Conceptual assignments - actual fields depend on RunConfig and LlmRequest.LiveConnectConfig definitions
			// if llmReq.LiveConnectConfig == nil { llmReq.LiveConnectConfig = &LiveConnectConfig{} }
			// llmReq.LiveConnectConfig.ResponseModalities = invocationCtx.RunConfig.ResponseModalities
			// llmReq.LiveConnectConfig.SpeechConfig = invocationCtx.RunConfig.SpeechConfig
			// llmReq.LiveConnectConfig.OutputAudioTranscription = invocationCtx.RunConfig.OutputAudioTranscription
			// llmReq.LiveConnectConfig.InputAudioTranscription = invocationCtx.RunConfig.InputAudioTranscription
			// These would be direct assignments if LiveConnectConfig fields match RunConfig fields.
		}

		// No events are yielded by this specific processor, it only modifies llmReq.
	}()
	return outCh, nil
}

// These GetGenerateContentConfig and GetOutputSchema methods should be part of the LlmAgent implementation.
// Example implementation for a hypothetical LlmAgentStruct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentGenerateContentConfig *GenerateContentConfig
    OutputSchema interface{}
}

func (a *LlmAgentStruct) GetGenerateContentConfig() *GenerateContentConfig { //
    return a.AgentGenerateContentConfig
}

func (a *LlmAgentStruct) GetOutputSchema() interface{} { //
    return a.OutputSchema
}
*/

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/identity.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
)

// IdentityLlmRequestProcessor gives the agent identity from the framework.
type IdentityLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *IdentityLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in IdentityLlmRequestProcessor")
			return
		}
		agent := invocationCtx.Agent // Assuming Agent interface has GetName() and GetDescription()

		var instructions []string
		instructions = append(instructions, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName())) //
		if desc := agent.GetDescription(); desc != "" { //
			instructions = append(instructions, fmt.Sprintf(" The description about you is \"%s\"", desc)) //
		}

		// Assuming LlmRequest has AppendInstructions method or similar mechanism
		if llmReq.Config == nil {
			llmReq.Config = &GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &Content{}
		}
		currentSysInstruction := ""
		if len(llmReq.Config.SystemInstruction.Parts) > 0 {
			currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
		}

		addedInstruction := strings.Join(instructions, "")
		if currentSysInstruction != "" {
			currentSysInstruction += "\n\n" // Ensure separation if there's existing instruction
		}
		llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + addedInstruction}}
		// This processor does not yield events, it modifies llmReq.
	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/nlplanning.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/planners" // For PlanReActPlanner, BuiltInPlanner
	// "github.com/google/generative-ai-go/genai" // For genai.Part, genai.ThinkingConfig
)

// Planner defines the interface for agent planners.
type Planner interface { // [cite: 2133]
	BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) // [cite: 2133]
	ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) // [cite: 2133]
	// ApplyThinkingConfig is specific to BuiltInPlanner in Python, might be part of a more specific interface in Go
	ApplyThinkingConfig(llmReq *LlmRequest)
}

// BuiltInPlanner is a placeholder for planners.BuiltInPlanner [cite: 2133]
type BuiltInPlanner struct {
	ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig [cite: 2133]
}

// ApplyThinkingConfig applies the thinking configuration to the LLM request.
func (bip *BuiltInPlanner) ApplyThinkingConfig(llmReq *LlmRequest) { // [cite: 2133]
	if bip.ThinkingConfig != nil && llmReq.Config != nil {
		// llmReq.Config.ThinkingConfig = bip.ThinkingConfig // This depends on GenerateContentConfig definition
		// For example, if GenerateContentConfig has a field `ThinkingConfig interface{}`
		// then the above line would work.
		log.Println("BuiltInPlanner.ApplyThinkingConfig: ThinkingConfig application is conceptual.")
	}
}
// BuildPlanningInstruction for BuiltInPlanner.
func (bip *BuiltInPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) { // [cite: 2133]
	return "", nil // Python version returns None, so empty string and no error for Go
}
// ProcessPlanningResponse for BuiltInPlanner.
func (bip *BuiltInPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) { // [cite: 2133, 2134]
	return responseParts, nil
}

// PlanReActPlanner is a placeholder for planners.PlanReActPlanner [cite: 2134]
type PlanReActPlanner struct{}

// Constants for PlanReActPlanner tags [cite: 2134]
const (
	PlanningTag    = "/*PLANNING*/"    // [cite: 2134]
	RePlanningTag  = "/*REPLANNING*/"  // [cite: 2134]
	ReasoningTag   = "/*REASONING*/"   // [cite: 2134]
	ActionTag      = "/*ACTION*/"      // [cite: 2134]
	FinalAnswerTag = "/*FINAL_ANSWER*/" // [cite: 2134]
)

// BuildPlanningInstruction for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) {
	// This directly translates the Python _build_nl_planner_instruction method [cite: 2134]
	highLevelPreamble := fmt.Sprintf(` When answering the question, try to leverage the available tools to gather the information instead of your memorized knowledge. [cite: 2134, 2135]
Follow this process when answering the question: (1) first come up with a plan in natural language text format; [cite: 2135, 2136]
(2) Then use tools to execute the plan and provide reasoning between tool code snippets to make a summary of current state and next step. [cite: 2136, 2137]
Tool code snippets and reasoning should be interleaved with each other. (3) In the end, return one final answer. [cite: 2137]
Follow this format when answering the question: (1) The planning part should be under %s. [cite: 2138]
(2) The tool code snippets should be under %s, and the reasoning parts should be under %s. [cite: 2139]
(3) The final answer part should be under %s. `, PlanningTag, ActionTag, ReasoningTag, FinalAnswerTag) // [cite: 2139, 2140]

	planningPreamble := fmt.Sprintf(` Below are the requirements for the planning: The plan is made to answer the user query if following the plan. The plan is coherent and covers all aspects of information from user query, and only involves the tools that are accessible by the agent. The plan contains the decomposed steps as a numbered list where each step should use one or multiple available tools. By reading the plan, you can intuitively know which tools to trigger or what actions to take. [cite: 2140, 2141]
If the initial plan cannot be successfully executed, you should learn from previous execution results and revise your plan. The revised plan should be be under %s. Then use tools to follow the new plan. `, RePlanningTag) // [cite: 2141]

	// The Python original has reasoning_preamble, final_answer_preamble, tool_code_without_python_libraries_preamble, user_input_preamble
	// These would be similarly defined and concatenated. For brevity, I'm showing the pattern.
	// Assuming those other preamble parts are defined similarly:
	reasoningPreamble := ` Below are the requirements for the reasoning: The reasoning makes a summary of the current trajectory based on the user query and tool outputs. Based on the tool outputs and plan, the reasoning also comes up with instructions to the next steps, making the trajectory closer to the final answer. `
	finalAnswerPreamble := ` Below are the requirements for the final answer: The final answer should be precise and follow query formatting requirements. Some queries may not be answerable with the available tools and information. In those cases, inform the user why you cannot process their query and ask for more information. `
	toolCodeWithoutPythonLibrariesPreamble := ` Below are the requirements for the tool code: **Custom Tools:** The available tools are described in the context and can be directly used. - Code must be valid self-contained Python snippets with no imports and no references to tools or Python libraries that are not in the context. - You cannot use any parameters or fields that are not explicitly defined in the APIs in the context. - The code snippets should be readable, efficient, and directly relevant to the user query and reasoning steps. - When using the tools, you should use the library name together with the function name, e.g., vertex_search.search(). - If Python libraries are not provided in the context, NEVER write your own code other than the function calls using the provided tools. `
	userInputPreamble := ` VERY IMPORTANT instruction that you MUST follow in addition to the above instructions: You should ask for clarification if you need more information to answer the question. You should prefer using the information available in the context instead of repeated tool use. `

	return strings.Join([]string{
		highLevelPreamble,
		planningPreamble,
		reasoningPreamble,
		finalAnswerPreamble,
		toolCodeWithoutPythonLibrariesPreamble,
		userInputPreamble,
	}, "\n\n"), nil
}

// markAsThought marks a response part as a thought.
// In Go, `Part` doesn't have a direct `Thought` field like in Python's genai types.
// This might be handled by a wrapper struct or by convention (e.g., specific role or metadata).
// For this conceptual translation, we'll assume a way to mark it, or it's implicit in the planner's logic.
func (prp *PlanReActPlanner) markAsThought(part *Part) {
	// Conceptual: If Part struct had a Thought field: part.Thought = true
	// Or, this could modify the Part's role or add metadata if the Go ADK defines such conventions.
	// For now, this is a no-op unless Part struct is extended.
	log.Printf("Marking part as thought (conceptual): %v", part.Text)
}


// ProcessPlanningResponse for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) {
	if responseParts == nil { // [cite: 2134]
		return nil, nil
	}

	var preservedParts []Part
	firstFCPartIndex := -1

	for i, part := range responseParts {
		if part.FunctionCall != nil { // [cite: 2134]
			if part.FunctionCall.Name == "" {
				continue
			}
			preservedParts = append(preservedParts, part)
			if firstFCPartIndex == -1 {
				firstFCPartIndex = i
			}
		} else {
			// Handle non-function call parts based on tags
			if part.Text != "" {
				// Split by FINAL_ANSWER_TAG first
				if strings.Contains(part.Text, FINAL_ANSWER_TAG) {
					splits := strings.SplitN(part.Text, FINAL_ANSWER_TAG, 2)
					reasoningText := strings.TrimSpace(splits[0])
					finalAnswerText := ""
					if len(splits) > 1 {
						finalAnswerText = strings.TrimSpace(splits[1])
					}

					if reasoningText != "" {
						reasoningPart := Part{Text: reasoningText}
						prp.markAsThought(&reasoningPart) // Conceptual
						preservedParts = append(preservedParts, reasoningPart)
					}
					if finalAnswerText != "" {
						preservedParts = append(preservedParts, Part{Text: finalAnswerText})
					}
				} else {
					// If no FINAL_ANSWER_TAG, check for other thought tags
					isThought := false
					for _, tag := range []string{PlanningTag, ReasoningTag, ActionTag, RePlanningTag} {
						if strings.HasPrefix(part.Text, tag) {
							isThought = true
							break
						}
					}
					currentPart := part
					if isThought {
						prp.markAsThought(&currentPart) // Conceptual
					}
					preservedParts = append(preservedParts, currentPart)
				}
			}
		}
	}
	return preservedParts, nil
}

// ApplyThinkingConfig for PlanReActPlanner (not directly applicable as it doesn't use genai.ThinkingConfig)
func (prp *PlanReActPlanner) ApplyThinkingConfig(llmReq *LlmRequest) {
	// PlanReActPlanner builds instructions directly, doesn't use genai.ThinkingConfig
}


// LlmAgent interface needs to be fully defined for the below to be valid outside this file.
// For example, within an LlmAgent struct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentPlanner Planner
}

func (a *LlmAgentStruct) GetPlanner() Planner {
    return a.AgentPlanner
}
*/

// NlPlanningRequestProcessor handles NL planning for LLM flow.
type NlPlanningRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *NlPlanningRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in NlPlanningRequestProcessor")
			return
		}

		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}
		// Python has: if isinstance(planner, BuiltInPlanner): planner.apply_thinking_config(llm_req)
		// In Go, this would typically be handled by type assertion or checking an interface method.
		if bp, ok := planner.(*BuiltInPlanner); ok { // [cite: 2133]
			bp.ApplyThinkingConfig(llmReq) // [cite: 2133]
		}


		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx}
		instruction, err := planner.BuildPlanningInstruction(readonlyCtx, llmReq) // [cite: 2133]
		if err != nil {
			log.Printf("Error building planning instruction: %v", err)
			// Optionally send an error event or handle error
			return
		}
		if instruction != "" {
			// Assuming LlmRequest has AppendInstructions
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + instruction}}
		}
	}()
	return outCh, nil
}

// NlPlanningResponseProcessor processes LLM responses for NL planning.
type NlPlanningResponseProcessor struct{}

// RunAsync implements the BaseLlmResponseProcessor interface (conceptual).
func (p *NlPlanningResponseProcessor) RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || llmResp == nil || llmResp.Content == nil {
			log.Println("Error: InvocationContext, Agent, or LlmResponse content is nil in NlPlanningResponseProcessor")
			return
		}
		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}

		callbackCtx := &CallbackContext{InvocationContext: invocationCtx, EventActions: modelResponseEvent.Actions} // [cite: 2133]
		if callbackCtx.EventActions == nil { // Ensure EventActions is not nil
		    callbackCtx.EventActions = &EventActions{}
		}


		processedParts, err := planner.ProcessPlanningResponse(callbackCtx, llmResp.Content.Parts) // [cite: 2134]
		if err != nil {
			log.Printf("Error processing planning response: %v", err)
			// Optionally send an error event or handle error
			return
		}

		if processedParts != nil {
			llmResp.Content.Parts = processedParts
		}

		// If callbackCtx.State has changed (meaning planner updated state via EventActions)
		// and an event needs to be yielded. In Python, this is handled by the framework.
		// Here, we might need to explicitly yield an event if the state delta implies one.
		// However, the processor's role is usually to modify the llmResp or yield new events based on llmResp processing.
		// If state changes in callbackCtx are meant to produce a new event, that logic would go here.
		// For now, this processor primarily modifies the llmResp. If an event with updated actions
		// is needed, it would be constructed and sent to outCh.
		// This part of the Python code doesn't explicitly yield an Event from this processor.

	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/auto_flow.go
package llmflows

import (
	"fmt"
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
)

// AutoFlow dynamically selects and runs the appropriate flow for an agent.
// In Go, the dynamic creation of the LlmAgent itself as done in Python's AutoFlow
// (creating an LlmAgent on the fly with specific tools/planners based on config)
// is less straightforward due to static typing.
// Typically, the LlmAgent would already be constructed with its configuration.
// This AutoFlow equivalent will focus on selecting the flow and running it.
type AutoFlow struct {
	// Configuration for AutoFlow, if any, can go here.
	// For example, a map of agent types to flow constructors.
}

// NewAutoFlow creates a new AutoFlow.
func NewAutoFlow() *AutoFlow {
	return &AutoFlow{}
}

// RunAsync executes the appropriate flow for the given agent.
func (af *AutoFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	if invocationCtx == nil || invocationCtx.Agent == nil {
		err := fmt.Errorf("AutoFlow: InvocationContext or Agent is nil")
		log.Println(err)
		// Return a channel that immediately closes or sends an error event
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	agent := invocationCtx.Agent // Agent from InvocationContext

	// The Python AutoFlow can dynamically create an LlmAgent if the provided agent
	// is just a configuration. In Go, we'd expect `agent` to already be a runnable LlmAgent.
	// If `agent` is a configuration struct, a factory would be needed here to build the LlmAgent.

	llmAgent, ok := agent.(LlmAgent) // Assuming the agent in context is already an LlmAgent
	if !ok {
		err := fmt.Errorf("AutoFlow: agent in InvocationContext is not an LlmAgent, but %T", agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	// --- Flow Selection Logic ---
	// In Python, this checks agent.flow or agent.tools, agent.planner etc.
	// For Go, we'll assume a simple logic: if the agent is an LlmAgent, use SingleFlow.
	// More complex logic could be based on LlmAgent's properties or specific type.

	var selectedFlow interface { // Using interface{} to represent a generic Flow type
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	}

	// Example: If LlmAgent has a GetFlowType() method or similar configuration.
	// flowType := llmAgent.GetFlowType() // Hypothetical method
	// switch flowType {
	// case "single":
	//  selectedFlow = NewSingleFlow()
	// case "sequential":
	//  selectedFlow = NewSequentialFlow() // If SequentialFlow exists
	// default:
	//  selectedFlow = NewSingleFlow() // Default
	// }

	// For now, directly use SingleFlow if it's an LlmAgent
	if _, isLlmAgent := agent.(LlmAgent); isLlmAgent {
		log.Printf("AutoFlow: Using SingleFlow for LlmAgent: %s", llmAgent.GetName())
		selectedFlow = NewSingleFlow()
	} else {
		// Handle other agent types if necessary, or default
		err := fmt.Errorf("AutoFlow: Unhandled agent type %T for flow selection. Defaulting to SingleFlow if possible or erroring.", agent)
		log.Println(err)
        // Attempt to use SingleFlow if the base agent can be cast.
        // This part is tricky without knowing all agent types.
        // For robustnes, it's better to ensure llmAgent is correctly typed above.
		log.Println("AutoFlow: Defaulting to SingleFlow (or error if not LlmAgent).")
		selectedFlow = NewSingleFlow() // This will run if the initial cast to LlmAgent succeeded.
	}


	if selectedFlow == nil {
		err := fmt.Errorf("AutoFlow: Could not determine a flow for agent: %s (%T)", llmAgent.GetName(), agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}

	// Type assert selectedFlow to the expected Flow interface/type to call RunAsync
	// This assumes all flows (SingleFlow, etc.) implement this method signature.
	concreteFlow, flowOk := selectedFlow.(interface {
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	})
	if !flowOk {
		err := fmt.Errorf("AutoFlow: Selected flow does not implement the required RunAsync method signature.")
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}


	log.Printf("AutoFlow: Running flow for agent: %s", llmAgent.GetName())
	return concreteFlow.RunAsync(invocationCtx, llmAgent)
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/single_flow.go
package llmflows

import (
	"github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
)

// SingleFlow represents a simple LLM flow with a standard set of processors.
type SingleFlow struct {
	*BaseLlmFlow
}

// NewSingleFlow creates a new SingleFlow.
func NewSingleFlow() *SingleFlow {
	requestProcessors := []processors.BaseLlmRequestProcessor{
		&processors.BasicRequestProcessor{},
		&processors.IdentityLlmRequestProcessor{},
		&processors.ToolLlmRequestProcessor{}, // Assuming ToolLlmRequestProcessor is defined
		&processors.NlPlanningRequestProcessor{},
		&processors.InstructionsLlmRequestProcessor{},
		&processors.ContentLlmRequestProcessor{},
		&processors.CodeExecutionRequestProcessor{}, // Assuming CodeExecutionRequestProcessor is defined
	}

	responseProcessors := []processors.BaseLlmResponseProcessor{
		&processors.NlPlanningResponseProcessor{},
		&processors.FunctionCallResponseProcessor{}, // Assuming FunctionCallResponseProcessor is defined
		&processors.CodeExecutionResponseProcessor{},// Assuming CodeExecutionResponseProcessor is defined
		&processors.ToolUsageResponseProcessor{},    // Assuming ToolUsageResponseProcessor is defined
	}

	baseFlow := NewBaseLlmFlow(requestProcessors, responseProcessors)
	return &SingleFlow{BaseLlmFlow: baseFlow}
}

// Ensure SingleFlow implements an expected Flow interface if one exists
// type Flow interface {
//    RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error)
// }
// var _ Flow = &SingleFlow{}

# /Users/gopher/Desktop/workspace/adk-go/golang_code.txt
# /Users/gopher/Desktop/workspace/adk-go/auth/auth_preprocessor.go
package auth

import "log"

// "github.com/KennethanCeyer/adk-go/agents"
// "github.com/KennethanCeyer/adk-go/events"
// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions" // For REQUEST_EUC_FUNCTION_CALL_NAME
// "github.com/KennethanCeyer/adk-go/models"

// --- Placeholder types (defined conceptually in previous snippets) ---
/*
type InvocationContext struct { ... }
type LlmRequest struct { ... }
type Event struct { ... }
type LlmAgent interface { ... }
type Session struct { ... }
type State map[string]interface{} // Simplified
type Content struct { ... }
type Part struct { ... }
type FunctionResponse struct { ... }
type AuthConfig struct { ... } // from auth_tool.go
*/
const REQUEST_EUC_FUNCTION_CALL_NAME = "adk_request_credential" // from functions.py

// AuthLlmRequestProcessor handles auth information to build the LLM request.
type AuthLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *AuthLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			return // Not an LlmAgent, nothing to do for auth preprocessing in this context
		}
		_ = llmAgent // Use llmAgent if needed for future logic, e.g. accessing agent-specific auth settings.

		session := invocationCtx.Session
		if session == nil || len(session.Events) == 0 {
			return
		}

		requestEUCFunctionCallIDs := make(map[string]bool)

		// Iterate backwards through events to find the latest user event
		// and its subsequent function responses for EUC.
		var lastUserEventIndex = -1
		for i := len(session.Events) - 1; i >= 0; i-- {
			if session.Events[i].Author == "user" { // Assuming "user" role for user events
				lastUserEventIndex = i
				break
			}
		}

		if lastUserEventIndex == -1 {
			return // No user event found
		}
		
		// Process events after the last user event up to the current point (before new LLM call)
		// In Python, this looks for the *very last* event if it's a user event with FR.
		// The Go logic here simplifies to process function responses directly following the last user message.
		// This might need refinement based on exact Python ADK event ordering for EUC.

		// Check the last event in the session if it's a user event containing function responses
		lastEventInSession := session.Events[len(session.Events)-1]
		if lastEventInSession.Author == "user" {
			responses := lastEventInSession.GetFunctionResponses()
			if len(responses) > 0 {
				for _, fr := range responses {
					if fr.Name == REQUEST_EUC_FUNCTION_CALL_NAME {
						requestEUCFunctionCallIDs[fr.ID] = true
						// Assuming fr.Response is map[string]interface{} that can be unmarshalled into AuthConfig
						var authCfg AuthConfig
						// authCfgData, _ := json.Marshal(fr.Response) // Conceptual
						// json.Unmarshal(authCfgData, &authCfg) // Conceptual
						// Simplified:
						if frMap, ok := fr.Response["authConfig"].(map[string]interface{}); ok {
							// authCfg = AuthConfigFromMap(frMap) // Placeholder for actual conversion
							authHandler := NewAuthHandler(&authCfg) // Placeholder for NewAuthHandler
							authHandler.ParseAndStoreAuthResponse(session.State) // Placeholder
						}
					}
				}
			}
		}


		if len(requestEUCFunctionCallIDs) == 0 {
			return
		}

		// This part of Python logic finds the original function call that requested EUC.
		// And then re-evaluates that original tool call with the now-available auth credentials.
		// This is complex and requires careful event traversal.
		// Simplified conceptual flow:
		for i := len(session.Events) -1 ; i >= 0; i-- {
			event := session.Events[i]
			functionCalls := event.GetFunctionCalls()
			if len(functionCalls) == 0 {
				continue
			}

			var toolsToResume []string // Store original function call IDs that need resuming
			for _, fc := range functionCalls {
				if requestEUCFunctionCallIDs[fc.ID] { // Check if this EUC request matches our processed ones
					// The Python code uses fc.args (AuthToolArguments) to get the *original* function_call_id
					// that needed auth. This detail is missing in current placeholders.
					// Assuming AuthToolArguments is part of fc.Args:
					// originalFcID := fc.Args["functionCallId"].(string)
					// toolsToResume = append(toolsToResume, originalFcID)
				}
			}

			if len(toolsToResume) > 0 {
				// Found the event with function calls that initiated the EUC requests.
				// Now, re-handle these function calls.
				// This implies a call to something like functions.handle_function_calls_async
				// which would then execute the original tool with the new credentials.
				// The result of that would be yielded as an event.
				// Simplified:
				// toolsMap, _ := llmAgent.GetCanonicalTools(&ReadonlyContext{InvocationContext: invocationCtx})
				// toolResponseEvent, err := functionsHandleFunctionCallsAsync(invocationCtx, event, toolsMap, toolsToResume)
				// if err == nil && toolResponseEvent != nil {
				//   outCh <- toolResponseEvent
				// }
				// For this placeholder, we will log that this step is needed.
				log.Printf("Conceptual: Need to re-handle original function calls for IDs: %v with new auth", toolsToResume)
				return // Stop further processing in this simplified version
			}
		}
	}()
	return outCh, nil
}

// Placeholder for AuthHandler and its methods (would be in auth/auth_handler.go)
type AuthHandler struct {
	authConfig *AuthConfig
}
func NewAuthHandler(cfg *AuthConfig) *AuthHandler { return &AuthHandler{authConfig: cfg} }
func (ah *AuthHandler) ParseAndStoreAuthResponse(state map[string]interface{}) { /* ... */ }

// Placeholder for AuthConfig struct (would be in auth/auth_tool.go)
type AuthConfig struct {
	// ... fields like AuthScheme, RawAuthCredential, ExchangedAuthCredential
}

# /Users/gopher/Desktop/workspace/adk-go/agents/base_agent.go
package agents

import (
	"fmt"
	"regexp"
	// Hypothetical Go ADK packages
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/genai/types" // For genai.Content
	// "github.com/KennethanCeyer/adk-go/telemetry"
)

// Forward declarations for types that would be defined in other packages.
// In a real Go ADK, these would be imported.
type Content struct { // Placeholder for genai.types.Content
	Role  string `json:"role,omitempty"`
	Parts []Part `json:"parts,omitempty"`
}

type Part struct { // Placeholder for genai.types.Part
	Text string `json:"text,omitempty"`
	// Other fields like FunctionCall, FunctionResponse, InlineData would be here
}

type Event struct { // Placeholder for events.Event
	InvocationID string       `json:"invocationId,omitempty"`
	Author       string       `json:"author,omitempty"`
	Branch       string       `json:"branch,omitempty"`
	Content      *Content     `json:"content,omitempty"`
	Actions      EventActions `json:"actions,omitempty"` // Placeholder
	ID           string       `json:"id,omitempty"`
	Timestamp    float64      `json:"timestamp,omitempty"`
	// ... other Event fields
}

type EventActions struct { // Placeholder for events.EventActions
	StateDelta map[string]interface{} `json:"stateDelta,omitempty"`
	// ... other EventActions fields
}

type InvocationContext struct { // Placeholder
	Agent            Agent
	Branch           string
	InvocationID     string
	Session          *Session // Placeholder
	EndInvocation    bool
	UserContent      *Content
	ArtifactService  interface{} // Placeholder
	SessionService   interface{} // Placeholder
	MemoryService    interface{} // Placeholder
	LiveRequestQueue interface{} // Placeholder for LiveRequestQueue
	RunConfig        *RunConfig  // Placeholder
}

type Session struct { // Placeholder for sessions.Session
	State map[string]interface{}
	// ... other Session fields
}

type RunConfig struct { // Placeholder for RunConfig
	// ... RunConfig fields
}

type CallbackContext struct { // Placeholder
	InvocationContext *InvocationContext
	EventActions      EventActions
	State             MutableState // Placeholder for a state map that tracks deltas
}

type MutableState map[string]interface{} // Simplified placeholder

func (ms MutableState) HasDelta() bool {
	// In a real implementation, this would check if _delta has items
	return len(ms) > 0 // Simplified
}

// Agent is the base interface for all agents in the Agent Development Kit.
type Agent interface {
	GetName() string
	GetDescription() string
	ParentAgent() Agent
	SetParentAgent(parent Agent) error
	SubAgents() []Agent
	AddSubAgent(subAgent Agent)
	RunAsync(parentCtx *InvocationContext) (<-chan *Event, error)
	RunLive(parentCtx *InvocationContext) (<-chan *Event, error)
	RootAgent() Agent
	FindAgent(name string) Agent
	FindSubAgent(name string) Agent
	GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback
	GetCanonicalAfterAgentCallbacks() []SingleAgentCallback
}

// SingleAgentCallback defines the signature for agent lifecycle callbacks.
type SingleAgentCallback func(callbackCtx *CallbackContext) (*Content, error)

// BaseAgent provides a common structure for agents.
type BaseAgent struct {
	Name                   string                `json:"name"`
	Description            string                `json:"description,omitempty"`
	Parent                 Agent                 `json:"-"` // Exclude from JSON, handle manually
	Subs                   []Agent               `json:"subAgents,omitempty"`
	BeforeAgentCallback    []SingleAgentCallback `json:"-"` // Callbacks are not typically part of serialized state
	AfterAgentCallback     []SingleAgentCallback `json:"-"`
	agentInvocationContext *InvocationContext    // Internal context for the current run
}

// NewBaseAgent creates a new BaseAgent.
// Name must be a valid Go identifier and not "user".
func NewBaseAgent(name string, description string) (*BaseAgent, error) {
	if !isValidIdentifier(name) {
		return nil, fmt.Errorf("invalid agent name: `%s`. Agent name must be a valid identifier", name)
	}
	if name == "user" {
		return nil, fmt.Errorf("agent name cannot be `user`. `user` is reserved for end-user's input")
	}
	return &BaseAgent{
		Name:        name,
		Description: description,
		Subs:        make([]Agent, 0),
	}, nil
}

func (ba *BaseAgent) GetName() string {
	return ba.Name
}

func (ba *BaseAgent) GetDescription() string {
	return ba.Description
}

func (ba *BaseAgent) ParentAgent() Agent {
	return ba.Parent
}

func (ba *BaseAgent) SetParentAgent(parent Agent) error {
	if ba.Parent != nil && ba.Parent != parent {
		return fmt.Errorf("agent `%s` already has a parent agent: `%s`, cannot add to: `%s`", ba.Name, ba.Parent.GetName(), parent.GetName())
	}
	ba.Parent = parent
	return nil
}

func (ba *BaseAgent) SubAgents() []Agent {
	return ba.Subs
}

func (ba *BaseAgent) AddSubAgent(subAgent Agent) {
	if err := subAgent.SetParentAgent(ba); err != nil {
		// Or handle error as appropriate, e.g. log and skip
		fmt.Printf("Error setting parent for sub-agent %s: %v\n", subAgent.GetName(), err)
		return
	}
	ba.Subs = append(ba.Subs, subAgent)
}

// RunAsync is the entry method to run an agent via text-based conversation.
func (ba *BaseAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	// In Go, we'd typically use channels for async generators.
	// The actual implementation of _runAsyncImpl would push events to this channel.
	// The telemetry.tracer logic would wrap the core processing.
	// For this snippet, we'll simulate the channel.

	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run [%s]", ba.Name)):
		
		event, err := ba.handleBeforeAgentCallback(ctx)
		if err != nil {
			// ì–´ë–»ê²Œ ì—ëŸ¬ë¥¼ outChìœ¼ë¡œ ë³´ë‚¼ ì§€ ê³ ë¯¼ í•„ìš”. Eventì— Error í•„ë“œ ì¶”ê°€?
			// For now, log and potentially send an error event.
			fmt.Printf("Error in beforeAgentCallback for %s: %v\n", ba.Name, err)
			// outCh <- &events.Event{Error: err.Error()} // Conceptual
			return
		}
		if event != nil {
			outCh <- event
		}
		if ctx.EndInvocation {
			return
		}

		err = ba.runAsyncImpl(ctx, outCh) // Pass channel to implementation
		if err != nil {
			fmt.Printf("Error in runAsyncImpl for %s: %v\n", ba.Name, err)
			return
		}

		if ctx.EndInvocation {
			return
		}

		event, err = ba.handleAfterAgentCallback(ctx)
		if err != nil {
			fmt.Printf("Error in afterAgentCallback for %s: %v\n", ba.Name, err)
			return
		}
		if event != nil {
			outCh <- event
		}
	}()

	return outCh, nil
}

// runAsyncImpl is the core logic to run this agent via text-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runAsyncImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	// This is where the Python _run_async_impl's `yield Event(...)` would happen.
	// In Go, we send to the channel.
	// Example: outCh <- &events.Event{Author: ba.Name, Content: ...}
	return fmt.Errorf("_runAsyncImpl for %T is not implemented", ba)
}

// RunLive is the entry method to run an agent via video/audio-based conversation.
func (ba *BaseAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	ctx := ba.createInvocationContext(parentCtx)
	ba.agentInvocationContext = ctx

	go func() {
		defer close(outCh)
		// with telemetry.tracer.StartAsCurrentSpan(fmt.Sprintf("agent_run_live [%s]", ba.Name)):
		err := ba.runLiveImpl(ctx, outCh) // Pass channel
		if err != nil {
			fmt.Printf("Error in runLiveImpl for %s: %v\n", ba.Name, err)
			// Potentially send an error event
		}
	}()
	return outCh, nil
}

// runLiveImpl is the core logic to run this agent via video/audio-based conversation.
// Subclasses must override this.
func (ba *BaseAgent) runLiveImpl(ctx *InvocationContext, outCh chan<- *Event) error {
	return fmt.Errorf("_runLiveImpl for %T is not implemented", ba)
}

func (ba *BaseAgent) RootAgent() Agent {
	current := ba
	for current.ParentAgent() != nil {
		current = current.ParentAgent().(*BaseAgent) // Assuming BaseAgent for simplicity
	}
	return current
}

func (ba *BaseAgent) FindAgent(name string) Agent {
	if ba.Name == name {
		return ba
	}
	return ba.FindSubAgent(name)
}

func (ba *BaseAgent) FindSubAgent(name string) Agent {
	for _, subAgent := range ba.Subs {
		if found := subAgent.FindAgent(name); found != nil {
			return found
		}
	}
	return nil
}

func (ba *BaseAgent) createInvocationContext(parentCtx *InvocationContext) *InvocationContext {
	// Create a copy and update agent and branch
	// This is a simplified deep copy. Real implementation needs care.
	ctxCopy := *parentCtx
	ctxCopy.Agent = ba
	if parentCtx.Branch != "" {
		ctxCopy.Branch = fmt.Sprintf("%s.%s", parentCtx.Branch, ba.Name)
	} else {
		ctxCopy.Branch = ba.Name
	}
	return &ctxCopy
}

func (ba *BaseAgent) GetCanonicalBeforeAgentCallbacks() []SingleAgentCallback {
	return ba.BeforeAgentCallback
}

func (ba *BaseAgent) GetCanonicalAfterAgentCallbacks() []SingleAgentCallback {
	return ba.AfterAgentCallback
}

func (ba *BaseAgent) handleBeforeAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalBeforeAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalBeforeAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("beforeAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions, // Assuming EventActions is part of CallbackContext
			}
			ctx.EndInvocation = true
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			Actions:      callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

func (ba *BaseAgent) handleAfterAgentCallback(ctx *InvocationContext) (*Event, error) {
	var retEvent *Event
	if len(ba.GetCanonicalAfterAgentCallbacks()) == 0 {
		return nil, nil
	}

	callbackCtx := &CallbackContext{InvocationContext: ctx, State: ctx.Session.State} // Simplified state handling

	for _, callback := range ba.GetCanonicalAfterAgentCallbacks() {
		content, err := callback(callbackCtx)
		if err != nil {
			return nil, fmt.Errorf("afterAgentCallback error: %w", err)
		}
		if content != nil {
			retEvent = &Event{
				InvocationID: ctx.InvocationID,
				Author:       ba.Name,
				Branch:       ctx.Branch,
				Content:      content,
				Actions:      callbackCtx.EventActions,
			}
			return retEvent, nil
		}
	}

	if callbackCtx.State.HasDelta() { // Simplified
		retEvent = &Event{
			InvocationID: ctx.InvocationID,
			Author:       ba.Name,
			Branch:       ctx.Branch,
			// Content might be nil if callback only changed state
			Actions: callbackCtx.EventActions,
		}
	}
	return retEvent, nil
}

// isValidIdentifier checks if a string is a valid Go identifier (simplified).
// Go identifiers are more restrictive than Python's.
func isValidIdentifier(name string) bool {
	if name == "" {
		return false
	}
	// Regex for a simplified Go identifier: starts with a letter, followed by letters or digits.
	// Go allows Unicode letters/digits. This regex is simplified.
	// Actual Go spec: letter { letter | unicode_digit } .
	// For ADK agent names, we might enforce stricter ASCII for interoperability or simplicity.
	matched, _ := regexp.MatchString(`^[a-zA-Z_][a-zA-Z0-9_]*$`, name)
	return matched
}

# /Users/gopher/Desktop/workspace/adk-go/agents/llm_agent.go
package agents

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/tools" // Placeholder for tools package
	// "github.com/KennethanCeyer/adk-go/models" // Placeholder for models package
	// "github.com/KennethanCeyer/adk-go/examples" // Placeholder for examples package
	// "github.com/KennethanCeyer/adk-go/planners" // Placeholder for planners package
	// "github.com/KennethanCeyer/adk-go/codeexecutors" // Placeholder for code executors
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, genai.Content etc.
)

// --- Forward declarations / Placeholders ---
// These would be actual types in a full Go ADK.

type BaseLlm struct { // Placeholder for models.BaseLlm
	ModelName string
}

type GenerateContentConfig struct { // Placeholder for genai.GenerateContentConfig
	SystemInstruction   string        `json:"systemInstruction,omitempty"`
	Tools               []Tool        `json:"tools,omitempty"`         // Placeholder for genai.Tool
	ResponseSchema      interface{}   `json:"responseSchema,omitempty"` // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string        `json:"responseMimeType,omitempty"`
	ThinkingConfig      interface{}   `json:"thinkingConfig,omitempty"`   // Placeholder for genai.ThinkingConfig
	Temperature         *float32      `json:"temperature,omitempty"`
	SafetySettings      []interface{} `json:"safetySettings,omitempty"` // Placeholder for genai.SafetySetting
}

type LlmRequest struct { // Placeholder for models.LlmRequest
	Model               string
	Config              *GenerateContentConfig
	Contents            []Content
	ToolsDict           map[string]Tool // Placeholder
	LiveConnectConfig   interface{}     // Placeholder for genai.LiveConnectConfig
	SystemInstruction   string          // Added for simplicity, original ADK appends to config
	OutputSchema        interface{}     // Placeholder for pydantic.BaseModel
	ResponseMIMEType    string
}

func (lr *LlmRequest) AppendInstructions(instructions []string) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	if lr.Config.SystemInstruction != "" {
		lr.Config.SystemInstruction += "\n\n"
	}
	lr.Config.SystemInstruction += strings.Join(instructions, "\n\n")
}

func (lr *LlmRequest) SetOutputSchema(schema interface{}) {
	if lr.Config == nil {
		lr.Config = &GenerateContentConfig{}
	}
	lr.Config.ResponseSchema = schema
	lr.Config.ResponseMIMEType = "application/json"
}

type LlmResponse struct { // Placeholder for models.LlmResponse
	Content         *Content    `json:"content,omitempty"`
	ErrorCode       string      `json:"errorCode,omitempty"`
	ErrorMessage    string      `json:"errorMessage,omitempty"`
	Partial         bool        `json:"partial,omitempty"`
	CustomMetadata  map[string]interface{} `json:"customMetadata,omitempty"`
}


type Tool interface { // Placeholder for tools.BaseTool
	GetName() string
	GetDescription() string
	GetDeclaration() (*ToolDeclaration, error) // Placeholder for genai.FunctionDeclaration
	ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error
}

type ToolDeclaration struct { // Placeholder for genai.FunctionDeclaration
	Name        string      `json:"name"`
	Description string      `json:"description"`
	Parameters  interface{} `json:"parameters"`
}

type FunctionTool struct { // Placeholder for tools.FunctionTool
	BaseToolImpl
	fn interface{}
}

func NewFunctionTool(fn interface{}) *FunctionTool {
	// Simplified name/description extraction
	name := "unknown_function"
	// In Go, reflection for function name/doc is more involved
	// For now, users might need to provide this explicitly if FunctionTool wraps a raw func
	return &FunctionTool{BaseToolImpl: BaseToolImpl{ToolName: name}, fn: fn}
}

func (ft *FunctionTool) GetName() string { return ft.ToolName }
func (ft *FunctionTool) GetDescription() string { return ft.ToolDescription }
func (ft *FunctionTool) GetDeclaration() (*ToolDeclaration, error) { /* TODO */ return nil, nil }
func (ft *FunctionTool) ProcessLLMRequest(toolCtx *ToolContext, llmReq *LlmRequest) error { /* TODO */ return nil }


type BaseToolset interface { // Placeholder for tools.BaseToolset
	GetTools(readonlyCtx *ReadonlyContext) ([]Tool, error)
}

type Example struct { // Placeholder for examples.Example
	Input  Content   `json:"input"`
	Output []Content `json:"output"`
}

type BaseExampleProvider interface { // Placeholder for examples.BaseExampleProvider
	GetExamples(query string) ([]Example, error)
}

type Planner interface { // Placeholder for planners.BasePlanner
	// Methods would be defined here
}

type CodeExecutor interface { // Placeholder for codeexecutors.BaseCodeExecutor
	// Methods would be defined here
}

type ToolContext struct { // Placeholder for tools.ToolContext
	InvocationContext *InvocationContext
	State             MutableState
	// ... other fields like FunctionCallID, EventActions
}

// Represents ReadonlyContext
type ReadonlyContext struct {
	InvocationContext *InvocationContext
}

func (rc *ReadonlyContext) GetState() map[string]interface{} {
	// Return a read-only copy or wrapper
	// For simplicity, returning the direct map for now, assuming no modification.
	if rc.InvocationContext != nil && rc.InvocationContext.Session != nil {
		return rc.InvocationContext.Session.State
	}
	return make(map[string]interface{})
}


// LLMProvider interface (simplified from previous example)
type LLMProvider interface {
    GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error)
    GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error)
    // Potentially a Connect method for bidirectional streaming if needed for live mode
}

// MockLLMProvider for LlmAgent
type MockLLM struct {
	BaseLlm
}

func (m *MockLLM) GenerateContentStream(ctx context.Context, request *LlmRequest) (<-chan *LlmResponse, error) {
	// Simulate LLM call
	log.Printf("MockLLM GenerateContentStream called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	ch := make(chan *LlmResponse, 1)
	go func() {
		defer close(ch)
		// Simplified: just echo back part of the first user message if available
		responseText := "Default mock response"
		if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
			responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
		}
		ch <- &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}
	}()
	return ch, nil
}

func (m *MockLLM) GenerateContent(ctx context.Context, request *LlmRequest) (*LlmResponse, error) {
	log.Printf("MockLLM GenerateContent called with model: %s, instruction: %s", request.Model, request.Config.SystemInstruction)
	responseText := "Default mock response"
	if len(request.Contents) > 0 && len(request.Contents[0].Parts) > 0 {
		responseText = "Mock response to: " + request.Contents[0].Parts[0].Text
	}
	return &LlmResponse{Content: &Content{Role: "model", Parts: []Part{{Text: responseText}}}}, nil
}


// SingleBeforeModelCallback defines the signature for a single before-model callback.
type SingleBeforeModelCallback func(callbackCtx *CallbackContext, llmReq *LlmRequest) (*LlmResponse, error)

// SingleAfterModelCallback defines the signature for a single after-model callback.
type SingleAfterModelCallback func(callbackCtx *CallbackContext, llmResp *LlmResponse) (*LlmResponse, error)

// SingleBeforeToolCallback defines the signature for a single before-tool callback.
type SingleBeforeToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext) (map[string]interface{}, error)

// SingleAfterToolCallback defines the signature for a single after-tool callback.
type SingleAfterToolCallback func(tool Tool, args map[string]interface{}, toolCtx *ToolContext, toolResponse map[string]interface{}) (map[string]interface{}, error)

// InstructionProvider defines a function that provides an instruction string.
type InstructionProvider func(readonlyCtx *ReadonlyContext) (string, error)

// ToolUnion represents a type that can be a callable, a BaseTool, or a BaseToolset.
// In Go, this would typically be handled by interfaces or by checking types at runtime.
// For this conceptual translation, we'll use `interface{}` and runtime type assertions.
type ToolUnion interface{}

// ExamplesUnion represents types that can provide examples.
type ExamplesUnion interface{} // list[Example] or BaseExampleProvider

// LlmAgent is an LLM-based Agent.
type LlmAgent struct {
	BaseAgent
	AgentModel                  interface{} // string or *BaseLlm (actual model instance)
	AgentInstruction            interface{} // string or InstructionProvider
	AgentGlobalInstruction      interface{} // string or InstructionProvider
	AgentTools                  []ToolUnion
	AgentGenerateContentConfig  *GenerateContentConfig
	DisallowTransferToParent    bool
	DisallowTransferToPeers     bool
	IncludeContents             string // "default" or "none"
	InputSchema                 interface{} // reflect.Type of a struct, or nil
	OutputSchema                interface{} // reflect.Type of a struct, or nil
	OutputKey                   string
	AgentPlanner                Planner
	AgentCodeExecutor           CodeExecutor
	AgentExamples               ExamplesUnion
	AgentBeforeModelCallbacks   []SingleBeforeModelCallback
	AgentAfterModelCallbacks    []SingleAfterModelCallback
	AgentBeforeToolCallbacks    []SingleBeforeToolCallback
	AgentAfterToolCallbacks     []SingleAfterToolCallback

	// Internal flow strategy
	llmFlow LlmFlow
}

// LlmFlow defines the behavior of the LLM agent.
type LlmFlow interface {
	RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error)
	RunLive(invocationCtx *InvocationContext) (<-chan *Event, error)
}

// NewLlmAgent creates a new LlmAgent.
// This constructor attempts to mirror Pydantic's initialization logic.
func NewLlmAgent(name, description string, model interface{}, instruction interface{}) (*LlmAgent, error) {
	base, err := NewBaseAgent(name, description)
	if err != nil {
		return nil, err
	}

	agent := &LlmAgent{
		BaseAgent:        *base,
		AgentModel:       model,
		AgentInstruction: instruction,
		IncludeContents:  "default",
		// Default to AutoFlow which includes SingleFlow + agent transfer
		llmFlow: newAutoFlow(), // Placeholder for flow initialization
	}

	// Simulating Pydantic's model_post_init and field_validator logic
	if err := agent.validateAndProcessConfig(); err != nil {
		return nil, err
	}

	return agent, nil
}

func (a *LlmAgent) validateAndProcessConfig() error {
	if err := a.checkOutputSchema(); err != nil {
		return err
	}
	if err := a.validateGenerateContentConfig(); err != nil {
		return err
	}
	// In Python, Pydantic's model_post_init calls __set_parent_agent_for_sub_agents
	// This is handled by AddSubAgent in Go's BaseAgent.

	// Determine flow based on transfer disallowed flags
	if a.DisallowTransferToParent && a.DisallowTransferToPeers && len(a.Subs) == 0 {
		a.llmFlow = newSingleFlow() // Placeholder
	} else {
		a.llmFlow = newAutoFlow() // Placeholder
	}

	return nil
}


// RunAsync implements the Agent interface.
// It delegates to the internal LlmFlow.
func (a *LlmAgent) RunAsync(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx) // from BaseAgent
	a.agentInvocationContext = ctx // Store context for this agent's run
	return a.llmFlow.RunAsync(ctx)
}

// RunLive implements the Agent interface.
func (a *LlmAgent) RunLive(parentCtx *InvocationContext) (<-chan *Event, error) {
	ctx := a.createInvocationContext(parentCtx)
	a.agentInvocationContext = ctx
	return a.llmFlow.RunLive(ctx)
}


func (a *LlmAgent) CanonicalModel() (BaseLlm, error) {
	if modelInst, ok := a.AgentModel.(BaseLlm); ok {
		return modelInst, nil
	}
	if modelStr, ok := a.AgentModel.(string); ok && modelStr != "" {
		// Placeholder for LLMRegistry.NewLlm(modelStr)
		// In a real Go ADK, this would look up and instantiate the model.
		return MockLLM{BaseLlm{ModelName: modelStr}}, nil
	}

	current := a.ParentAgent()
	for current != nil {
		if llmAgent, ok := current.(*LlmAgent); ok {
			return llmAgent.CanonicalModel()
		}
		current = current.ParentAgent()
	}
	return MockLLM{}, fmt.Errorf("no model found for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentInstruction.(func(ctx *ReadonlyContext) (string, error)); ok { // Allow func type as well
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, fmt.Errorf("invalid instruction type for agent %s", a.Name)
}

func (a *LlmAgent) CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error) {
	if instructionStr, ok := a.AgentGlobalInstruction.(string); ok {
		return instructionStr, false, nil
	}
	if provider, ok := a.AgentGlobalInstruction.(InstructionProvider); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	if provider, ok := a.AgentGlobalInstruction.(func(ctx *ReadonlyContext) (string, error)); ok {
		instr, err := provider(ctx)
		return instr, true, err
	}
	return "", false, nil // Global instruction can be empty
}

func (a *LlmAgent) CanonicalTools(ctx *ReadonlyContext) ([]Tool, error) {
	var resolvedTools []Tool
	for _, toolUnion := range a.AgentTools {
		switch t := toolUnion.(type) {
		case Tool:
			resolvedTools = append(resolvedTools, t)
		case func(interface{}) interface{}: // Simplified check for a callable
			// This is a very basic check. Robustly handling arbitrary callables
			// and converting them to tools.Tool would require more reflection
			// or specific registration mechanisms.
			// For now, wrapping it in a conceptual FunctionTool.
			resolvedTools = append(resolvedTools, NewFunctionTool(t))
		case BaseToolset:
			toolsFromSet, err := t.GetTools(ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to get tools from toolset: %w", err)
			}
			resolvedTools = append(resolvedTools, toolsFromSet...)
		default:
			return nil, fmt.Errorf("unsupported tool type: %T", t)
		}
	}
	return resolvedTools, nil
}


func (a *LlmAgent) GetCanonicalBeforeModelCallbacks() []SingleBeforeModelCallback {
	return a.AgentBeforeModelCallbacks
}

func (a *LlmAgent) GetCanonicalAfterModelCallbacks() []SingleAfterModelCallback {
	return a.AgentAfterModelCallbacks
}

func (a *LlmAgent) GetCanonicalBeforeToolCallbacks() []SingleBeforeToolCallback {
	return a.AgentBeforeToolCallbacks
}

func (a *LlmAgent) GetCanonicalAfterToolCallbacks() []SingleAfterToolCallback {
	return a.AgentAfterToolCallbacks
}

func (a *LlmAgent) maybeSaveOutputToState(event *Event) {
	if a.OutputKey == "" || !eventIsFinalResponse(event) || event.Content == nil || len(event.Content.Parts) == 0 {
		return
	}

	var resultData interface{}
	var combinedText []string
	for _, part := range event.Content.Parts {
		if part.Text != "" {
			combinedText = append(combinedText, part.Text)
		}
		// In a real scenario, you'd handle other part types like function calls/responses if they constituted the "output"
	}
	fullText := strings.Join(combinedText, "\n")


	if a.OutputSchema != nil {
		// In Go, Pydantic's model_validate_json would be equivalent to json.Unmarshal into a struct.
		// This requires a.OutputSchema to be a reflect.Type of the target struct.
		// For simplicity, this placeholder assumes fullText is valid JSON for the schema.
		// A real implementation would use json.Unmarshal(byte(fullText), &targetStructInstance)
		// and then potentially convert targetStructInstance to map[string]interface{} if needed.
		var tempMap map[string]interface{}
		// This is a simplified placeholder. Real unmarshalling and validation needed.
		if err := json.Unmarshal([]byte(fullText), &tempMap); err == nil {
			resultData = tempMap
		} else {
			log.Printf("Warning: Failed to parse output for key '%s' against schema: %v. Storing raw text.", a.OutputKey, err)
			resultData = fullText // Fallback to raw text
		}
	} else {
		resultData = fullText
	}

	if event.Actions.StateDelta == nil {
		event.Actions.StateDelta = make(map[string]interface{})
	}
	event.Actions.StateDelta[a.OutputKey] = resultData
}

func (a *LlmAgent) checkOutputSchema() error {
	if a.OutputSchema == nil {
		return nil
	}

	if !a.DisallowTransferToParent || !a.DisallowTransferToPeers {
		log.Println(
			fmt.Sprintf("Warning: Invalid config for agent %s: outputSchema cannot co-exist with agent transfer configurations. Setting disallowTransferToParent=true, disallowTransferToPeers=true", a.Name),
		)
		a.DisallowTransferToParent = true
		a.DisallowTransferToPeers = true
	}

	if len(a.SubAgents()) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, subAgents must be empty to disable agent transfer", a.Name)
	}

	if len(a.AgentTools) > 0 {
		return fmt.Errorf("invalid config for agent %s: if outputSchema is set, tools must be empty", a.Name)
	}
	return nil
}

func (a *LlmAgent) validateGenerateContentConfig() error {
	if a.AgentGenerateContentConfig == nil {
		a.AgentGenerateContentConfig = &GenerateContentConfig{} // Ensure it's not nil
		return nil
	}
	if a.AgentGenerateContentConfig.ThinkingConfig != nil {
		return fmt.Errorf("thinkingConfig should be set via LlmAgent.Planner")
	}
	if len(a.AgentGenerateContentConfig.Tools) > 0 {
		return fmt.Errorf("all tools must be set via LlmAgent.Tools")
	}
	if a.AgentGenerateContentConfig.SystemInstruction != "" {
		return fmt.Errorf("system instruction must be set via LlmAgent.Instruction or LlmAgent.GlobalInstruction")
	}
	if a.AgentGenerateContentConfig.ResponseSchema != nil {
		return fmt.Errorf("response schema must be set via LlmAgent.OutputSchema")
	}
	return nil
}

// Helper, would be part of event logic
func eventIsFinalResponse(e *Event) bool {
	if e.Actions.StateDelta["skipSummarization"] == true { // Assuming skipSummarization is stored in StateDelta
		return true
	}
	// Simplified: In Python ADK, this checks for function calls/responses and partial flags.
	// Here, we'll assume an event is final if it's not marked partial and has content.
	return e.Content != nil && !e.Partial // Placeholder for e.Partial
}

// --- Placeholder Flow Implementations ---
// In a real Go ADK, these would be more fleshed out and likely in their own package.

type SingleFlow struct {
	// RequestProcessors  []BaseLlmRequestProcessor  // Placeholder
	// ResponseProcessors []BaseLlmResponseProcessor // Placeholder
}

func newSingleFlow() *SingleFlow {
	// Initialize processors
	return &SingleFlow{}
}

func (sf *SingleFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)
		llmReq := &LlmRequest{} // Initialize LlmRequest

		// Simulate preprocessing (would iterate through sf.RequestProcessors)
		if err := sf.preprocessAsync(invocationCtx, llmReq); err != nil {
			log.Printf("Error during preprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()} // Conceptual error event
			return
		}

		llmAgent, ok := invocationCtx.Agent.(*LlmAgent)
		if !ok {
			log.Printf("Error: agent is not an LlmAgent")
			return
		}
		llm, err := llmAgent.CanonicalModel()
		if err != nil {
			log.Printf("Error getting canonical model: %v", err)
			return
		}
		
		// Simplified LLM call for SingleFlow
		// A real implementation would handle streaming, function calling loops, etc.
		var llmResponse *LlmResponse

		// This is a simplification. The Python ADK has a loop for multiple LLM calls if tools are used.
		// For this Go conceptual translation, we'll assume a single LLM call or a very simplified tool use.
		if provider, ok := llm.(LLMProvider); ok {
			// Use GenerateContent for non-streaming for simplicity in this conceptual flow
			llmResponse, err = provider.GenerateContent(context.Background(), llmReq)
			if err != nil {
				log.Printf("Error calling LLM: %v", err)
				// outCh <- &Event{Error: err.Error()}
				return
			}
		} else {
			log.Printf("Error: LLM model does not implement LLMProvider interface")
			return
		}
		

		modelResponseEvent := &Event{
			ID:            newID(), // Generate new event ID
			InvocationID:  invocationCtx.InvocationID,
			Author:        invocationCtx.Agent.GetName(),
			Branch:        invocationCtx.Branch,
			Content:       llmResponse.Content, // Simplified
			// Actions, ErrorCode, ErrorMessage, etc. would be populated
		}

		// Simulate postprocessing (would iterate through sf.ResponseProcessors)
		if err := sf.postprocessAsync(invocationCtx, llmReq, llmResponse, modelResponseEvent, outCh); err != nil {
			log.Printf("Error during postprocessing: %v", err)
			// outCh <- &Event{Error: err.Error()}
			return
		}
		
		// If not handled by post-processors (e.g. tool calls), yield the direct LLM response event
		if modelResponseEvent.Content != nil || modelResponseEvent.Actions.StateDelta != nil { // Yield if there's something to yield
			outCh <- modelResponseEvent
		}

	}()
	return outCh, nil
}


func (sf *SingleFlow) RunLive(invocationCtx *InvocationContext) (<-chan *Event, error) {
	// Simplified RunLive, a real one would be more complex with bidirectional streaming
	return sf.RunAsync(invocationCtx) // Fallback for now
}

func (sf *SingleFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	// Conceptual: Iterate through request_processors from Python ADK
	// Example: basic.request_processor, instructions.request_processor, etc.
	// For this Go version, we'll call helper methods directly for some core logic.
	agent := invocationCtx.Agent.(*LlmAgent) // Assuming LlmAgent

	// Basic processor logic
	modelInstance, err := agent.CanonicalModel()
	if err != nil {
		return err
	}
	llmReq.Model = modelInstance.ModelName // Assuming BaseLlm has ModelName
	if agent.AgentGenerateContentConfig != nil {
		cfgCopy := *agent.AgentGenerateContentConfig
		llmReq.Config = &cfgCopy
	} else {
		llmReq.Config = &GenerateContentConfig{}
	}
	if agent.OutputSchema != nil {
		llmReq.SetOutputSchema(agent.OutputSchema)
	}
	// ... (add live_connect_config population from RunConfig)

	// Instructions processor logic
	// Global Instruction
	rootAgent := agent.RootAgent().(*LlmAgent) // Assuming LlmAgent
	if rootAgent.AgentGlobalInstruction != nil {
		instr, _, err := rootAgent.CanonicalGlobalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		// Simplified: In Python, instructions_utils.inject_session_state is used.
		// For Go, this would involve a similar template processing.
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	// Agent Instruction
	if agent.AgentInstruction != nil {
		instr, _, err := agent.CanonicalInstruction(&ReadonlyContext{InvocationContext: invocationCtx})
		if err != nil { return err }
		processedInstr, err := processInstructionTemplate(instr, &ReadonlyContext{InvocationContext: invocationCtx} )
		if err != nil { return err }
		llmReq.AppendInstructions([]string{processedInstr})
	}
	
	// Identity processor
	var si []string
	si = append(si, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName()))
	if agent.GetDescription() != "" {
		si = append(si, fmt.Sprintf(" The description about you is \"%s\"", agent.GetDescription()))
	}
	llmReq.AppendInstructions(si)

	// Contents processor logic (simplified)
	if agent.IncludeContents != "none" {
		llmReq.Contents = getContentsForLlm(invocationCtx.Branch, invocationCtx.Session.Events, agent.GetName())
	}

	// Tool processing (adding declarations to llmReq.Config.Tools)
	tools, err := agent.CanonicalTools(&ReadonlyContext{InvocationContext: invocationCtx})
	if err != nil { return err}
	if len(tools) > 0 {
		if llmReq.Config.Tools == nil {
			llmReq.Config.Tools = make([]Tool, 0)
		}
	}
	for _, tool := range tools {
		// Simplified: directly appending, Python ADK has more complex logic for FunctionDeclaration
		llmReq.Config.Tools = append(llmReq.Config.Tools, tool) // This assumes tool itself is the declaration or can provide it
		
		// Store tool in llmReq.ToolsDict for later execution lookup by name
		if llmReq.ToolsDict == nil {
			llmReq.ToolsDict = make(map[string]Tool)
		}
		llmReq.ToolsDict[tool.GetName()] = tool
	}

	return nil
}

func (sf *SingleFlow) postprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest, llmResp *LlmResponse, modelResponseEvent *Event, outCh chan<- *Event) error {
	// Conceptual: Iterate through response_processors
	// Example: _nl_planning.response_processor, _code_execution.response_processor
	
	// Handle function calls (simplified from functions.py)
	if llmResp.Content != nil {
		var functionCalls []struct{ Name string; Args map[string]interface{}; ID string } // Simplified representation
		for _, part := range llmResp.Content.Parts {
			// Placeholder: In Python this uses part.function_call
			// For Go, we'd check a similar field if LlmResponse.Content.Parts had FunctionCall type
			// For this example, assume FunctionCall is identified by a specific structure in Part
			// For now, this logic is highly simplified and conceptual
			if strings.HasPrefix(part.Text, "CALL_TOOL:") { // Very basic trigger
				parts := strings.SplitN(strings.TrimPrefix(part.Text, "CALL_TOOL:"), "(", 2)
				name := parts[0]
				argsStr := ""
				if len(parts) > 1 {
					argsStr = strings.TrimSuffix(parts[1], ")")
				}
				var args map[string]interface{}
				json.Unmarshal([]byte(argsStr), &args) // Simplified arg parsing
				functionCalls = append(functionCalls, struct{ Name string; Args map[string]interface{}; ID string }{Name: name, Args: args, ID: newID()})

				// Populate client function call ID if LLM didn't provide one
				modelResponseEvent.Content.Parts[0].Text = "" // Clear text part conceptually
				// modelResponseEvent.Content.Parts[0].FunctionCall = &genai.FunctionCall{Name: name, Args: args, ID: ...}
			}
		}

		if len(functionCalls) > 0 {
			// In Python ADK, this calls functions.handle_function_calls_async
			// which then calls tool.run_async and creates a response event.
			// This is a very complex part involving callbacks, state updates, etc.
			// Here's a highly simplified conceptual flow:
			var toolResponseParts []Part
			for _, fc := range functionCalls {
				tool, ok := llmReq.ToolsDict[fc.Name]
				if !ok {
					log.Printf("Tool %s not found", fc.Name)
					toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Error: Tool %s not found", fc.Name)})
					continue
				}
				// Conceptual tool execution
				// toolResult, err := tool.Execute(context.Background(), fc.Args) // This is complex
				// Simplified:
				toolResponseParts = append(toolResponseParts, Part{Text: fmt.Sprintf("Tool %s called with %v (mocked)", fc.Name, fc.Args)})
			}
			
			// Create a new event for tool responses and send it
			toolResponseEvent := &Event{
				ID:            newID(),
				InvocationID:  invocationCtx.InvocationID,
				Author:        invocationCtx.Agent.GetName(),
				Branch:        invocationCtx.Branch,
				Content:       &Content{Role: "model", Parts: toolResponseParts}, // Simplified: role should be 'user' for function response in Gemini
			}
			outCh <- toolResponseEvent
			
			// The original LLM response event (modelResponseEvent) that *contained* the function call
			// is also yielded. Then, the flow would typically make *another* LLM call with the tool responses.
			// For simplicity, we will stop here in this conceptual snippet.
			// The python ADK has a loop in _run_one_step_async for this.
			modelResponseEvent.Content = nil // After tool call, the original content is usually superseded or followed by tool response processing
		}
	}
	
	return nil
}

type AutoFlow struct {
	SingleFlow
	// agentTransferRequestProcessor // Placeholder for agent_transfer.request_processor
}

func newAutoFlow() *AutoFlow {
	return &AutoFlow{SingleFlow: *newSingleFlow()}
}

func (af *AutoFlow) preprocessAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) error {
	if err := af.SingleFlow.preprocessAsync(invocationCtx, llmReq); err != nil {
		return err
	}
	// Agent transfer logic
	agent := invocationCtx.Agent.(*LlmAgent)
	transferTargets := getTransferTargets(agent)
	if len(transferTargets) > 0 {
		llmReq.AppendInstructions([]string{buildTargetAgentsInstructions(agent, transferTargets)})
		// Conceptually add transfer_to_agent tool
		// transferTool := NewFunctionTool(transferToAgent) // transferToAgent needs to be defined
		// ... add transferTool to llmReq.Config.Tools and llmReq.ToolsDict
	}
	return nil
}

// Helper to simulate Python's list comprehension and filtering for contents
func getContentsForLlm(currentBranch string, allEvents []*Event, agentName string) []Content {
	var contents []Content
	// This is a simplified version of Python ADK's _get_contents, which has complex logic
	// for rearranging events, handling async function responses, and filtering by branch.
	for _, event := range allEvents {
		if event.Content != nil && len(event.Content.Parts) > 0 {
			// Simplified branch check and foreign event conversion
			if isEventOnBranch(currentBranch, event) && !isAuthEvent(event) {
				if isOtherAgentReply(agentName, event) {
					contents = append(contents, *convertForeignEvent(event))
				} else {
					// Deep copy content before modifying (e.g. removing client func call ID)
					contentCopy := *event.Content 
					// removeClientFunctionCallID(&contentCopy) // Placeholder for actual logic
					contents = append(contents, contentCopy)
				}
			}
		}
	}
	return contents
}

// Simplified placeholders for helper functions from Python's contents.py
func isEventOnBranch(invocationBranch string, event *Event) bool { 
	if invocationBranch == "" || event.Branch == "" { return true }
	return strings.HasPrefix(invocationBranch, event.Branch)
}
func isAuthEvent(event *Event) bool { return false /* TODO */ }
func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return event.Author != currentAgentName && event.Author != "user"
}
func convertForeignEvent(event *Event) *Content { 
	// Simplified conversion
	return &Content{Role: "user", Parts: []Part{{Text: fmt.Sprintf("Context from %s: %s", event.Author, event.Content.Parts[0].Text)}}}
}
func newID() string { return "temp-id" } // Placeholder for ID generation

func getTransferTargets(agent *LlmAgent) []Agent {
    var targets []Agent
    targets = append(targets, agent.SubAgents()...) // Assuming SubAgents returns []Agent

    if agent.ParentAgent() != nil {
        if llmParent, ok := agent.ParentAgent().(*LlmAgent); ok {
            if !agent.DisallowTransferToParent {
                targets = append(targets, llmParent)
            }
            if !agent.DisallowTransferToPeers {
                for _, peerAgent := range llmParent.SubAgents() {
                    if peerAgent.GetName() != agent.GetName() {
                        targets = append(targets, peerAgent)
                    }
                }
            }
        }
    }
    return targets
}

func buildTargetAgentsInfo(targetAgent Agent) string {
    return fmt.Sprintf("\nAgent name: %s\nAgent description: %s\n", targetAgent.GetName(), targetAgent.GetDescription())
}

func buildTargetAgentsInstructions(agent *LlmAgent, targetAgents []Agent) string {
    var sb strings.Builder
    sb.WriteString("\nYou have a list of other agents to transfer to:\n")
    for _, target := range targetAgents {
        sb.WriteString(buildTargetAgentsInfo(target))
    }
    sb.WriteString("\nIf you are the best to answer the question according to your description, you can answer it.")
    sb.WriteString("\nIf another agent is better for answering the question according to its description, call `transfer_to_agent` function to transfer the question to that agent. When transferring, do not generate any text other than the function call.\n")

    if agent.ParentAgent() != nil {
        sb.WriteString(fmt.Sprintf("\nYour parent agent is %s. If neither the other agents nor you are best for answering the question according to the descriptions, transfer to your parent agent. If you don't have parent agent, try answer by yourself.\n", agent.ParentAgent().GetName()))
    }
    return sb.String()
}

// Placeholder for instruction template processing
func processInstructionTemplate(template string, ctx *ReadonlyContext) (string, error) {
	// In Python, this uses string.Formatter with a custom class to access state.
	// In Go, this would require a more explicit template engine or string replacement.
	// This is a very simplified version.
	processed := template
	// Regex to find {key} or {key?}
	re := regexp.MustCompile(`{([^}]+)}`)
	state := ctx.GetState() // Assuming GetState returns map[string]interface{}

	processed = re.ReplaceAllStringFunc(processed, func(match string) string {
		keyWithOptionalMarker := strings.Trim(match, "{} ")
		key := strings.TrimSuffix(keyWithOptionalMarker, "?")
		isOptional := strings.HasSuffix(keyWithOptionalMarker, "?")

		// Handle artifact.filename format
		if strings.HasPrefix(key, "artifact.") {
			// artifactName := strings.TrimPrefix(key, "artifact.")
			// artifactContent, err := loadArtifact(ctx.InvocationContext, artifactName) // Placeholder
			// if err != nil {
			// 	if isOptional { return "" }
			// 	log.Printf("Error loading artifact %s: %v", artifactName, err)
			// 	return match // return original on error
			// }
			// return artifactContent
			return "[ARTIFACT_CONTENT_PLACEHOLDER]" // Simplified
		}

		val, ok := state[key]
		if ok {
			return fmt.Sprintf("%v", val)
		}
		if isOptional {
			return ""
		}
		log.Printf("Warning: Context variable not found and not optional: `%s`", key)
		return match // Return original placeholder if not found and not optional
	})
	return processed, nil
}

# /Users/gopher/Desktop/workspace/adk-go/README.md

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/base_llm_flow.go
package llmflows

import (
	"fmt"
	"log"
	"reflect"
	"runtime"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/sessions"
	// "github.com/KennethanCeyer/adk-go/utils"
)

// Placeholder types from assumed packages (if not already fully defined in processors)
type InvocationContext = processors.InvocationContext
type LlmRequest = models.LlmRequest
type LlmResponse = models.LlmResponse
type Event = events.Event
type EventActions = events.EventActions
type LlmAgent = agents.LlmAgent // This should be the primary agent interface
type Content = models.Content   // Assuming models package defines Content, Part etc.
type Part = models.Part
type FunctionCall = models.FunctionCall
type FunctionResponse = models.FunctionResponse
type State = sessions.State
type RunConfig = agents.RunConfig // Assuming RunConfig is in agents

// BaseLlmFlow is the base for LLM-based flows.
type BaseLlmFlow struct {
	RequestProcessors  []processors.BaseLlmRequestProcessor
	ResponseProcessors []processors.BaseLlmResponseProcessor
}

// NewBaseLlmFlow creates a new BaseLlmFlow.
func NewBaseLlmFlow(
	requestProcessors []processors.BaseLlmRequestProcessor,
	responseProcessors []processors.BaseLlmResponseProcessor) *BaseLlmFlow {
	return &BaseLlmFlow{
		RequestProcessors:  requestProcessors,
		ResponseProcessors: responseProcessors,
	}
}

// RunAsync executes the LLM flow.
func (f *BaseLlmFlow) RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error) {
	outputEventCh := make(chan *Event)

	go func() {
		defer close(outputEventCh)

		llmReq := &LlmRequest{} // Initialize with appropriate defaults if any
		if llmAgent.GetGenerateContentConfig() != nil { // Assuming LlmAgent has GetGenerateContentConfig
			llmReq.Config = llmAgent.GetGenerateContentConfig()
		} else {
			llmReq.Config = &models.GenerateContentConfig{} // Ensure config is not nil
		}


		// 1. Pre-LLM processing
		for _, processor := range f.RequestProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name() // Get processor name for logging
			log.Printf("Running request processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmReq)
			if err != nil {
				log.Printf("Error running request processor %s: %v", processorName, err)
				outputEventCh <- &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in request processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				}
				return
			}
			// Drain events from processor if any (though these processors typically modify llmReq)
			for event := range eventCh {
				log.Printf("Event from request processor %s: %v", processorName, event.ID)
				outputEventCh <- event
			}
		}

		// 2. Call LLM
		// In Python ADK, this is `llm_agent.predict_async(llm_req, invocation_context=invocation_ctx)`
		// which internally calls the model connection.
		// Here, we assume LlmAgent has a method to interact with the LLM.
		log.Printf("Sending request to LLM for agent: %s, model: %s", llmAgent.GetName(), llmReq.Model)
		// For debugging: log.Printf("LLM Request Contents: %+v", llmReq.Contents)
		// For debugging: log.Printf("LLM Request System Instruction: %+v", llmReq.Config.SystemInstruction)


		llmRespCh, err := llmAgent.PredictAsync(invocationCtx, llmReq) // Assuming LlmAgent has PredictAsync
		if err != nil {
			log.Printf("Error initiating LLM prediction for agent %s: %v", llmAgent.GetName(), err)
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error calling LLM: %v", err)}}},
				ErrorCode:    "llm_call_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// Wait for LLM response (event containing LlmResponse)
		modelResponseEvent, ok := <-llmRespCh
		if !ok {
			log.Printf("LLM response channel closed unexpectedly for agent %s", llmAgent.GetName())
			outputEventCh <- &Event{
				ID:           utils.NewEventID(),
				InvocationID: invocationCtx.InvocationID,
				Author:       llmAgent.GetName(),
				Branch:       invocationCtx.Branch,
				Content:      &Content{Parts: []Part{{Text: "LLM response channel closed unexpectedly"}}},
				ErrorCode:    "llm_response_error",
				Timestamp:    utils.GetTimestamp(),
			}
			return
		}

		// The event from PredictAsync should contain the LlmResponse.
		// We need a way to extract it. Let's assume Event has an LlmResponse field or a method.
		var llmResponse *LlmResponse
		if modelResponseEvent.LlmResponse != nil { // Assuming Event has a direct LlmResponse field
			llmResponse = modelResponseEvent.LlmResponse
		} else {
			// Fallback: if LlmResponse is embedded in Content.Parts as a special Part type,
			// or if the event itself *is* the LlmResponse structure (less likely for an Event).
			// This part needs clarification based on how PredictAsync structures its output event.
			log.Printf("Warning: LlmResponse not directly found in modelResponseEvent for agent %s. Event: %+v", llmAgent.GetName(), modelResponseEvent)
			// For now, let's assume if LlmResponse is nil, the event content itself is the raw model output.
            // This means the response processors might need to handle raw Content directly.
            // To make it work with current ResponseProcessor interface, we'll construct a dummy LlmResponse if Content exists.
            if modelResponseEvent.Content != nil && llmResponse == nil {
                llmResponse = &LlmResponse{Content: modelResponseEvent.Content}
                 // Also ensure the event itself is passed, as it contains actions, etc.
            } else if llmResponse == nil {
                 log.Printf("Error: No LlmResponse or Content in modelResponseEvent for agent %s.", llmAgent.GetName())
                 outputEventCh <- &Event{
                    ID:           utils.NewEventID(),
                    InvocationID: invocationCtx.InvocationID,
                    Author:       llmAgent.GetName(),
                    Branch:       invocationCtx.Branch,
                    Content:      &Content{Parts: []Part{{Text: "Invalid or empty LLM response event"}}},
                    ErrorCode:    "llm_response_invalid",
                    Timestamp:    utils.GetTimestamp(),
                }
                return
            }
		}


		// 3. Post-LLM processing
		// The Python ADK iterates response_processors and then sends the modelResponseEvent.
		// If processors yield further events, those are sent too.
		// The original modelResponseEvent is sent *after* all processors complete.

		var accumulatedProcessorEvents []*Event

		for _, processor := range f.ResponseProcessors {
			processorName := runtime.FuncForPC(reflect.ValueOf(processor).Pointer()).Name()
			log.Printf("Running response processor: %s for agent: %s", processorName, llmAgent.GetName())

			eventCh, err := processor.RunAsync(invocationCtx, llmResponse, modelResponseEvent) // Pass the original event for context (e.g., actions)
			if err != nil {
				log.Printf("Error running response processor %s: %v", processorName, err)
				// Decide if this error is fatal or if we should continue with other processors
				// For now, accumulate and proceed, then send error event.
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, &Event{
					ID:           utils.NewEventID(),
					InvocationID: invocationCtx.InvocationID,
					Author:       llmAgent.GetName(),
					Branch:       invocationCtx.Branch,
					Content:      &Content{Parts: []Part{{Text: fmt.Sprintf("Error in response processor %s: %v", processorName, err)}}},
					ErrorCode:    "processor_error",
					Timestamp:    utils.GetTimestamp(),
				})
				continue // Or return if fatal
			}
			for procEvent := range eventCh {
				log.Printf("Event from response processor %s: %v", processorName, procEvent.ID)
				accumulatedProcessorEvents = append(accumulatedProcessorEvents, procEvent)
			}
		}

		// Send the (potentially modified by processors) modelResponseEvent first
		outputEventCh <- modelResponseEvent

		// Then send any events generated by the response processors
		for _, procEvent := range accumulatedProcessorEvents {
			outputEventCh <- procEvent
		}

	}()

	return outputEventCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/contents.go
package processors

import (
	"fmt"
	"log"
	"sort"
	"strings"
	"time"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions" // For REQUEST_EUC_FUNCTION_CALL_NAME & removeClientFunctionCallID
)

// InvocationContext defines the context for a single invocation of an agent.
type InvocationContext struct {
	InvocationID    string
	Agent           LlmAgent // Assuming LlmAgent or a more general Agent interface
	Session         *Session // Assuming this contains Events and State
	Branch          string
	UserContent     *Content
	ArtifactService interface{} // Placeholder for actual ArtifactService type
	// ... other fields like SessionService, MemoryService, RunConfig, etc.
}

// LlmAgent represents an LLM-based agent.
// This is a simplified interface based on usage in the Python code.
type LlmAgent interface {
	GetName() string
	GetIncludeContents() string // "default", "none"
	// ... other methods like GetPlanner, RootAgent, CanonicalInstruction etc.
}

// Session represents a user session.
type Session struct {
	ID     string
	Events []*Event
	State  map[string]interface{}
	AppName string
	UserID  string
	// ... other session fields
}

// Event represents an event in the system.
type Event struct {
	ID            string
	InvocationID  string
	Author        string
	Content       *Content
	Actions       *EventActions
	Timestamp     float64
	Branch        string
	// ... other event fields like TurnComplete, Partial, ErrorCode, ErrorMessage, etc.
}

// GetFunctionCalls extracts function calls from the event's content parts.
func (e *Event) GetFunctionCalls() []FunctionCall {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var calls []FunctionCall
	for _, part := range e.Content.Parts {
		if part.FunctionCall != nil {
			calls = append(calls, *part.FunctionCall)
		}
	}
	return calls
}

// GetFunctionResponses extracts function responses from the event's content parts.
func (e *Event) GetFunctionResponses() []FunctionResponse {
	if e == nil || e.Content == nil || e.Content.Parts == nil {
		return nil
	}
	var responses []FunctionResponse
	for _, part := range e.Content.Parts {
		if part.FunctionResponse != nil {
			responses = append(responses, *part.FunctionResponse)
		}
	}
	return responses
}


// Content represents the content of a message.
type Content struct {
	Role  string
	Parts []Part
}

// Part represents a part of a message's content.
type Part struct {
	Text             string
	FunctionCall     *FunctionCall
	FunctionResponse *FunctionResponse
	// ... other part types like InlineData, FileData, etc.
}

// FunctionCall represents a function call requested by the LLM.
type FunctionCall struct {
	ID   string
	Name string
	Args map[string]interface{}
}

// FunctionResponse represents the result of a function call.
type FunctionResponse struct {
	ID       string
	Name     string
	Response map[string]interface{}
}

// EventActions represents actions associated with an event.
type EventActions struct {
	StateDelta             map[string]interface{}
	RequestedAuthConfigs map[string]interface{} // Placeholder for actual AuthConfig type
	// ... other actions like SkipSummarization, TransferToAgent, Escalate
}


// LlmRequest represents a request to the LLM.
type LlmRequest struct {
	Model    string
	Contents []Content
	Config   *GenerateContentConfig // Assuming GenerateContentConfig is defined elsewhere
	// ... other fields like LiveConnectConfig, ToolsDict
}

// GenerateContentConfig holds configuration for LLM content generation.
type GenerateContentConfig struct {
	// Define fields based on google.genai.types.GenerateContentConfig
	// For example:
	// Temperature *float32
	// TopP *float32
	// TopK *int32
	// CandidateCount *int32
	// MaxOutputTokens *int32
	// StopSequences []string
	// SafetySettings []*SafetySetting
	// Tools []*Tool
	// SystemInstruction *Content
	// ResponseSchema interface{}
	// ResponseMimeType string
	// ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig
}


// Placeholder, assuming it's defined in the functions package or similar
const REQUEST_EUC_FUNCTION_CALL_NAME = "adk_request_credential"

// ContentLlmRequestProcessor builds the contents for the LLM request.
type ContentLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *ContentLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		if llmAgent.GetIncludeContents() != "none" { // Assuming LlmAgent has GetIncludeContents()
			if invocationCtx.Session == nil {
				log.Println("Error: InvocationContext.Session is nil")
				return
			}
			llmReq.Contents = getContents(
				invocationCtx.Branch,
				invocationCtx.Session.Events, // Assuming Session has Events []*Event
				llmAgent.GetName(),
			)
		}
		// This processor does not yield events.
	}()
	return outCh, nil
}

func rearrangeEventsForAsyncFunctionResponsesInHistory(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	functionCallIDToResponseEventsIndex := make(map[string]int)
	for i, event := range events {
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			for _, fr := range responses {
				functionCallIDToResponseEventsIndex[fr.ID] = i
			}
		}
	}

	var resultEvents []*Event
	processedResponseIndices := make(map[int]bool) // To avoid adding same response event multiple times

	for i, event := range events {
		if _, ok := processedResponseIndices[i]; ok && len(event.GetFunctionResponses()) > 0 { //
			// This response event was already merged with its call, skip it.
			continue //
		}

		if calls := event.GetFunctionCalls(); len(calls) > 0 { //
			resultEvents = append(resultEvents, event) // Add the function call event itself

			var responseEventsToMerge []*Event
			var indicesToMarkProcessed []int

			for _, fc := range calls {
				if responseIdx, ok := functionCallIDToResponseEventsIndex[fc.ID]; ok { //
					isAlreadyProcessedByAnotherCall := false
					for _, resEvent := range resultEvents {
						if resEvent == events[responseIdx] {
							isAlreadyProcessedByAnotherCall = true
							break
						}
						// Also check if the response event's content (parts) were merged into another event.
						// For simplicity, we rely on direct event object comparison or index tracking.
					}
					if !isAlreadyProcessedByAnotherCall && !processedResponseIndices[responseIdx] { //
						responseEventsToMerge = append(responseEventsToMerge, events[responseIdx])
						indicesToMarkProcessed = append(indicesToMarkProcessed, responseIdx)
					}
				}
			}

			if len(responseEventsToMerge) > 0 {
				sort.SliceStable(responseEventsToMerge, func(i, j int) bool { //
					var idxI, idxJ int = -1, -1
					for k, e := range events {
						if e == responseEventsToMerge[i] {
							idxI = k
						}
						if e == responseEventsToMerge[j] {
							idxJ = k
						}
						if idxI != -1 && idxJ != -1 {
							break
						}
					}
					return idxI < idxJ
				})
				mergedRespEvent := mergeFunctionResponseEvents(responseEventsToMerge)
				if mergedRespEvent != nil {
					resultEvents = append(resultEvents, mergedRespEvent)
					for _, idx := range indicesToMarkProcessed { //
						processedResponseIndices[idx] = true
					}
				}
			}
		} else {
			if !(len(event.GetFunctionResponses()) > 0 && processedResponseIndices[i]) { //
				resultEvents = append(resultEvents, event)
			}
		}
	}
	return resultEvents
}

func rearrangeEventsForLatestFunctionResponse(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	lastEvent := events[len(events)-1]
	responsesInLastEvent := lastEvent.GetFunctionResponses()
	if len(responsesInLastEvent) == 0 { //
		return events
	}

	responseIDs := make(map[string]bool)
	for _, fr := range responsesInLastEvent {
		responseIDs[fr.ID] = true
	}

	if len(events) >= 2 { //
		eventBeforeLast := events[len(events)-2]
		callsInEventBeforeLast := eventBeforeLast.GetFunctionCalls()
		allCallsMatched := true
		if len(callsInEventBeforeLast) == len(responseIDs) && len(callsInEventBeforeLast) > 0 { //
			for _, fc := range callsInEventBeforeLast {
				if !responseIDs[fc.ID] {
					allCallsMatched = false
					break
				}
			}
			if allCallsMatched {
				return events
			}
		}
	}

	functionCallEventIdx := -1 //
	for i := len(events) - 2; i >= 0; i-- { //
		event := events[i]
		calls := event.GetFunctionCalls()
		if len(calls) > 0 {
			foundMatch := false
			for _, fc := range calls {
				if responseIDs[fc.ID] {
					foundMatch = true
					for _, otherFc := range calls {
						responseIDs[otherFc.ID] = true
					}
					break
				}
			}
			if foundMatch {
				functionCallEventIdx = i
				break
			}
		}
	}

	if functionCallEventIdx == -1 { //
		log.Printf("Warning: No matching function call event found for function response IDs: %v", getKeys(responseIDs))
		return events //
	}

	var functionResponseEventsToMerge []*Event
	for i := functionCallEventIdx + 1; i < len(events)-1; i++ { //
		event := events[i]
		if responses := event.GetFunctionResponses(); len(responses) > 0 { //
			isRelevantResponse := false
			for _, fr := range responses {
				if responseIDs[fr.ID] {
					isRelevantResponse = true
					break
				}
			}
			if isRelevantResponse {
				functionResponseEventsToMerge = append(functionResponseEventsToMerge, event)
			}
		}
	}
	functionResponseEventsToMerge = append(functionResponseEventsToMerge, lastEvent)

	resultEvents := make([]*Event, 0, functionCallEventIdx+2) //
	resultEvents = append(resultEvents, events[:functionCallEventIdx+1]...)
	if merged := mergeFunctionResponseEvents(functionResponseEventsToMerge); merged != nil { //
		resultEvents = append(resultEvents, merged)
	}

	return resultEvents
}

func getContents(currentBranch string, historyEvents []*Event, agentName string) []Content {
	var filteredEvents []*Event
	for _, event := range historyEvents {
		if event.Content == nil || len(event.Content.Parts) == 0 { //
			continue
		}
		// Go: we assume an empty text part is still a valid part unless explicitly filtered.
		// For now, let's assume empty text part implies no meaningful content for LLM.
		hasMeaningfulText := false //
		for _, p := range event.Content.Parts {
			if p.Text != "" {
				hasMeaningfulText = true
				break
			}
		}
		if !hasMeaningfulText && len(event.GetFunctionCalls()) == 0 && len(event.GetFunctionResponses()) == 0 { //
			continue
		}

		if !isEventOnBranch(currentBranch, event) {
			continue
		}
		if isAuthEvent(event) {
			continue
		}

		if isOtherAgentReply(agentName, event) {
			filteredEvents = append(filteredEvents, convertForeignEventToGo(event))
		} else {
			filteredEvents = append(filteredEvents, event)
		}
	}

	// Python calls _rearrange_events_for_latest_function_response first, then _rearrange_events_for_async_function_responses_in_history.
	resultEventsStage1 := rearrangeEventsForLatestFunctionResponse(filteredEvents)
	resultEventsStage2 := rearrangeEventsForAsyncFunctionResponsesInHistory(resultEventsStage1)

	var contents []Content
	for _, event := range resultEventsStage2 {
		if event.Content != nil {
			contentCopy := deepCopyContent(*event.Content)
			removeClientFunctionCallID(&contentCopy) // Placeholder
			contents = append(contents, contentCopy)
		}
	}
	return contents
}

func isEventOnBranch(currentBranch string, event *Event) bool {
	// Placeholder for logic to check if event is on the current branch
	// This depends on how Branch is structured and compared.
	if event == nil {
		return false
	}
	return event.Branch == "" || currentBranch == "" || strings.HasPrefix(event.Branch, currentBranch) || strings.HasPrefix(currentBranch, event.Branch)
}

func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return currentAgentName != "" && event.Author != currentAgentName && event.Author != "user"
}

func convertForeignEventToGo(event *Event) *Event {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return event
	}

	newContent := Content{Role: "user"} //
	newContent.Parts = append(newContent.Parts, Part{Text: "For context:"})

	for _, part := range event.Content.Parts {
		if part.Text != "" {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] said: %s", event.Author, part.Text)})
		} else if part.FunctionCall != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] called tool `%s` with parameters: %v", event.Author, part.FunctionCall.Name, part.FunctionCall.Args)})
		} else if part.FunctionResponse != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] `%s` tool returned result: %v", event.Author, part.FunctionResponse.Name, part.FunctionResponse.Response)})
		} else {
			// This part needs careful consideration for how to represent non-text/non-FC/non-FR parts.
			// For simplicity, appending as-is if possible, or a placeholder.
			// newContent.Parts = append(newContent.Parts, part) // This assumes Part is copyable
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] provided non-text data.", event.Author)}) //
		}
	}

	newEvent := *event // Shallow copy for metadata
	newEvent.Author = "user" //
	newEvent.Content = &newContent
	return &newEvent
}

func mergeFunctionResponseEvents(functionResponseEvents []*Event) *Event {
	if len(functionResponseEvents) == 0 {
		return nil
	}
	if len(functionResponseEvents) == 1 {
		return functionResponseEvents[0]
	}

	baseEvent := functionResponseEvents[0]
	// mergedParts := make([]Part, 0) // Not used directly in Go like Python

	// Map to store the latest response part for each function call ID
	latestResponsesForID := make(map[string]Part)
	var nonResponseParts []Part // To collect text parts or other non-FR parts in order

	for _, event := range functionResponseEvents {
		if event.Content == nil {
			continue
		}
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil {
				latestResponsesForID[part.FunctionResponse.ID] = part
			} else {
				nonResponseParts = append(nonResponseParts, part)
			}
		}
	}

	// The Python code implicitly relies on the order of parts in the *first* event
	// if it contained multiple function responses, and then updates them.
	// For Go, if the first event had multiple FCs, its FRs would be the base.
	// If subsequent events provide FRs for *different* FC IDs not in the first event, they are appended.
	// This simplified version will collect all unique FRs based on their latest appearance.
	tempParts := make([]Part, 0, len(latestResponsesForID)+len(nonResponseParts)) //
	tempParts = append(tempParts, nonResponseParts...)                            //
	// To maintain order similar to Python (where original order of FCs in the *first* event matters,
	// and then other unique FRs are appended/merged), we need a more sophisticated merge.
	// For now, this collects unique FRs; their order relative to nonResponseParts and each other might differ from Python's.
	// A better approach would be to iterate through the function calls of the *triggering* event (if available)
	// and append responses in that order, or sort collected FRs by some original call order.

	// Example: Iterate through first event's FRs if they exist as a base order
	if baseEvent.Content != nil {
		for _, part := range baseEvent.Content.Parts {
			if part.FunctionResponse != nil {
				if respPart, ok := latestResponsesForID[part.FunctionResponse.ID]; ok {
					tempParts = append(tempParts, respPart)
					delete(latestResponsesForID, part.FunctionResponse.ID) // Mark as added
				}
			}
		}
	}
	// Append any remaining unique FRs (those not in the first event's call order)
	for _, part := range latestResponsesForID { // Order from map is not guaranteed.
		tempParts = append(tempParts, part)
	}

	mergedEvent := &Event{ //
		ID:           newEventID(), // Assuming newEventID() is defined
		InvocationID: baseEvent.InvocationID,
		Author:       baseEvent.Author,
		Branch:       baseEvent.Branch,
		Content:      &Content{Role: "user", Parts: tempParts}, // Gemini expects FRs to be role "user"
		Actions:      &EventActions{},                          // Initialize to avoid nil pointer if baseEvent.Actions is nil
		Timestamp:    baseEvent.Timestamp,                      // Or use latest timestamp
	}

	// Merge actions from all events (simplified)
	for _, event := range functionResponseEvents {
		if event.Actions != nil && event.Actions.StateDelta != nil { //
			if mergedEvent.Actions.StateDelta == nil {
				mergedEvent.Actions.StateDelta = make(map[string]interface{})
			}
			for k, v := range event.Actions.StateDelta {
				mergedEvent.Actions.StateDelta[k] = v
			}
		}
		// ... merge other actions like RequestedAuthConfigs, ArtifactDelta etc.
	}

	return mergedEvent
}

// newEventID generates a new unique ID for an event.
// This is a placeholder; a robust UUID generation should be used.
func newEventID() string {
	// In a real implementation, use something like github.com/google/uuid
	return fmt.Sprintf("evt_%d", time.Now().UnixNano())
}


func isAuthEvent(event *Event) bool {
	if event.Content == nil || len(event.Content.Parts) == 0 { //
		return false
	}
	for _, part := range event.Content.Parts {
		if part.FunctionCall != nil && part.FunctionCall.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
		if part.FunctionResponse != nil && part.FunctionResponse.Name == REQUEST_EUC_FUNCTION_CALL_NAME { //
			return true
		}
	}
	return false
}

// removeClientFunctionCallID placeholder
func removeClientFunctionCallID(content *Content) {
	// In Python, this removes IDs starting with AF_FUNCTION_CALL_ID_PREFIX
	// For Go, this would iterate through content.Parts and nil out IDs if they match a pattern.
	// For example:
	// const afFunctionCallIDPrefix = "adk-"
	// if content == nil || content.Parts == nil {
	// 	return
	// }
	// for i := range content.Parts {
	// 	if content.Parts[i].FunctionCall != nil && strings.HasPrefix(content.Parts[i].FunctionCall.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionCall.ID = "" // Or based on how Gemini handles empty vs nil
	// 	}
	// 	if content.Parts[i].FunctionResponse != nil && strings.HasPrefix(content.Parts[i].FunctionResponse.ID, afFunctionCallIDPrefix) {
	// 		content.Parts[i].FunctionResponse.ID = ""
	// 	}
	// }
}

// deepCopyContent performs a deep copy of the Content struct.
func deepCopyContent(original Content) Content { //
	// A real deep copy is needed here. This is a placeholder.
	// For Go structs, manual copying or a library might be needed for true deep copies.
	copied := original //
	if original.Parts != nil {
		copied.Parts = make([]Part, len(original.Parts))
		for i, p := range original.Parts {
			// Deep copy each part. This is simplified.
			// If Part contains pointers or slices, those need deep copying too.
			copiedPart := p
			if p.FunctionCall != nil {
				fcCopy := *p.FunctionCall
				if p.FunctionCall.Args != nil {
					fcCopy.Args = deepCopyMap(p.FunctionCall.Args)
				}
				copiedPart.FunctionCall = &fcCopy
			}
			if p.FunctionResponse != nil {
				frCopy := *p.FunctionResponse
				if p.FunctionResponse.Response != nil {
					frCopy.Response = deepCopyMap(p.FunctionResponse.Response)
				}
				copiedPart.FunctionResponse = &frCopy
			}
			// Add deep copy for other Part fields if necessary
			copied.Parts[i] = copiedPart
		}
	}
	return copied
}

// deepCopyMap creates a deep copy of a map[string]interface{}.
// This is a simplified version; a robust deep copy library might be better for complex nested structures.
func deepCopyMap(originalMap map[string]interface{}) map[string]interface{} {
	if originalMap == nil {
		return nil
	}
	copyMap := make(map[string]interface{})
	for key, value := range originalMap {
		switch v := value.(type) {
		case map[string]interface{}:
			copyMap[key] = deepCopyMap(v)
		case []interface{}:
			copyMap[key] = deepCopySlice(v)
		default:
			copyMap[key] = v // Assumes primitive types are copyable by value
		}
	}
	return copyMap
}

// deepCopySlice creates a deep copy of a slice of interface{}.
func deepCopySlice(originalSlice []interface{}) []interface{} {
	if originalSlice == nil {
		return nil
	}
	copySlice := make([]interface{}, len(originalSlice))
	for i, value := range originalSlice {
		switch v := value.(type) {
		case map[string]interface{}:
			copySlice[i] = deepCopyMap(v)
		case []interface{}:
			copySlice[i] = deepCopySlice(v)
		default:
			copySlice[i] = v
		}
	}
	return copySlice
}


func getKeys(m map[string]bool) []string { //
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}

// Placeholder for LlmAgent.GetIncludeContents() method.
// This should be part of the LlmAgent interface/struct definition.
// func (a *LlmAgentStruct) GetIncludeContents() string { return a.IncludeContents }

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/instructions.go
package processors

import (
	"fmt"
	"log"
	"regexp"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/utils" // For instructions_utils
)

// ReadonlyContext provides a read-only view of the invocation context.
type ReadonlyContext struct { //
	InvocationContext *InvocationContext
	// Potentially other fields if ReadonlyContext offers more than just InvocationContext access
}


// InstructionsLlmRequestProcessor handles instructions and global instructions for LLM flow.
type InstructionsLlmRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *InstructionsLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in InstructionsLlmRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			log.Println("Error: Agent in InvocationContext is not of type LlmAgent or compatible interface")
			return
		}

		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx} //

		// Append global instructions if set.
		// In Python ADK, global_instruction is on the root agent.
		// Assuming LlmAgent interface has a RootAgent() method that returns the root agent (LlmAgent type).
		var rootLlmAgent LlmAgent
		if ra := llmAgent.RootAgent(); ra != nil {
			var rOk bool
			rootLlmAgent, rOk = ra.(LlmAgent)
			if !rOk {
				log.Println("Warning: Root agent is not an LlmAgent, cannot process global instructions.")
			}
		}


		if rootLlmAgent != nil { //
			// Assuming LlmAgent interface has CanonicalGlobalInstruction
			globalInstructionStr, bypassGlobal, err := rootLlmAgent.CanonicalGlobalInstruction(readonlyCtx) //
			if err != nil {
				log.Printf("Error getting canonical global instruction: %v", err) //
			}
			if globalInstructionStr != "" {
				var processedGlobalInstr string
				if !bypassGlobal {
					processedGlobalInstr, err = injectSessionState(globalInstructionStr, readonlyCtx) //
					if err != nil {
						log.Printf("Error injecting state into global instruction: %v", err) //
						processedGlobalInstr = globalInstructionStr                               // Use raw on error
					}
				} else {
					processedGlobalInstr = globalInstructionStr
				}
				// Assuming LlmRequest has AppendInstructions
				if llmReq.Config == nil {
					llmReq.Config = &GenerateContentConfig{}
				}
				if llmReq.Config.SystemInstruction == nil {
					llmReq.Config.SystemInstruction = &Content{}
				}
				// This needs a proper AppendInstructions method on LlmRequest or its Config
				currentSysInstruction := ""
				if len(llmReq.Config.SystemInstruction.Parts) > 0 {
					currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
				}
				if currentSysInstruction != "" {
					currentSysInstruction += "\n\n"
				}
				llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedGlobalInstr}}

			}
		}

		// Append agent instructions if set.
		// Assuming LlmAgent interface has CanonicalInstruction
		agentInstructionStr, bypassAgent, err := llmAgent.CanonicalInstruction(readonlyCtx) //
		if err != nil {
			log.Printf("Error getting canonical agent instruction: %v", err) //
		}
		if agentInstructionStr != "" {
			var processedAgentInstr string
			if !bypassAgent {
				processedAgentInstr, err = injectSessionState(agentInstructionStr, readonlyCtx) //
				if err != nil {
					log.Printf("Error injecting state into agent instruction: %v", err) //
					processedAgentInstr = agentInstructionStr                             // Use raw on error
				}
			} else {
				processedAgentInstr = agentInstructionStr
			}
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + processedAgentInstr}}
		}
		// No events are yielded by this specific processor.
	}()
	return outCh, nil
}

// injectSessionState populates values in the instruction template.
// This is a Go equivalent of instructions_utils.inject_session_state.
func injectSessionState(template string, readonlyCtx *ReadonlyContext) (string, error) {
	if readonlyCtx == nil || readonlyCtx.InvocationContext == nil || readonlyCtx.InvocationContext.Session == nil {
		return template, fmt.Errorf("readonlyCtx or its internal fields are nil")
	}
	invocationCtx := readonlyCtx.InvocationContext //

	// Go's regexp package doesn't support direct async replacement functions.
	// This will be synchronous for now.
	// Pattern to find {var_name} or {var_name?} or {artifact.file_name}
	// For simplicity, a basic version is used here. A more robust parser might be needed.
	re := regexp.MustCompile(`{([^}]+)}`) //

	var replacementErr error

	result := re.ReplaceAllStringFunc(template, func(match string) string { //
		if replacementErr != nil {
			return match
		}

		varNameWithMarker := strings.Trim(match, "{} ")
		isOptional := strings.HasSuffix(varNameWithMarker, "?")
		varName := strings.TrimSuffix(varNameWithMarker, "?")

		if strings.HasPrefix(varName, "artifact.") { //
			artifactName := strings.TrimPrefix(varName, "artifact.")
			if invocationCtx.ArtifactService == nil { //
				replacementErr = fmt.Errorf("artifact service is not initialized")
				return match
			}
			// Conceptual: ArtifactService would need a synchronous LoadArtifact or this needs to be async.
			// For this snippet, assuming a conceptual synchronous load or pre-loaded artifact map.
			// This part is commented out as ArtifactService and its methods are not fully defined here.
			/*
				artifactPart, err := invocationCtx.ArtifactService.LoadArtifactSync( //
					invocationCtx.Session.AppName, // Assuming AppName field
					invocationCtx.Session.UserID,  // Assuming UserID field
					invocationCtx.Session.ID,      // Assuming ID field
					artifactName,
				)
				if err != nil {
					if isOptional { return "" }
					replacementErr = fmt.Errorf("artifact '%s' not found: %w", artifactName, err)
					return match
				}
				if artifactPart != nil {
					return artifactPart.Text // Assuming text artifact for simplicity
				}
			*/
			return fmt.Sprintf("[ARTIFACT:%s]", artifactName) // Placeholder
		}

		if !isValidStateName(varName) { //
			return match
		}

		if val, ok := invocationCtx.Session.State[varName]; ok { //
			return fmt.Sprintf("%v", val)
		}
		if isOptional { //
			return ""
		}
		replacementErr = fmt.Errorf("context variable not found: `%s`", varName) //
		return match
	})

	if replacementErr != nil {
		return "", replacementErr
	}
	return result, nil
}

// isValidStateName (simplified) checks if a variable name could be a state key.
func isValidStateName(varName string) bool {
	parts := strings.Split(varName, ":")
	if len(parts) == 1 {
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(varName) //
	}
	if len(parts) == 2 {
		// In Python ADK, prefixes are "app:", "user:", "temp:"
		// Simplified check for now.
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[0]) &&
			regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[1]) //
	}
	return false
}

// LlmAgent interface needs to be fully defined, including these methods for the above to compile:
// RootAgent() BaseAgent // Assuming BaseAgent is a common interface for all agents
// CanonicalGlobalInstruction(ctx *ReadonlyContext) (string, bool, error)
// CanonicalInstruction(ctx *ReadonlyContext) (string, bool, error)

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/base_llm_processors.go
package processors

// BaseLlmRequestProcessor defines the interface for processing LLM requests.
type BaseLlmRequestProcessor interface {
	RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error)
}

// BaseLlmResponseProcessor defines the interface for processing LLM responses.
type BaseLlmResponseProcessor interface {
	RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error)
}

// BaseLlmProcessor is a conceptual base for processors if common utility methods were needed.
// In Go, composition or helper functions are often preferred over deep inheritance hierarchies.
// For now, interfaces suffice.

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/basic.go
package processors

import (
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, etc.
)

// BaseLlm defines the interface for a Large Language Model.
type BaseLlm interface {
	ModelName() string
	// Potentially other methods like GenerateContent, Connect etc.
}

// MockLLMForFlow is a placeholder for testing, representing a specific LLM implementation.
type MockLLMForFlow struct {
	ModelName string
}

func (m *MockLLMForFlow) ModelName() string {
	return m.ModelName
}
// Ensure MockLLMForFlow implements BaseLlm
var _ BaseLlm = &MockLLMForFlow{}


// RunConfig defines configuration for an agent run.
type RunConfig struct {
	// Define fields based on google.adk.agents.run_config.RunConfig
	// Example fields (conceptual):
	// ResponseModalities []string
	// SpeechConfig interface{}
	// OutputAudioTranscription bool
	// InputAudioTranscription bool
	// MaxLlmCalls int
	// StreamingMode string
}


// LlmAgent interface part related to BasicRequestProcessor
type LlmAgentForBasicProcessor interface {
	LlmAgent // Embed the general LlmAgent interface
	CanonicalModel() (BaseLlm, error)
	GetGenerateContentConfig() *GenerateContentConfig
	GetOutputSchema() interface{}
}


// BasicRequestProcessor handles basic information to build the LLM request.
type BasicRequestProcessor struct{} //

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *BasicRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) { //
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in BasicRequestProcessor")
			return
		}

		llmAgent, ok := invocationCtx.Agent.(LlmAgentForBasicProcessor) // Use the more specific interface
		if !ok {
			// If agent is not an LlmAgent, this processor might not apply,
			// or it might indicate an issue with the agent setup.
			// For now, we simply return without error.
			log.Println("Error: Agent in InvocationContext is not of type LlmAgentForBasicProcessor or compatible interface")
			return
		}

		canonicalModel, err := llmAgent.CanonicalModel() //
		if err != nil {
			log.Printf("Error getting canonical model for agent %s: %v", llmAgent.GetName(), err) //
			return
		}

		if canonicalModel != nil { //
			llmReq.Model = canonicalModel.ModelName() //
		} else {
			log.Printf("Warning: CanonicalModel is nil for agent %s, model name might not be set correctly in LlmRequest.", llmAgent.GetName()) //
		}


		if cfg := llmAgent.GetGenerateContentConfig(); cfg != nil { //
			cfgCopy := *cfg // Shallow copy
			llmReq.Config = &cfgCopy //
		} else {
			llmReq.Config = &GenerateContentConfig{} // Ensure config is not nil
		}

		if outputSchema := llmAgent.GetOutputSchema(); outputSchema != nil { //
			// Assuming LlmRequest has a SetOutputSchema method
			// llmReq.SetOutputSchema(outputSchema) // This depends on LlmRequest's definition
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			llmReq.Config.ResponseSchema = outputSchema
			llmReq.Config.ResponseMimeType = "application/json" // Typically for structured output

		}

		if invocationCtx.RunConfig != nil { //
			// Conceptual assignments - actual fields depend on RunConfig and LlmRequest.LiveConnectConfig definitions
			// if llmReq.LiveConnectConfig == nil { llmReq.LiveConnectConfig = &LiveConnectConfig{} }
			// llmReq.LiveConnectConfig.ResponseModalities = invocationCtx.RunConfig.ResponseModalities
			// llmReq.LiveConnectConfig.SpeechConfig = invocationCtx.RunConfig.SpeechConfig
			// llmReq.LiveConnectConfig.OutputAudioTranscription = invocationCtx.RunConfig.OutputAudioTranscription
			// llmReq.LiveConnectConfig.InputAudioTranscription = invocationCtx.RunConfig.InputAudioTranscription
			// These would be direct assignments if LiveConnectConfig fields match RunConfig fields.
		}

		// No events are yielded by this specific processor, it only modifies llmReq.
	}()
	return outCh, nil
}

// These GetGenerateContentConfig and GetOutputSchema methods should be part of the LlmAgent implementation.
// Example implementation for a hypothetical LlmAgentStruct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentGenerateContentConfig *GenerateContentConfig
    OutputSchema interface{}
}

func (a *LlmAgentStruct) GetGenerateContentConfig() *GenerateContentConfig { //
    return a.AgentGenerateContentConfig
}

func (a *LlmAgentStruct) GetOutputSchema() interface{} { //
    return a.OutputSchema
}
*/

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/identity.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
)

// IdentityLlmRequestProcessor gives the agent identity from the framework.
type IdentityLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *IdentityLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in IdentityLlmRequestProcessor")
			return
		}
		agent := invocationCtx.Agent // Assuming Agent interface has GetName() and GetDescription()

		var instructions []string
		instructions = append(instructions, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName())) //
		if desc := agent.GetDescription(); desc != "" { //
			instructions = append(instructions, fmt.Sprintf(" The description about you is \"%s\"", desc)) //
		}

		// Assuming LlmRequest has AppendInstructions method or similar mechanism
		if llmReq.Config == nil {
			llmReq.Config = &GenerateContentConfig{}
		}
		if llmReq.Config.SystemInstruction == nil {
			llmReq.Config.SystemInstruction = &Content{}
		}
		currentSysInstruction := ""
		if len(llmReq.Config.SystemInstruction.Parts) > 0 {
			currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
		}

		addedInstruction := strings.Join(instructions, "")
		if currentSysInstruction != "" {
			currentSysInstruction += "\n\n" // Ensure separation if there's existing instruction
		}
		llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + addedInstruction}}
		// This processor does not yield events, it modifies llmReq.
	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/nlplanning.go
package processors

import (
	"fmt"
	"log"
	"strings"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/planners" // For PlanReActPlanner, BuiltInPlanner
	// "github.com/google/generative-ai-go/genai" // For genai.Part, genai.ThinkingConfig
)

// Planner defines the interface for agent planners.
type Planner interface { // [cite: 2133]
	BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) // [cite: 2133]
	ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) // [cite: 2133]
	// ApplyThinkingConfig is specific to BuiltInPlanner in Python, might be part of a more specific interface in Go
	ApplyThinkingConfig(llmReq *LlmRequest)
}

// BuiltInPlanner is a placeholder for planners.BuiltInPlanner [cite: 2133]
type BuiltInPlanner struct {
	ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig [cite: 2133]
}

// ApplyThinkingConfig applies the thinking configuration to the LLM request.
func (bip *BuiltInPlanner) ApplyThinkingConfig(llmReq *LlmRequest) { // [cite: 2133]
	if bip.ThinkingConfig != nil && llmReq.Config != nil {
		// llmReq.Config.ThinkingConfig = bip.ThinkingConfig // This depends on GenerateContentConfig definition
		// For example, if GenerateContentConfig has a field `ThinkingConfig interface{}`
		// then the above line would work.
		log.Println("BuiltInPlanner.ApplyThinkingConfig: ThinkingConfig application is conceptual.")
	}
}
// BuildPlanningInstruction for BuiltInPlanner.
func (bip *BuiltInPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) { // [cite: 2133]
	return "", nil // Python version returns None, so empty string and no error for Go
}
// ProcessPlanningResponse for BuiltInPlanner.
func (bip *BuiltInPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) { // [cite: 2133, 2134]
	return responseParts, nil
}

// PlanReActPlanner is a placeholder for planners.PlanReActPlanner [cite: 2134]
type PlanReActPlanner struct{}

// Constants for PlanReActPlanner tags [cite: 2134]
const (
	PlanningTag    = "/*PLANNING*/"    // [cite: 2134]
	RePlanningTag  = "/*REPLANNING*/"  // [cite: 2134]
	ReasoningTag   = "/*REASONING*/"   // [cite: 2134]
	ActionTag      = "/*ACTION*/"      // [cite: 2134]
	FinalAnswerTag = "/*FINAL_ANSWER*/" // [cite: 2134]
)

// BuildPlanningInstruction for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) {
	// This directly translates the Python _build_nl_planner_instruction method [cite: 2134]
	highLevelPreamble := fmt.Sprintf(` When answering the question, try to leverage the available tools to gather the information instead of your memorized knowledge. [cite: 2134, 2135]
Follow this process when answering the question: (1) first come up with a plan in natural language text format; [cite: 2135, 2136]
(2) Then use tools to execute the plan and provide reasoning between tool code snippets to make a summary of current state and next step. [cite: 2136, 2137]
Tool code snippets and reasoning should be interleaved with each other. (3) In the end, return one final answer. [cite: 2137]
Follow this format when answering the question: (1) The planning part should be under %s. [cite: 2138]
(2) The tool code snippets should be under %s, and the reasoning parts should be under %s. [cite: 2139]
(3) The final answer part should be under %s. `, PlanningTag, ActionTag, ReasoningTag, FinalAnswerTag) // [cite: 2139, 2140]

	planningPreamble := fmt.Sprintf(` Below are the requirements for the planning: The plan is made to answer the user query if following the plan. The plan is coherent and covers all aspects of information from user query, and only involves the tools that are accessible by the agent. The plan contains the decomposed steps as a numbered list where each step should use one or multiple available tools. By reading the plan, you can intuitively know which tools to trigger or what actions to take. [cite: 2140, 2141]
If the initial plan cannot be successfully executed, you should learn from previous execution results and revise your plan. The revised plan should be be under %s. Then use tools to follow the new plan. `, RePlanningTag) // [cite: 2141]

	// The Python original has reasoning_preamble, final_answer_preamble, tool_code_without_python_libraries_preamble, user_input_preamble
	// These would be similarly defined and concatenated. For brevity, I'm showing the pattern.
	// Assuming those other preamble parts are defined similarly:
	reasoningPreamble := ` Below are the requirements for the reasoning: The reasoning makes a summary of the current trajectory based on the user query and tool outputs. Based on the tool outputs and plan, the reasoning also comes up with instructions to the next steps, making the trajectory closer to the final answer. `
	finalAnswerPreamble := ` Below are the requirements for the final answer: The final answer should be precise and follow query formatting requirements. Some queries may not be answerable with the available tools and information. In those cases, inform the user why you cannot process their query and ask for more information. `
	toolCodeWithoutPythonLibrariesPreamble := ` Below are the requirements for the tool code: **Custom Tools:** The available tools are described in the context and can be directly used. - Code must be valid self-contained Python snippets with no imports and no references to tools or Python libraries that are not in the context. - You cannot use any parameters or fields that are not explicitly defined in the APIs in the context. - The code snippets should be readable, efficient, and directly relevant to the user query and reasoning steps. - When using the tools, you should use the library name together with the function name, e.g., vertex_search.search(). - If Python libraries are not provided in the context, NEVER write your own code other than the function calls using the provided tools. `
	userInputPreamble := ` VERY IMPORTANT instruction that you MUST follow in addition to the above instructions: You should ask for clarification if you need more information to answer the question. You should prefer using the information available in the context instead of repeated tool use. `

	return strings.Join([]string{
		highLevelPreamble,
		planningPreamble,
		reasoningPreamble,
		finalAnswerPreamble,
		toolCodeWithoutPythonLibrariesPreamble,
		userInputPreamble,
	}, "\n\n"), nil
}

// markAsThought marks a response part as a thought.
// In Go, `Part` doesn't have a direct `Thought` field like in Python's genai types.
// This might be handled by a wrapper struct or by convention (e.g., specific role or metadata).
// For this conceptual translation, we'll assume a way to mark it, or it's implicit in the planner's logic.
func (prp *PlanReActPlanner) markAsThought(part *Part) {
	// Conceptual: If Part struct had a Thought field: part.Thought = true
	// Or, this could modify the Part's role or add metadata if the Go ADK defines such conventions.
	// For now, this is a no-op unless Part struct is extended.
	log.Printf("Marking part as thought (conceptual): %v", part.Text)
}


// ProcessPlanningResponse for PlanReActPlanner. [cite: 2134]
func (prp *PlanReActPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) {
	if responseParts == nil { // [cite: 2134]
		return nil, nil
	}

	var preservedParts []Part
	firstFCPartIndex := -1

	for i, part := range responseParts {
		if part.FunctionCall != nil { // [cite: 2134]
			if part.FunctionCall.Name == "" {
				continue
			}
			preservedParts = append(preservedParts, part)
			if firstFCPartIndex == -1 {
				firstFCPartIndex = i
			}
		} else {
			// Handle non-function call parts based on tags
			if part.Text != "" {
				// Split by FINAL_ANSWER_TAG first
				if strings.Contains(part.Text, FINAL_ANSWER_TAG) {
					splits := strings.SplitN(part.Text, FINAL_ANSWER_TAG, 2)
					reasoningText := strings.TrimSpace(splits[0])
					finalAnswerText := ""
					if len(splits) > 1 {
						finalAnswerText = strings.TrimSpace(splits[1])
					}

					if reasoningText != "" {
						reasoningPart := Part{Text: reasoningText}
						prp.markAsThought(&reasoningPart) // Conceptual
						preservedParts = append(preservedParts, reasoningPart)
					}
					if finalAnswerText != "" {
						preservedParts = append(preservedParts, Part{Text: finalAnswerText})
					}
				} else {
					// If no FINAL_ANSWER_TAG, check for other thought tags
					isThought := false
					for _, tag := range []string{PlanningTag, ReasoningTag, ActionTag, RePlanningTag} {
						if strings.HasPrefix(part.Text, tag) {
							isThought = true
							break
						}
					}
					currentPart := part
					if isThought {
						prp.markAsThought(&currentPart) // Conceptual
					}
					preservedParts = append(preservedParts, currentPart)
				}
			}
		}
	}
	return preservedParts, nil
}

// ApplyThinkingConfig for PlanReActPlanner (not directly applicable as it doesn't use genai.ThinkingConfig)
func (prp *PlanReActPlanner) ApplyThinkingConfig(llmReq *LlmRequest) {
	// PlanReActPlanner builds instructions directly, doesn't use genai.ThinkingConfig
}


// LlmAgent interface needs to be fully defined for the below to be valid outside this file.
// For example, within an LlmAgent struct:
/*
type LlmAgentStruct struct {
    // ... other fields
    AgentPlanner Planner
}

func (a *LlmAgentStruct) GetPlanner() Planner {
    return a.AgentPlanner
}
*/

// NlPlanningRequestProcessor handles NL planning for LLM flow.
type NlPlanningRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *NlPlanningRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil {
			log.Println("Error: InvocationContext or Agent is nil in NlPlanningRequestProcessor")
			return
		}

		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}
		// Python has: if isinstance(planner, BuiltInPlanner): planner.apply_thinking_config(llm_req)
		// In Go, this would typically be handled by type assertion or checking an interface method.
		if bp, ok := planner.(*BuiltInPlanner); ok { // [cite: 2133]
			bp.ApplyThinkingConfig(llmReq) // [cite: 2133]
		}


		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx}
		instruction, err := planner.BuildPlanningInstruction(readonlyCtx, llmReq) // [cite: 2133]
		if err != nil {
			log.Printf("Error building planning instruction: %v", err)
			// Optionally send an error event or handle error
			return
		}
		if instruction != "" {
			// Assuming LlmRequest has AppendInstructions
			if llmReq.Config == nil {
				llmReq.Config = &GenerateContentConfig{}
			}
			if llmReq.Config.SystemInstruction == nil {
				llmReq.Config.SystemInstruction = &Content{}
			}
			currentSysInstruction := ""
			if len(llmReq.Config.SystemInstruction.Parts) > 0 {
				currentSysInstruction = llmReq.Config.SystemInstruction.Parts[0].Text
			}
			if currentSysInstruction != "" {
				currentSysInstruction += "\n\n"
			}
			llmReq.Config.SystemInstruction.Parts = []Part{{Text: currentSysInstruction + instruction}}
		}
	}()
	return outCh, nil
}

// NlPlanningResponseProcessor processes LLM responses for NL planning.
type NlPlanningResponseProcessor struct{}

// RunAsync implements the BaseLlmResponseProcessor interface (conceptual).
func (p *NlPlanningResponseProcessor) RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse, modelResponseEvent *Event) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if invocationCtx == nil || invocationCtx.Agent == nil || llmResp == nil || llmResp.Content == nil {
			log.Println("Error: InvocationContext, Agent, or LlmResponse content is nil in NlPlanningResponseProcessor")
			return
		}
		planner := getPlanner(invocationCtx) // Assumes getPlanner is defined
		if planner == nil {
			return
		}

		callbackCtx := &CallbackContext{InvocationContext: invocationCtx, EventActions: modelResponseEvent.Actions} // [cite: 2133]
		if callbackCtx.EventActions == nil { // Ensure EventActions is not nil
		    callbackCtx.EventActions = &EventActions{}
		}


		processedParts, err := planner.ProcessPlanningResponse(callbackCtx, llmResp.Content.Parts) // [cite: 2134]
		if err != nil {
			log.Printf("Error processing planning response: %v", err)
			// Optionally send an error event or handle error
			return
		}

		if processedParts != nil {
			llmResp.Content.Parts = processedParts
		}

		// If callbackCtx.State has changed (meaning planner updated state via EventActions)
		// and an event needs to be yielded. In Python, this is handled by the framework.
		// Here, we might need to explicitly yield an event if the state delta implies one.
		// However, the processor's role is usually to modify the llmResp or yield new events based on llmResp processing.
		// If state changes in callbackCtx are meant to produce a new event, that logic would go here.
		// For now, this processor primarily modifies the llmResp. If an event with updated actions
		// is needed, it would be constructed and sent to outCh.
		// This part of the Python code doesn't explicitly yield an Event from this processor.

	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/auto_flow.go
package llmflows

import (
	"fmt"
	"log"
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
)

// AutoFlow dynamically selects and runs the appropriate flow for an agent.
// In Go, the dynamic creation of the LlmAgent itself as done in Python's AutoFlow
// (creating an LlmAgent on the fly with specific tools/planners based on config)
// is less straightforward due to static typing.
// Typically, the LlmAgent would already be constructed with its configuration.
// This AutoFlow equivalent will focus on selecting the flow and running it.
type AutoFlow struct {
	// Configuration for AutoFlow, if any, can go here.
	// For example, a map of agent types to flow constructors.
}

// NewAutoFlow creates a new AutoFlow.
func NewAutoFlow() *AutoFlow {
	return &AutoFlow{}
}

// RunAsync executes the appropriate flow for the given agent.
func (af *AutoFlow) RunAsync(invocationCtx *InvocationContext) (<-chan *Event, error) {
	if invocationCtx == nil || invocationCtx.Agent == nil {
		err := fmt.Errorf("AutoFlow: InvocationContext or Agent is nil")
		log.Println(err)
		// Return a channel that immediately closes or sends an error event
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	agent := invocationCtx.Agent // Agent from InvocationContext

	// The Python AutoFlow can dynamically create an LlmAgent if the provided agent
	// is just a configuration. In Go, we'd expect `agent` to already be a runnable LlmAgent.
	// If `agent` is a configuration struct, a factory would be needed here to build the LlmAgent.

	llmAgent, ok := agent.(LlmAgent) // Assuming the agent in context is already an LlmAgent
	if !ok {
		err := fmt.Errorf("AutoFlow: agent in InvocationContext is not an LlmAgent, but %T", agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
			// Optionally send an error event
		}()
		return errCh, err
	}

	// --- Flow Selection Logic ---
	// In Python, this checks agent.flow or agent.tools, agent.planner etc.
	// For Go, we'll assume a simple logic: if the agent is an LlmAgent, use SingleFlow.
	// More complex logic could be based on LlmAgent's properties or specific type.

	var selectedFlow interface { // Using interface{} to represent a generic Flow type
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	}

	// Example: If LlmAgent has a GetFlowType() method or similar configuration.
	// flowType := llmAgent.GetFlowType() // Hypothetical method
	// switch flowType {
	// case "single":
	//  selectedFlow = NewSingleFlow()
	// case "sequential":
	//  selectedFlow = NewSequentialFlow() // If SequentialFlow exists
	// default:
	//  selectedFlow = NewSingleFlow() // Default
	// }

	// For now, directly use SingleFlow if it's an LlmAgent
	if _, isLlmAgent := agent.(LlmAgent); isLlmAgent {
		log.Printf("AutoFlow: Using SingleFlow for LlmAgent: %s", llmAgent.GetName())
		selectedFlow = NewSingleFlow()
	} else {
		// Handle other agent types if necessary, or default
		err := fmt.Errorf("AutoFlow: Unhandled agent type %T for flow selection. Defaulting to SingleFlow if possible or erroring.", agent)
		log.Println(err)
        // Attempt to use SingleFlow if the base agent can be cast.
        // This part is tricky without knowing all agent types.
        // For robustnes, it's better to ensure llmAgent is correctly typed above.
		log.Println("AutoFlow: Defaulting to SingleFlow (or error if not LlmAgent).")
		selectedFlow = NewSingleFlow() // This will run if the initial cast to LlmAgent succeeded.
	}


	if selectedFlow == nil {
		err := fmt.Errorf("AutoFlow: Could not determine a flow for agent: %s (%T)", llmAgent.GetName(), agent)
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}

	// Type assert selectedFlow to the expected Flow interface/type to call RunAsync
	// This assumes all flows (SingleFlow, etc.) implement this method signature.
	concreteFlow, flowOk := selectedFlow.(interface {
		RunAsync(invocationCtx *InvocationContext, agent LlmAgent) (<-chan *Event, error)
	})
	if !flowOk {
		err := fmt.Errorf("AutoFlow: Selected flow does not implement the required RunAsync method signature.")
		log.Println(err)
		errCh := make(chan *Event)
		go func() {
			defer close(errCh)
		}()
		return errCh, err
	}


	log.Printf("AutoFlow: Running flow for agent: %s", llmAgent.GetName())
	return concreteFlow.RunAsync(invocationCtx, llmAgent)
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/single_flow.go
package llmflows

import (
	"github.com/KennethanCeyer/adk-go/flows/llmflows/processors"
)

// SingleFlow represents a simple LLM flow with a standard set of processors.
type SingleFlow struct {
	*BaseLlmFlow
}

// NewSingleFlow creates a new SingleFlow.
func NewSingleFlow() *SingleFlow {
	requestProcessors := []processors.BaseLlmRequestProcessor{
		&processors.BasicRequestProcessor{},
		&processors.IdentityLlmRequestProcessor{},
		&processors.ToolLlmRequestProcessor{}, // Assuming ToolLlmRequestProcessor is defined
		&processors.NlPlanningRequestProcessor{},
		&processors.InstructionsLlmRequestProcessor{},
		&processors.ContentLlmRequestProcessor{},
		&processors.CodeExecutionRequestProcessor{}, // Assuming CodeExecutionRequestProcessor is defined
	}

	responseProcessors := []processors.BaseLlmResponseProcessor{
		&processors.NlPlanningResponseProcessor{},
		&processors.FunctionCallResponseProcessor{}, // Assuming FunctionCallResponseProcessor is defined
		&processors.CodeExecutionResponseProcessor{},// Assuming CodeExecutionResponseProcessor is defined
		&processors.ToolUsageResponseProcessor{},    // Assuming ToolUsageResponseProcessor is defined
	}

	baseFlow := NewBaseLlmFlow(requestProcessors, responseProcessors)
	return &SingleFlow{BaseLlmFlow: baseFlow}
}

// Ensure SingleFlow implements an expected Flow interface if one exists
// type Flow interface {
//    RunAsync(invocationCtx *InvocationContext, llmAgent LlmAgent) (<-chan *Event, error)
// }
// var _ Flow = &SingleFlow{}

# /Users/gopher/Desktop/workspace/adk-go/golang_code.txt
# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/contents.go
package processors

import (
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/flows/llmflows/functions" // For REQUEST_EUC_FUNCTION_CALL_NAME & removeClientFunctionCallID
	"fmt"
	"log"
	"sort"
)

// --- Placeholder types (defined conceptually in previous snippets) ---
/*
type InvocationContext struct { ... }
type LlmRequest struct { ... }
type Event struct { ... }
type LlmAgent interface { ... } // Assumed to embed/implement agents.Agent
type Content struct { ... }
type Part struct { ... }
type FunctionCall struct { ... }
type FunctionResponse struct { ... }
*/

// const REQUEST_EUC_FUNCTION_CALL_NAME = "adk_request_credential" // From functions.py

// ContentLlmRequestProcessor builds the contents for the LLM request.
type ContentLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *ContentLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			return
		}

		if llmAgent.GetIncludeContents() != "none" { // Assuming LlmAgent has GetIncludeContents()
			llmReq.Contents = getContents(
				invocationCtx.Branch,
				invocationCtx.Session.Events, // Assuming Session has Events []*Event
				llmAgent.GetName(),
			)
		}
		// This processor does not yield events.
	}()
	return outCh, nil
}

// --- Helper functions (translations from Python's contents.py) ---

func rearrangeEventsForAsyncFunctionResponsesInHistory(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	functionCallIDToResponseEventsIndex := make(map[string]int)
	for i, event := range events {
		if responses := event.GetFunctionResponses(); len(responses) > 0 {
			for _, fr := range responses {
				functionCallIDToResponseEventsIndex[fr.ID] = i
			}
		}
	}

	var resultEvents []*Event
	processedResponseIndices := make(map[int]bool) // To avoid adding same response event multiple times

	for i, event := range events {
		if _, ok := processedResponseIndices[i]; ok && len(event.GetFunctionResponses()) > 0 {
			// This response event was already merged with its call, skip it.
			continue
		}
		
		if calls := event.GetFunctionCalls(); len(calls) > 0 {
			resultEvents = append(resultEvents, event) // Add the function call event itself

			var responseEventsToMerge []*Event
			var indicesToMarkProcessed []int

			for _, fc := range calls {
				if responseIdx, ok := functionCallIDToResponseEventsIndex[fc.ID]; ok {
					// Check if this response event hasn't been merged with another call event already
					isAlreadyProcessedByAnotherCall := false
					for _, resEvent := range resultEvents {
						if resEvent == events[responseIdx] {
							isAlreadyProcessedByAnotherCall = true
							break
						}
						// Also check if the response event's content (parts) were merged into another event.
						// This requires a more complex check if parts are mutable and shared.
						// For simplicity, we rely on direct event object comparison or index tracking.
					}
					if !isAlreadyProcessedByAnotherCall && !processedResponseIndices[responseIdx] {
						responseEventsToMerge = append(responseEventsToMerge, events[responseIdx])
						indicesToMarkProcessed = append(indicesToMarkProcessed, responseIdx)
					}
				}
			}
			
			if len(responseEventsToMerge) > 0 {
				// Sort responseEventsToMerge by original index to maintain order before merging
				sort.SliceStable(responseEventsToMerge, func(i, j int) bool {
					var idxI, idxJ int = -1, -1
					for k, e := range events {
						if e == responseEventsToMerge[i] { idxI = k }
						if e == responseEventsToMerge[j] { idxJ = k }
						if idxI != -1 && idxJ != -1 { break }
					}
					return idxI < idxJ
				})
				mergedRespEvent := mergeFunctionResponseEvents(responseEventsToMerge)
				if mergedRespEvent != nil {
					resultEvents = append(resultEvents, mergedRespEvent)
					for _, idx := range indicesToMarkProcessed {
						processedResponseIndices[idx] = true
					}
				}
			}
		} else {
			// Not a function call event, and not an already processed response event
			if ! (len(event.GetFunctionResponses()) > 0 && processedResponseIndices[i]) {
				resultEvents = append(resultEvents, event)
			}
		}
	}
	return resultEvents
}


func rearrangeEventsForLatestFunctionResponse(events []*Event) []*Event {
	if len(events) == 0 {
		return events
	}

	lastEvent := events[len(events)-1]
	responsesInLastEvent := lastEvent.GetFunctionResponses()
	if len(responsesInLastEvent) == 0 {
		return events // Latest event is not a function response event
	}

	responseIDs := make(map[string]bool)
	for _, fr := range responsesInLastEvent {
		responseIDs[fr.ID] = true
	}
	
	// Check if the event just before the last one is the matching function call
	if len(events) >= 2 {
		eventBeforeLast := events[len(events)-2]
		callsInEventBeforeLast := eventBeforeLast.GetFunctionCalls()
		allCallsMatched := true
		if len(callsInEventBeforeLast) == len(responseIDs) && len(callsInEventBeforeLast) > 0 {
			for _, fc := range callsInEventBeforeLast {
				if !responseIDs[fc.ID] {
					allCallsMatched = false
					break
				}
			}
			if allCallsMatched {
				return events // Already correctly paired
			}
		}
	}


	functionCallEventIdx := -1
	// Look for corresponding function call event by iterating backwards
	for i := len(events) - 2; i >= 0; i-- {
		event := events[i]
		calls := event.GetFunctionCalls()
		if len(calls) > 0 {
			foundMatch := false
			for _, fc := range calls {
				if responseIDs[fc.ID] {
					foundMatch = true
					// Collect all function call IDs from this event, as they were part of a single LLM turn
					for _, otherFc := range calls {
						responseIDs[otherFc.ID] = true 
					}
					break 
				}
			}
			if foundMatch {
				functionCallEventIdx = i
				break
			}
		}
	}
	
	if functionCallEventIdx == -1 {
		log.Printf("Warning: No matching function call event found for function response IDs: %v", getKeys(responseIDs))
		return events // Return original if no clear call found
	}

	// Collect all function response events between the call and the latest response event
	var functionResponseEventsToMerge []*Event
	for i := functionCallEventIdx + 1; i < len(events)-1; i++ {
		event := events[i]
		if responses := event.GetFunctionResponses(); len(responses) > 0 {
			isRelevantResponse := false
			for _, fr := range responses {
				if responseIDs[fr.ID] {
					isRelevantResponse = true
					break
				}
			}
			if isRelevantResponse {
				functionResponseEventsToMerge = append(functionResponseEventsToMerge, event)
			}
		}
	}
	functionResponseEventsToMerge = append(functionResponseEventsToMerge, lastEvent) // Add the very last event

	resultEvents := make([]*Event, 0, functionCallEventIdx+2)
	resultEvents = append(resultEvents, events[:functionCallEventIdx+1]...)
	if merged := mergeFunctionResponseEvents(functionResponseEventsToMerge); merged != nil {
		resultEvents = append(resultEvents, merged)
	}

	return resultEvents
}

func getContents(currentBranch string, historyEvents []*Event, agentName string) []Content {
	var filteredEvents []*Event
	for _, event := range historyEvents {
		if event.Content == nil || len(event.Content.Parts) == 0 {
			continue
		}
		// Python ADK has: or event.Content.Parts[0].Text == ""
		// Go: we assume an empty text part is still a valid part unless explicitly filtered.
		// For now, let's assume empty text part implies no meaningful content for LLM.
		hasMeaningfulText := false
		for _, p := range event.Content.Parts {
			if p.Text != "" { // Consider only non-empty text as meaningful for this filter
				hasMeaningfulText = true
				break
			}
		}
		if !hasMeaningfulText && len(event.GetFunctionCalls()) == 0 && len(event.GetFunctionResponses()) == 0 {
			// If no text, no FC, no FR, skip
			continue
		}


		if !isEventOnBranch(currentBranch, event) {
			continue
		}
		if isAuthEvent(event) {
			continue
		}

		if isOtherAgentReply(agentName, event) {
			filteredEvents = append(filteredEvents, convertForeignEventToGo(event))
		} else {
			filteredEvents = append(filteredEvents, event)
		}
	}
	
	// Event rearrangement logic
	// Note: The order of these rearrangements might matter and should match Python ADK's intent.
	// Python calls _rearrange_events_for_latest_function_response first, then _rearrange_events_for_async_function_responses_in_history.
	resultEventsStage1 := rearrangeEventsForLatestFunctionResponse(filteredEvents)
	resultEventsStage2 := rearrangeEventsForAsyncFunctionResponsesInHistory(resultEventsStage1)


	var contents []Content
	for _, event := range resultEventsStage2 {
		if event.Content != nil { // Ensure content exists
			contentCopy := deepCopyContent(*event.Content) // Deep copy
			removeClientFunctionCallID(&contentCopy)   // Placeholder
			contents = append(contents, contentCopy)
		}
	}
	return contents
}

func isOtherAgentReply(currentAgentName string, event *Event) bool {
	return currentAgentName != "" && event.Author != currentAgentName && event.Author != "user"
}

func convertForeignEventToGo(event *Event) *Event {
	if event.Content == nil || len(event.Content.Parts) == 0 {
		return event // Should not happen if filtered before
	}

	newContent := Content{Role: "user"} // Foreign agent replies become "user" context for current agent
	newContent.Parts = append(newContent.Parts, Part{Text: "For context:"})

	for _, part := range event.Content.Parts {
		if part.Text != "" {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] said: %s", event.Author, part.Text)})
		} else if part.FunctionCall != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] called tool `%s` with parameters: %v", event.Author, part.FunctionCall.Name, part.FunctionCall.Args)})
		} else if part.FunctionResponse != nil {
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] `%s` tool returned result: %v", event.Author, part.FunctionResponse.Name, part.FunctionResponse.Response)})
		} else {
			// Fallback for other part types (e.g., inline_data)
			// This part needs careful consideration for how to represent non-text/non-FC/non-FR parts.
			// For simplicity, appending as-is if possible, or a placeholder.
			// newContent.Parts = append(newContent.Parts, part) // This assumes Part is copyable
			newContent.Parts = append(newContent.Parts, Part{Text: fmt.Sprintf("[%s] provided non-text data.", event.Author)})
		}
	}
	
	// Create a new event, don't modify the original
	newEvent := *event // Shallow copy for metadata
	newEvent.Author = "user" // Now treated as user input for the current agent
	newEvent.Content = &newContent
	return &newEvent
}

func mergeFunctionResponseEvents(functionResponseEvents []*Event) *Event {
	if len(functionResponseEvents) == 0 {
		return nil
	}
	if len(functionResponseEvents) == 1 {
		return functionResponseEvents[0]
	}

	// Use the first event as the base for metadata
	baseEvent := functionResponseEvents[0]
	mergedParts := make([]Part, 0)
	
	// In Python, this logic updates parts in place in merged_event.content.parts.
	// For Go, we collect parts and then create the final content.
	// We need to ensure that if multiple events update the *same* function call response (by ID),
	// the latest one wins.
	
	// Map to store the latest response part for each function call ID
	latestResponsesForID := make(map[string]Part)
	var nonResponseParts []Part // To collect text parts or other non-FR parts in order

	for _, event := range functionResponseEvents {
		if event.Content == nil { continue }
		for _, part := range event.Content.Parts {
			if part.FunctionResponse != nil {
				latestResponsesForID[part.FunctionResponse.ID] = part
			} else {
				nonResponseParts = append(nonResponseParts, part)
			}
		}
	}

	// Add the latest function responses first (order might matter based on original FC order)
	// To preserve order of calls if multiple unique calls were responded to across events:
	// This part is tricky. The Python code implicitly relies on the order of parts in the *first* event
	// if it contained multiple function responses, and then updates them.
	// For Go, if the first event had multiple FCs, its FRs would be the base.
	// If subsequent events provide FRs for *different* FC IDs not in the first event, they are appended.
	// This simplified version will collect all unique FRs based on their latest appearance.

	tempParts := make([]Part, 0, len(latestResponsesForID) + len(nonResponseParts))
	// Add non-FR parts first, then FR parts. Or interleave based on original event order if critical.
	// For now, simple append:
	tempParts = append(tempParts, nonResponseParts...)
	for _, part := range latestResponsesForID { // Order from map is not guaranteed.
		tempParts = append(tempParts, part)
	}


	mergedEvent := &Event{
		ID:           newEventID(), // New ID for the merged event
		InvocationID: baseEvent.InvocationID,
		Author:       baseEvent.Author,
		Branch:       baseEvent.Branch,
		Content:      &Content{Role: "user", Parts: tempParts}, // Gemini expects FRs to be role "user"
		Actions:      EventActions{},                            // Merge actions if necessary
		Timestamp:    baseEvent.Timestamp, // Or use latest timestamp
	}
	// Merge actions from all events (simplified)
	for _, event := range functionResponseEvents {
		if event.Actions.StateDelta != nil {
			if mergedEvent.Actions.StateDelta == nil { mergedEvent.Actions.StateDelta = make(map[string]interface{})}
			for k, v := range event.Actions.StateDelta { mergedEvent.Actions.StateDelta[k] = v }
		}
		// ... merge other actions like RequestedAuthConfigs, ArtifactDelta etc.
	}

	return mergedEvent
}


func isAuthEvent(event *Event) bool {
	if event.Content == nil || len(event.Content.Parts) == 0 {
		return false
	}
	for _, part := range event.Content.Parts {
		if part.FunctionCall != nil && part.FunctionCall.Name == REQUEST_EUC_FUNCTION_CALL_NAME {
			return true
		}
		if part.FunctionResponse != nil && part.FunctionResponse.Name == REQUEST_EUC_FUNCTION_CALL_NAME {
			return true
		}
	}
	return false
}

// removeClientFunctionCallID placeholder
func removeClientFunctionCallID(content *Content) {
	// In Python, this removes IDs starting with AF_FUNCTION_CALL_ID_PREFIX
	// For Go, this would iterate through content.Parts and nil out IDs if they match a pattern.
}

// deepCopyContent placeholder
func deepCopyContent(original Content) Content {
	// A real deep copy is needed here. This is a placeholder.
	// For Pydantic models, .model_copy(deep=True) is used.
	// For Go structs, manual copying or a library might be needed for true deep copies.
	copied := original
	if original.Parts != nil {
		copied.Parts = make([]Part, len(original.Parts))
		copy(copied.Parts, original.Parts) // This is a shallow copy of parts themselves if Part is a struct with pointers
		// To deep copy parts, iterate and copy each field.
	}
	return copied
}

func getKeys(m map[string]bool) []string {
    keys := make([]string, 0, len(m))
    for k := range m {
        keys = append(keys, k)
    }
    return keys
}

// Placeholder for LlmAgent.GetIncludeContents()
func (a *LlmAgent) GetIncludeContents() string { return a.IncludeContents }

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/instructions.go
package processors

import (
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/utils" // For instructions_utils
	"fmt"
	"log"
	"regexp"
	"strings"
)

// --- Placeholder types (defined conceptually in previous snippets) ---
/*
type InvocationContext struct { ... }
type LlmRequest struct { ... }
type Event struct { ... }
type LlmAgent interface { ... }
type ReadonlyContext struct { ... }
type Session struct { ... } // Used by ReadonlyContext indirectly
*/

// InstructionsLlmRequestProcessor handles instructions and global instructions for LLM flow.
type InstructionsLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *InstructionsLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			return
		}

		readonlyCtx := &ReadonlyContext{InvocationContext: invocationCtx} // Create ReadonlyContext

		// Append global instructions if set.
		// In Python ADK, global_instruction is on the root agent.
		rootAgent := llmAgent.RootAgent()
		if rootLlmAgent, rootOk := rootAgent.(LlmAgent); rootOk {
			globalInstructionStr, bypassGlobal, err := rootLlmAgent.CanonicalGlobalInstruction(readonlyCtx)
			if err != nil {
				log.Printf("Error getting canonical global instruction: %v", err)
				// Decide on error handling: continue or stop
			}
			if globalInstructionStr != "" {
				var processedGlobalInstr string
				if !bypassGlobal {
					processedGlobalInstr, err = injectSessionState(globalInstructionStr, readonlyCtx)
					if err != nil {
						log.Printf("Error injecting state into global instruction: %v", err)
						processedGlobalInstr = globalInstructionStr // Use raw on error
					}
				} else {
					processedGlobalInstr = globalInstructionStr
				}
				llmReq.AppendInstructions([]string{processedGlobalInstr})
			}
		}

		// Append agent instructions if set.
		agentInstructionStr, bypassAgent, err := llmAgent.CanonicalInstruction(readonlyCtx)
		if err != nil {
			log.Printf("Error getting canonical agent instruction: %v", err)
			// Decide on error handling
		}
		if agentInstructionStr != "" {
			var processedAgentInstr string
			if !bypassAgent {
				processedAgentInstr, err = injectSessionState(agentInstructionStr, readonlyCtx)
				if err != nil {
					log.Printf("Error injecting state into agent instruction: %v", err)
					processedAgentInstr = agentInstructionStr // Use raw on error
				}
			} else {
				processedAgentInstr = agentInstructionStr
			}
			llmReq.AppendInstructions([]string{processedAgentInstr})
		}

		// No events are yielded by this specific processor.
	}()
	return outCh, nil
}

// injectSessionState populates values in the instruction template.
// This is a Go equivalent of instructions_utils.inject_session_state.
func injectSessionState(template string, readonlyCtx *ReadonlyContext) (string, error) {
	invocationCtx := readonlyCtx.InvocationContext // Assuming ReadonlyContext has InvocationContext

	// Simplified version of Python's regex and replacement logic.
	// Go's regexp package doesn't support direct async replacement functions.
	// This will be synchronous for now.
	// Pattern to find {var_name} or {var_name?} or {artifact.file_name}
	// This regex is more complex to handle the nested structures correctly.
	// For simplicity, a basic version is used here. A more robust parser might be needed.
	re := regexp.MustCompile(`{([^}]+)}`) // Finds content within {}

	var replacementErr error

	result := re.ReplaceAllStringFunc(template, func(match string) string {
		if replacementErr != nil { // If an error occurred in a previous replacement, skip further processing
			return match
		}

		varNameWithMarker := strings.Trim(match, "{} ")
		isOptional := strings.HasSuffix(varNameWithMarker, "?")
		varName := strings.TrimSuffix(varNameWithMarker, "?")

		if strings.HasPrefix(varName, "artifact.") {
			artifactName := strings.TrimPrefix(varName, "artifact.")
			if invocationCtx.ArtifactService == nil {
				replacementErr = fmt.Errorf("artifact service is not initialized")
				return match
			}
			// Conceptual: ArtifactService would need a synchronous LoadArtifact or this needs to be async.
			// For this snippet, assuming a conceptual synchronous load or pre-loaded artifact map.
			// artifactPart, err := invocationCtx.ArtifactService.LoadArtifactSync(
			// 	invocationCtx.Session.AppName, // Assuming AppName field
			// 	invocationCtx.Session.UserID,  // Assuming UserID field
			// 	invocationCtx.Session.ID,      // Assuming ID field
			// 	artifactName,
			// )
			// if err != nil {
			// 	if isOptional { return "" }
			// 	replacementErr = fmt.Errorf("artifact '%s' not found: %w", artifactName, err)
			// 	return match
			// }
			// if artifactPart != nil {
			// 	return artifactPart.Text // Assuming text artifact for simplicity
			// }
			return fmt.Sprintf("[ARTIFACT:%s]", artifactName) // Placeholder
		}

		// Check for valid state name structure (e.g., simple identifier or prefix:identifier)
		if !isValidStateName(varName) {
			return match // Return original if not a valid-looking state variable placeholder
		}

		if val, ok := invocationCtx.Session.State[varName]; ok {
			return fmt.Sprintf("%v", val)
		}
		if isOptional {
			return ""
		}
		replacementErr = fmt.Errorf("context variable not found: `%s`", varName)
		return match
	})

	if replacementErr != nil {
		return "", replacementErr
	}
	return result, nil
}

// isValidStateName (simplified) checks if a variable name could be a state key.
func isValidStateName(varName string) bool {
	parts := strings.Split(varName, ":")
	if len(parts) == 1 {
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(varName)
	}
	if len(parts) == 2 {
		// In Python ADK, prefixes are "app:", "user:", "temp:"
		// Simplified check for now.
		return regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[0]) &&
		       regexp.MustCompile(`^[a-zA-Z_][a-zA-Z0-9_]*$`).MatchString(parts[1])
	}
	return false
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/basic.go
package processors // Assuming a 'processors' sub-package for these

import "log"

// "github.com/KennethanCeyer/adk-go/agents"
// "github.com/KennethanCeyer/adk-go/events"
// "github.com/KennethanCeyer/adk-go/models"
// "github.com/google/generative-ai-go/genai" // For genai.GenerateContentConfig, etc.

// --- Placeholder types (already defined in llm_agent.go or base_llm_flow.go conceptually) ---
/*
type InvocationContext struct { ... }
type LlmRequest struct { ... }
type Event struct { ... }
type LlmAgent interface { ... } // From agents package
type GenerateContentConfig struct { ... } // From genai types
type BaseLlm interface { ... } // From models package
type RunConfig struct { ... } // From agents package
*/

// BasicRequestProcessor handles basic information to build the LLM request.
type BasicRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *BasicRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh) // Ensure channel is closed when goroutine finishes

		llmAgent, ok := invocationCtx.Agent.(LlmAgent)
		if !ok {
			// If agent is not an LlmAgent, this processor might not apply,
			// or it might indicate an issue with the agent setup.
			// For now, we simply return without error.
			return
		}

		canonicalModel, err := llmAgent.CanonicalModel()
		if err != nil {
			// Handle error, e.g., log it or send an error event
			log.Printf("Error getting canonical model for agent %s: %v", llmAgent.GetName(), err)
			// outCh <- &Event{Error: err.Error()} // Conceptual error event
			return
		}
		// In Python, BaseLlm has a 'model' string attribute.
		// Assuming Go's BaseLlm interface has a ModelName() method or similar.
		// For the placeholder BaseLlm, let's assume it's a field.
		if bm, bmOk := canonicalModel.(*MockLLMForFlow); bmOk { // Using the mock from base_llm_flow.go
			llmReq.Model = bm.ModelName
		} else {
			// llmReq.Model = canonicalModel.ModelName() // If BaseLlm was an interface with ModelName()
			log.Printf("Warning: CanonicalModel is not of expected mock type, model name might not be set correctly in LlmRequest.")
		}


		if llmAgent.GetGenerateContentConfig() != nil { // Assuming LlmAgent has this getter
			// Deep copy needed if GenerateContentConfig is mutable and shared.
			// For simplicity, doing a shallow copy.
			cfgCopy := *llmAgent.GetGenerateContentConfig()
			llmReq.Config = &cfgCopy
		} else {
			llmReq.Config = &GenerateContentConfig{} // Ensure config is not nil
		}

		if outputSchema := llmAgent.GetOutputSchema(); outputSchema != nil { // Assuming LlmAgent has this getter
			llmReq.SetOutputSchema(outputSchema)
		}
		
		// Populate live_connect_config from run_config
		if invocationCtx.RunConfig != nil {
			// llmReq.LiveConnectConfig.ResponseModalities = invocationCtx.RunConfig.ResponseModalities
			// llmReq.LiveConnectConfig.SpeechConfig = invocationCtx.RunConfig.SpeechConfig
			// llmReq.LiveConnectConfig.OutputAudioTranscription = invocationCtx.RunConfig.OutputAudioTranscription
			// llmReq.LiveConnectConfig.InputAudioTranscription = invocationCtx.RunConfig.InputAudioTranscription
			// These would be direct assignments if LiveConnectConfig fields match RunConfig fields.
			// For now, this is conceptual.
		}


		// No events are yielded by this specific processor, it only modifies llmReq.
	}()
	return outCh, nil
}

func (a *LlmAgent) GetGenerateContentConfig() *GenerateContentConfig {
	return a.AgentGenerateContentConfig
}

func (a *LlmAgent) GetOutputSchema() interface{} {
	return a.OutputSchema
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/identity.go
package processors

import (
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	"fmt"
)

// --- Placeholder types (defined conceptually in previous snippets) ---
/*
type InvocationContext struct { ... }
type LlmRequest struct { ... }
type Event struct { ... }
*/

// IdentityLlmRequestProcessor gives the agent identity from the framework.
type IdentityLlmRequestProcessor struct{}

// RunAsync implements the BaseLlmRequestProcessor interface.
func (p *IdentityLlmRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh) // Ensure channel is closed

		agent := invocationCtx.Agent // Assuming Agent interface has GetName() and GetDescription()

		var instructions []string
		instructions = append(instructions, fmt.Sprintf("You are an agent. Your internal name is \"%s\".", agent.GetName()))
		if agent.GetDescription() != "" {
			instructions = append(instructions, fmt.Sprintf(" The description about you is \"%s\"", agent.GetDescription()))
		}
		llmReq.AppendInstructions(instructions) // Assuming LlmRequest has AppendInstructions

		// This processor does not yield events, it modifies llmReq.
	}()
	return outCh, nil
}

# /Users/gopher/Desktop/workspace/adk-go/flows/llmflows/processors/nlplanning.go
package processors

import (
	// "github.com/KennethanCeyer/adk-go/agents"
	// "github.com/KennethanCeyer/adk-go/events"
	// "github.com/KennethanCeyer/adk-go/models"
	// "github.com/KennethanCeyer/adk-go/planners" // For PlanReActPlanner, BuiltInPlanner
	// "github.com/google/generative-ai-go/genai" // For genai.Part, genai.ThinkingConfig
	"fmt"
	"log"
	"strings"
)

// --- Placeholder types (defined conceptually in previous snippets) ---
/*
type InvocationContext struct { ... }
type LlmRequest struct { ... }
type LlmResponse struct { ... }
type Event struct { ... }
type Part struct { ... FunctionCall, Text, Thought fields } // Placeholder for genai.Part
type Content struct { Parts []Part } // Placeholder for genai.Content
type Agent interface { ... }
type LlmAgent interface { // Assumed to embed/implement agents.Agent
	Agent
	GetPlanner() Planner // Assumed getter
}
type Planner interface { // Placeholder for planners.BasePlanner
	BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error)
	ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error)
}
type ReadonlyContext struct { ... }
type CallbackContext struct { ... }
type BuiltInPlanner struct { // Placeholder for planners.BuiltInPlanner
    ThinkingConfig interface{} // Placeholder for genai.ThinkingConfig
}
func (bip *BuiltInPlanner) ApplyThinkingConfig(llmReq *LlmRequest) {
	if bip.ThinkingConfig != nil && llmReq.Config != nil {
		llmReq.Config.ThinkingConfig = bip.ThinkingConfig
	}
}
func (bip *BuiltInPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) { return "", nil }
func (bip *BuiltInPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) { return responseParts, nil }


type PlanReActPlanner struct { // Placeholder for planners.PlanReActPlanner
	// No specific fields needed for this conceptual translation of its methods
}
// Constants for PlanReActPlanner tags
const (
	PlanningTag     = "/*PLANNING*/"
	RePlanningTag   = "/*REPLANNING*/"
	ReasoningTag    = "/*REASONING*/"
	ActionTag       = "/*ACTION*/"
	FinalAnswerTag  = "/*FINAL_ANSWER*/"
)
func (prp *PlanReActPlanner) BuildPlanningInstruction(readonlyCtx *ReadonlyContext, llmReq *LlmRequest) (string, error) {
	// This directly translates the Python _build_nl_planner_instruction method
	highLevelPreamble := fmt.Sprintf(`
When answering the question, try to leverage the available tools to gather the information instead of your memorized knowledge.

Follow this process when answering the question: (1) first come up with a plan in natural language text format; (2) Then use tools to execute the plan and provide reasoning between tool code snippets to make a summary of current state and next step. Tool code snippets and reasoning should be interleaved with each other. (3) In the end, return one final answer.

Follow this format when answering the question: (1) The planning part should be under %s. (2) The tool code snippets should be under %s, and the reasoning parts should be under %s. (3) The final answer part should be under %s.
`, PlanningTag, ActionTag, ReasoningTag, FinalAnswerTag)

	planningPreamble := fmt.Sprintf(`
Below are the requirements for the planning:
The plan is made to answer the user query if following the plan. The plan is coherent and covers all aspects of information from user query, and only involves the tools that are accessible by the agent. The plan contains the decomposed steps as a numbered list where each step should use one or multiple available tools. By reading the plan, you can intuitively know which tools to trigger or what actions to take.
If the initial plan cannot be successfully executed, you should learn from previous execution results and revise your plan. The revised plan should be be under %s. Then use tools to follow the new plan.
`, RePlanningTag)

	reasoningPreamble := `
Below are the requirements for the reasoning:
The reasoning makes a summary of the current trajectory based on the user query and tool outputs. Based on the tool outputs and plan, the reasoning also comes up with instructions to the next steps, making the trajectory closer to the final answer.
`
	finalAnswerPreamble := `
Below are the requirements for the final answer:
The final answer should be precise and follow query formatting requirements. Some queries may not be answerable with the available tools and information. In those cases, inform the user why you cannot process their query and ask for more information.
`
	toolCodeWithoutPythonLibrariesPreamble := `
Below are the requirements for the tool code:

**Custom Tools:** The available tools are described in the context and can be directly used.
- Code must be valid self-contained Python snippets with no imports and no references to tools or Python libraries that are not in the context.
- You cannot use any parameters or fields that are not explicitly defined in the APIs in the context.
- The code snippets should be readable, efficient, and directly relevant to the user query and reasoning steps.
- When using the tools, you should use the library name together with the function name, e.g., vertex_search.search().
- If Python libraries are not provided in the context, NEVER write your own code other than the function calls using the provided tools.
`
	userInputPreamble := `
VERY IMPORTANT instruction that you MUST follow in addition to the above instructions:

You should ask for clarification if you need more information to answer the question.
You should prefer using the information available in the context instead of repeated tool use.
`
	return strings.Join([]string{
		highLevelPreamble,
		planningPreamble,
		reasoningPreamble,
		finalAnswerPreamble,
		toolCodeWithoutPythonLibrariesPreamble,
		userInputPreamble,
	}, "\n\n"), nil
}

func (prp *PlanReActPlanner) processPlanningResponsePart(responsePart Part) (Part, bool) {
	// Helper to mark a part as thought
	isThought := false
	if responsePart.Text != "" &&
		(strings.HasPrefix(responsePart.Text, PlanningTag) ||
			strings.HasPrefix(responsePart.Text, ReasoningTag) ||
			strings.HasPrefix(responsePart.Text, ActionTag) ||
			strings.HasPrefix(responsePart.Text, RePlanningTag)) {
		// In Python, part.thought = True. In Go, genai.Part doesn't have a Thought field.
		// This needs to be handled by how the event/part is processed later or by adding a custom field.
		// For now, we'll just note it.
		// responsePart.Thought = true // Conceptual
		isThought = true
	}
	return responsePart, isThought
}


func (prp *PlanReActPlanner) ProcessPlanningResponse(callbackCtx *CallbackContext, responseParts []Part) ([]Part, error) {
	if len(responseParts) == 0 {
		return nil, nil
	}

	var preservedParts []Part
	firstFCCalled := false // Flag to track if a function call has already been processed

	for _, part := range responseParts {
		if part.FunctionCall != nil {
			// If a function call has already been seen and processed, subsequent function calls are usually part of parallel execution.
			// The original python code's logic for `first_fc_part_index` and the subsequent loop for parallel calls
			// suggests that once the first function call (or block of parallel function calls) is encountered,
			// no further text processing (like splitting by FINAL_ANSWER_TAG or marking as thought) should occur for *that* LLM turn's text parts.
			// All function calls in that turn are collected.
			if part.FunctionCall.Name != "" { // Ignore function calls with empty names
				preservedParts = append(preservedParts, part)
				firstFCCalled = true
			}
			continue
		}

		// If we've already processed function calls in this LLM response,
		// we don't process further text parts for planning/reasoning tags for this specific method's purpose.
		// The Python code structure implies this by breaking after finding the first FC and then only appending subsequent FCs.
		// Any text after the first block of FCs is not re-evaluated for FINAL_ANSWER_TAG by *this* method.
		if firstFCCalled {
			preservedParts = append(preservedParts, part) // Append text part as is if it follows FCs
			continue
		}
		
		if part.Text != "" && strings.Contains(part.Text, FinalAnswerTag) {
			reasoningText, finalAnswerText := splitByLastPattern(part.Text, FinalAnswerTag)
			if reasoningText != "" {
				reasoningPart := Part{Text: reasoningText}
				// Mark as thought: In Go, this would mean setting a flag or using a wrapper if genai.Part is immutable.
				// For now, assume a conceptual `Thought` field or side effect.
				// reasoningPart.Thought = true // Conceptual
				preservedParts = append(preservedParts, reasoningPart)
			}
			if finalAnswerText != "" {
				preservedParts = append(preservedParts, Part{Text: finalAnswerText})
			}
		} else {
			processedPart, _ := prp.processPlanningResponsePart(part) // The boolean thought flag isn't directly used here
			preservedParts = append(preservedParts, processedPart)
		}
	}
	return preservedParts, nil
}

// splitByLastPattern splits text by the last occurrence of a separator.
func splitByLastPattern(text, separator string) (string, string) {
	index := strings.LastIndex(text, separator)
	if index == -1 {
		return text, ""
	}
	// Include the separator in the first part, as per Python ADK's logic.
	return text[:index+len(separator)], text[index+len(separator):]
}


*/

// NLPlanningRequestProcessor is the request processor for NL planning.
type NLPlanningRequestProcessor struct{}

func (p *NLPlanningRequestProcessor) RunAsync(invocationCtx *InvocationContext, llmReq *LlmRequest) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		planner := getPlanner(invocationCtx)
		if planner == nil {
			return
		}

		if bip, ok := planner.(*BuiltInPlanner); ok { // Using type assertion for specific planner types
			bip.ApplyThinkingConfig(llmReq) // Assumes LlmRequest has Config.ThinkingConfig field
		}

		planningInstruction, err := planner.BuildPlanningInstruction(&ReadonlyContext{InvocationContext: invocationCtx}, llmReq)
		if err != nil {
			log.Printf("Error building planning instruction: %v", err)
			// Potentially yield an error event or handle otherwise
			return
		}
		if planningInstruction != "" {
			llmReq.AppendInstructions([]string{planningInstruction})
		}

		removeThoughtFromRequest(llmReq)
	}()
	return outCh, nil
}

// NLPlanningResponseProcessor is the response processor for NL planning.
type NLPlanningResponseProcessor struct{}

func (p *NLPlanningResponseProcessor) RunAsync(invocationCtx *InvocationContext, llmResp *LlmResponse) (<-chan *Event, error) {
	outCh := make(chan *Event)
	go func() {
		defer close(outCh)

		if llmResp == nil || llmResp.Content == nil || len(llmResp.Content.Parts) == 0 {
			return
		}

		planner := getPlanner(invocationCtx)
		if planner == nil {
			return
		}

		callbackCtx := &CallbackContext{InvocationContext: invocationCtx} // Simplified state
		
		processedParts, err := planner.ProcessPlanningResponse(callbackCtx, llmResp.Content.Parts)
		if err != nil {
			log.Printf("Error processing planning response: %v", err)
			// Potentially yield an error event or handle otherwise
			return
		}

		if processedParts != nil { // Check if parts were actually processed and returned
			llmResp.Content.Parts = processedParts
		}

		if callbackCtx.State.HasDelta() { // Assuming State has HasDelta and EventActions is part of CallbackContext
			outCh <- &Event{
				InvocationID: invocationCtx.InvocationID,
				Author:       invocationCtx.Agent.GetName(),
				Branch:       invocationCtx.Branch,
				Actions:      callbackCtx.EventActions,
			}
		}
	}()
	return outCh, nil
}

func getPlanner(invocationCtx *InvocationContext) Planner {
	llmAgent, ok := invocationCtx.Agent.(LlmAgent)
	if !ok || llmAgent.GetPlanner() == nil {
		return nil
	}
	// In Python, if agent.planner is True, it defaults to PlanReActPlanner.
	// For Go, this needs to be explicit. Let's assume GetPlanner returns the concrete type or nil.
	return llmAgent.GetPlanner()
}

func removeThoughtFromRequest(llmReq *LlmRequest) {
	if llmReq.Contents == nil {
		return
	}
	for i := range llmReq.Contents {
		if llmReq.Contents[i].Parts == nil {
			continue
		}
		for j := range llmReq.Contents[i].Parts {
			// In Go, genai.Part does not have a 'Thought' field.
			// This concept needs to be handled differently if it's essential,
			// perhaps by filtering parts based on a conventional prefix/suffix in Text,
			// or by having a custom Part wrapper struct.
			// For now, this function is a conceptual no-op regarding 'Thought'.
			// llmReq.Contents[i].Parts[j].Thought = nil // Conceptual
		}
	}
}

// Placeholder for LlmAgent.GetPlanner()
/*
func (a *LlmAgent) GetPlanner() Planner {
	return a.AgentPlanner
}
*/








